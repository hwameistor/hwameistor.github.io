{"searchDocs":[{"title":"自动化、高可用云原生本地存储系统 —— HwameiStor 开源上线","type":0,"sectionRef":"#","url":"/cn/blog/1","content":"","keywords":"","version":null},{"title":"智、稳、敏 全面增强本地存储​","type":1,"pageTitle":"自动化、高可用云原生本地存储系统 —— HwameiStor 开源上线","url":"/cn/blog/1#智稳敏-全面增强本地存储","content":"自动化运维管理 可以自动发现、识别、管理、分配磁盘。 根据亲和性，智能调度应用和数据。自动监测磁盘状态，并及时预警。 高可用的数据 使用跨节点副本同步数据， 实现高可用。发生问题时，会自动将应用调度到高可用数据节点上，保证应用的连续性。 丰富的数据卷类型 聚合 HDD、SSD、NVMe 类型的磁盘，提供非低延时，高吞吐的数据服务。 灵活动态的线性扩展 可以根据集群规模大小进行动态的扩容，灵活满足应用的数据持久化需求。 ","version":null,"tagName":"h2"},{"title":"丰富应用场景 广泛适配企业需求​","type":1,"pageTitle":"自动化、高可用云原生本地存储系统 —— HwameiStor 开源上线","url":"/cn/blog/1#丰富应用场景-广泛适配企业需求","content":"适配高可用架构中间件 Kafka、ElasticSearch、Redis等，这类中间件应用自身具备高可用架构，同时对数据的 IO 访问有很高要求。HwameiStor 提供的基于 LVM 的单副本本地数据卷，可以很好地满足它们的要求。 为应用提供高可用数据卷 MySQL 等 OLTP 数据库，要求底层存储提供高可用的数据存储，当发生问题时可快速恢复数据，同时，也要求保证高性能的数据访问。HwameiStor 提供的双副本的高可用数据卷，可以很好地满足此类需求。 自动化运维传统存储软件 MinIO、Ceph 等存储软件，需要使用 Kubernetes 节点上的磁盘，可以采用 PVC/PV 的方式，通过 CSI 驱动自动化地使用 HwameiStor 的单副本本地卷，快速响应业务系统提出的部署、扩容、迁移等需求，实现基于 Kubernetes 的自动化运维。 ","version":null,"tagName":"h2"},{"title":"加入我们​","type":1,"pageTitle":"自动化、高可用云原生本地存储系统 —— HwameiStor 开源上线","url":"/cn/blog/1#加入我们","content":"如果说未来是智能互联时代，那么程序员就是通往未来的领路人，开源社区就是程序员们的 “元宇宙”。 目前，「HwameiStor 云原生本地储存系统」已经正式在 Github 开源上线，对它感兴趣的话，就来加入我们吧，一起开垦这块属于程序员的 “元宇宙” 新土地，成为未来行道者。 ","version":null,"tagName":"h2"},{"title":"HwameiStor Reliable Helper System 开源上线","type":0,"sectionRef":"#","url":"/cn/blog/2","content":"","keywords":"","version":null},{"title":"全面增强运维可靠维护系统​","type":1,"pageTitle":"HwameiStor Reliable Helper System 开源上线","url":"/cn/blog/2#全面增强运维可靠维护系统","content":"","version":null,"tagName":"h2"},{"title":"可靠、一键更换、告警提示​","type":1,"pageTitle":"HwameiStor Reliable Helper System 开源上线","url":"/cn/blog/2#可靠一键更换告警提示","content":"可靠数据迁移及数据回填 可以通过自动识别raid磁盘与否，进行判断是否需要数据迁移和回填，保障数据可靠性。 一键硬盘更换 通过新旧硬盘唯一uuid来实现一键硬盘更换操作。 直观告警提示 一键换盘过程中，存在换盘异常信息，进行及时预警。 ","version":null,"tagName":"h2"},{"title":"加入我们​","type":1,"pageTitle":"HwameiStor Reliable Helper System 开源上线","url":"/cn/blog/2#加入我们","content":"如果说未来是智能互联时代，那么程序员就是通往未来的领路人，开源社区就是程序员们的 “元宇宙”。 目前，「HwameiStor 云原生本地储存系统」已经正式在 Github 开源上线，对它感兴趣的话，就来加入我们吧，一起开垦这块属于程序员的 “元宇宙” 新土地，成为未来行道者。 ","version":null,"tagName":"h2"},{"title":"LV 和 LVReplica","type":0,"sectionRef":"#","url":"/cn/blog/3","content":"","keywords":"","version":null},{"title":"LocalVolume​","type":1,"pageTitle":"LV 和 LVReplica","url":"/cn/blog/3#localvolume","content":"LocalVolume 是 HwameiStor 定义的 CRD，代表 HwameiStor 为用户提供的数据卷。LocalVolume 和 Kubernetes 的 PersistentVolume 是一一对应的，含义也是类似的，均代表一个数据卷。不同之处在于，LocalVolume 记录 HwameiStor 相关的信息，而 PersistentVolume 记录 Kubernetes 平台本身的信息，并关联到 LocalVolume。 可以通过以下命令查看系统中 LocalVolume 的详细信息： # check status of local volume and volume replica $ kubectl get lv # or localvolume NAME POOL KIND REPLICAS CAPACITY ACCESSIBILITY STATE RESOURCE PUBLISHED AGE pvc-996b05e8-80f2-4240-ace4-5f5f250310e2 LocalStorage_PoolHDD LVM 1 1073741824 k8s-node1 Ready -1 22m  既然 HwameiStor 可以通过 LocalVolume 表示一个数据卷，为什么还需要 LocalVolumeReplica 呢？ ","version":null,"tagName":"h2"},{"title":"LocalVolumeReplica​","type":1,"pageTitle":"LV 和 LVReplica","url":"/cn/blog/3#localvolumereplica","content":"LocalVolumeReplica 也是 HwameiStor 定义的 CRD。但是与 LocalVolume 不同，LocalVolumeReplica 代表数据卷的副本。 在 HwameiStor 中，LocalVolume 会指定某个属于它的 LocalVolumeReplica 作为当前激活的副本。可以看出LocalVolume 可以拥有多个 LocalVolumeReplica，即一个数据卷可以有多个副本。目前 HwameiStor 会在众多副本中激活其中一个，被应用程序挂载，其他副本作为热备副本。 可以通过以下命令查看系统中 LocalVolumeReplica 的详细信息： $ kubectl get lvr # or localvolumereplica NAME KIND CAPACITY NODE STATE SYNCED DEVICE AGE pvc-996b05e8-80f2-4240-ace4-5f5f250310e2-v5scm9 LVM 1073741824 k8s-node1 Ready true /dev/LocalStorage_PoolHDD/pvc-996b05e8-80f2-4240-ace4-5f5f250310e2 80s  有了卷副本（LocalVolumeReplica）的概念后，HwameiStor 作为一款本地存储系统，具备了一些很有竞争力的特性，例如数据卷的HA，迁移，热备，Kubernetes 应用快速恢复等等。 ","version":null,"tagName":"h2"},{"title":"总结​","type":1,"pageTitle":"LV 和 LVReplica","url":"/cn/blog/3#总结","content":"其实 LocalVolume 和 LocalVolumeReplica 在很多存储系统中都有引入，是个很通用的概念。只是通过这一概念，实现了各具特色的产品，在解决某个技术难点的时候也可能采取不同的解决方案，因此而适合于不同的生产场景。 随着 HwameiStor 的迭代和演进，我们将会提供更多的能力，从而适配越来越多的使用场景。无论您是用户还是开发者，欢迎您加入 HwameiStor 的大家庭！ ","version":null,"tagName":"h2"},{"title":"直播回顾论道原生：云原生存储","type":0,"sectionRef":"#","url":"/cn/blog/live","content":"","keywords":"","version":null},{"title":"云原生本地存储 HwameiStor​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#云原生本地存储-hwameistor","content":"云原生存储作为云原生堆栈中容器化操作的底层架构之一，将底层存储服务暴露给容器和微服务，可以聚合来自不同介质的存储资源，通过提供持久卷使有状态的工作负载能够在容器内运行。CNCF 官方对于云原生存储形态的定义，一般包括三点，第一是运行在 K8s 上，即其本身是容器形态。第二是使用 K8s 对象类，主要是自定义资源 Custom Resource Define (CRD)。第三是最重要的一点，必须使用容器存储接口 Container Storage Interface (CSI) 。HwameiStor 正是基于以上定义开发的，一个端到端的云原生本地存储系统。  HwameiStor 最底层是一个本地磁盘管理器 Local Disk Manager (LDM)，管理器将各种存储介质 (比如 HDD、SSD 和 NVMe 磁盘) 形成本地存储资源池进行统一管理，管理和接入完全自动。部署完 HwameiStor 后，即可根据介质的不同，分配至不同的资源池。资源池之上采用逻辑卷管理 Logical Volume Manager (LVM) 进行管理，使用 CSI 架构提供分布式的本地数据卷服务，为有状态的云原生应用或组件，提供数据持久化能力。 HwameiStor 是专门为云原生需求设计的存储系统，有高可用、自动化、低成本、快速部署、高性能等优点，可以替代昂贵的传统存储区域网络 Storage Area Network (SAN)。它有三个核心组件： 本地盘管理 (LDM)，使用 CRD 定义及管理本地数据盘，通过 LDM，在 K8s内部可明确获取到本地磁盘的属性、大小等信息。 本地存储 (LS)，在实现本地盘管理之后，通过 LVM 卷组管理，将逻辑卷 LV 分配给物理卷 PV 。 调度器 (Scheduler)，把容器调度到本地有数据的节点上。 HwameiStor 的核心在于自定义资源 CRD 的定义及实现，在 K8s 已有 PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) 对象类之上，Hwameistor 定义了更丰富的对象类把 PV/PVC 和本地数据盘关联起来。 HwameiStor 有四点特性： 自动化运维管理，自动发现、识别、管理、分配磁盘，根据亲和性智能调度应用和数据，还可自动监测磁盘状态并及时预警。高可用的数据支持，HwameiStor 使用跨节点副本同步数据，实现高可用，发生问题时，它会自动将应用调度到高可用数据节点上，保证应用的连续性。丰富的数据卷类型，HwameiStor 聚合 HDD、SSD、NVMe 类型的磁盘，提供低延时，高吞吐的数据服务。灵活动态的线性扩展，HwameiStor 根据集群规模大小进行动态的扩容，灵活满足应用的数据持久化需求。 HwameiStor 的应用场景主要包括以下三类： 适配高可用架构中间件 Kafka、ElasticSearch、Redis 等，这类中间件应用自身具备高可用架构，同时对数据的 IO 访问有很高要求。HwameiStor 提供的基于 LVM 的单副本本地数据卷，可以很好地满足它们的要求。 为应用提供高可用数据卷 MySQL 等 OLTP 数据库，要求底层存储提供高可用的数据存储，当发生问题时可快速恢复数据，同时，也要求保证高性能的数据访问。HwameiStor 提供的双副本的高可用数据卷，可以很好地满足此类需求。 自动化运维传统存储软件 MinIO、Ceph 等存储软件，需要使用 K8s 节点上的磁盘，可以采用 PVC/PV 的方式，通过 CSI 驱动自动化地使用 HwameiStor 的单副本本地卷，快速响应业务系统提出的部署、扩容、迁移等需求，实现基于 K8s 的自动化运维。 ","version":null,"tagName":"h2"},{"title":"K8s 场景下的云原生存储解决方案————ZettaStor HASP​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#k8s-场景下的云原生存储解决方案zettastor-hasp","content":"根据云原生基金会 (CNCF) 2020 年度报告，有状态应用已经成为了容器应用的主流，占比达到了 55%，其中有 29% 将存储列为了采用容器技术的主要挑战，而现有云原生环境面临的一个存储难题，就是性能与可用性难以兼顾，单纯使用一种存储，无法满足所有需求。因此在有状态应用的实际落地过程中，数据存储技术是关键。  ZettaStor HASP 是一个云原生高性能数据聚合存储平台，它是一个高性能的用户态文件系统，具备跨异构存储多副本冗余保护的能力。相比传统的分布式存储系统而言，数据副本可以在不同类型的存储系统之间流动。除此之外，还可实现存储资源统一灵活编排，并与容器平台紧密集成。 以三个场景为例： 对于 Kafka、Redis、MongoDB、HDFS 等性能要求高，且自身具备数据冗余机制的分布式应用，ZettaStor HASP 拥有更高的数据访问性能，可以实现存储资源动态分配及精细化管理。 MySQL、PostgreSQL 等自身不提供数据冗余机制，只能依赖外部存储，舍弃本地存储的高性能，ZettaStor HASP 可通过跨节点冗余保护，实现数据的高可用，同时兼具本地存储的高性能。 对于关键业务而言，需要防范两节点同时故障的风险，而 ZettaStor HASP 跨本地及外部存储的副本冗余保护，可保障关键业务运行无忧。  ZettaStor HASP 分为三层架构，最上层是高性能的分布式文件系统，也是 HASP 的核心，这个用户态的文件系统是完全自主研发的，全面兼容 POSIX 标准，在用户态与内核态之间实现零数据拷贝，可彻底发挥 NVMe SSD/SCM/PM 等高速介质的性能。第二层是数据服务层，除了在不同节点的本地存储之间、本地存储与外部存储之间、异构外部存储之间提供服务，还可针对单副本和多副本、强一致性和弱一致性分别提供存储方案。第三层是存储管理层，负责对不同的存储设备进行交互，通过统一的数据格式打破设备壁垒及数据孤岛，让数据及业务能够跨异构设备自由流动。除此之外，该系统还可通过 CSI 接口进行存储设备的调度。 ZettaStor HASP 是一个与容器平台紧密结合的分布式存储平台，可进行超融合部署和 CSI 集成，具备节点亲和性，可对用户 Pod 运行状态进行感知，使得存储行为更加适配。 ","version":null,"tagName":"h2"},{"title":"圆桌讨论​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#圆桌讨论","content":"","version":null,"tagName":"h2"},{"title":"Q1: 什么是云原生存储？​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#q1-什么是云原生存储","content":"郑泓超：狭义的云原生存储需要满足三点，第一，与 CSI 接口实现良好的对接，满足 CSI 规范；其次，需要以容器的形式部署在 K8s 上；第三，存储系统内部的信息，也需要通过 CRD 产生新的对象类，并最终存储在 K8s 中。 冯钦：云原生上的存储是满足多种特性、多种需求的一个解决方案，除了提供统一的存储平台，应对不同的存储应用，提供不同的存储特性，实现统一纳管之外，还需要对接 CSI 接口，并打通存储与 K8s 之间的交流。 牛乐川：现在应用最广的云原生存储，还是在云存储和或者分布式存储的基础上实现云原生化。同时，也有厂商在传统存储具备的特殊能力方面进行一些延伸的尝试。 ","version":null,"tagName":"h3"},{"title":"Q2: 云原生存储应该如何支持云原生应用？​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#q2-云原生存储应该如何支持云原生应用","content":"冯钦：主要分为两个方面，第一，云原生存储需要支持云原生应用的特性，特性决定了应用对于存储的要求。第二，需要满足 CSI 的要求，以支持云原生特殊的需求。 牛乐川：从性能角度来看，云原生存储需要全方位满足 CSI 架构的要求，以应对多样化的云原生场景。为了给云原生应用提供良好的底层支持和响应保障，云原生存储需要实现高效运维。根据现实情况，云原生存储在成本、迁移性和技术支持方面也有考量。和云原生应用一样，云原生存储需要 “弹性”，运算方面应当具备弹性扩展能力，数据方面也需要实现扩容缩容，以及冷热数据之间的转换。除此之外，云原生存储应当具备一个开放的生态。 ","version":null,"tagName":"h3"},{"title":"Q3:云原生存储和传统存储之间的异同点及优劣势？​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#q3云原生存储和传统存储之间的异同点及优劣势","content":"郑泓超：云原生存储部署之后是聚合形态，而传统存储接入 K8s 一般是分离形态。云原生存储因为运行在 K8s 上，有利于开发微服务，而传统存储往往需要开发者进行存储的 API 扩展。但是，聚合形态也在一定程度上，导致 K8s 的问题容易蔓延到存储当中，给运维带来困难。除此之外，云原生存储还存在网络共享和硬盘负载方面的问题。 冯钦：外部存储中，存储节点和计算节点之间的影响很低，后端存储一般为分布式存储，安全性和可用性相对较高。但在云原生场景下，单一使用外部存储存在一定的劣势，性能上会增加网络的消耗，同时还存在额外成本增加以及与 K8s 的联动不足的问题。 ","version":null,"tagName":"h3"},{"title":"Q4: 云原生存储中应该如何赋能传统存储？​","type":1,"pageTitle":"直播回顾论道原生：云原生存储","url":"/cn/blog/live#q4-云原生存储中应该如何赋能传统存储","content":"牛乐川：这是一个非常迫切的需求，对于 K8s 原生的能力，比如删除、创建、扩容的能力，主要是通过 CRD 进行实现，社区也在积极部署。同时，传统存储中积攒的一些能力，尚未在云原生存储体系中得到体现，比如定时任务、可观测性等。如何在平台侧更好地让云原生赋能传统存储，充分发挥双方的优势能力，将云原生存储发展成熟，还有很多需要努力的空间，我们团队也正在进行这方面的研发。 郑泓超：再总结一个点，在常规的做法里，K8s 对接传统存储，需要完成的是 CSI 驱动集合，但这只局限于表面，CSI 定义了一部分存储的操作流程，但更多还只是一个接口。所以，CSI 社区是否应该对常规存储的一些功能 (例如定时备份、高可用等) 用 CRD 进行定义？而厂商能否做一部分工作，把一些高级的、特殊的流程用 CRD 进行定义？从而使得 K8s 能在存储领域得到更广泛的应用，是值得我们去思考和实践的。 ","version":null,"tagName":"h3"},{"title":"HwameiStor 对 Minio 的支持","type":0,"sectionRef":"#","url":"/cn/blog/minio","content":"","keywords":"","version":null},{"title":"MinIO 简介​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#minio-简介","content":"MinIO 是一款高性能、分布式、兼容 S3 的多云对象存储系统套件。MinIO 原生支持 Kubernetes，能够支持所有公有云、私有云及边缘计算环境。 MinIO 是 GNU AGPL v3 开源的软件定义产品，能够很好地运行在标准硬件如 X86 等设备上。  MinIO 的架构设计从一开始就是针对性能要求很高的私有云标准，在实现对象存储所需要的全部功能的基础上追求极致的性能。 MinIO 具备易用性、高效性及高性能，能够以更简单的方式提供具有弹性伸缩能力的云原生对象存储服务。 MinIO 在传统对象存储场景（如辅助存储、灾难恢复和归档）方面表现出色，同时在机器学习、大数据、私有云、混合云等方面的存储技术上也独树一帜，包括数据分析、高性能应用负载、原生云应用等。 ","version":null,"tagName":"h2"},{"title":"MinIO 架构设计​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#minio-架构设计","content":"MinIO 为云原生架构设计，可以作为轻量级容器运行并由外部编排服务如 Kubernetes 管理。 MinIO 整个服务包约为不到 100 MB 的静态二进制文件，即使在很高负载下也可以高效利用 CPU 和内存资源并可以在共享硬件上共同托管大量租户。 对应的架构图如下：  MinIO 用作云原生应用程序的主要存储，与传统对象存储相比，云原生应用程序需要更高的吞吐量和更低的延迟，而这些都是 MinIO 能够达成的性能指标，读/写速度高达 183 GB/秒和 171 GB/秒。 MinIO 极致的高性能离不开底层存储基础平台。本地存储在众多的存储协议中具有最高的读写性能无疑能为 MinIO 提供性能保障。 HwameiStor 正是满足云原生时代要求的储存系统。它具有高性能、高可用、自动化、低成本、快速部署等优点，可以替代昂贵的传统 SAN 存储。 MinIO 可以在带有本地驱动器（JBOD/JBOF）的标准服务器上运行。 集群为完全对称的体系架构，即所有服务器的功能均相同，没有名称节点或元数据服务器。 MinIO 将数据和元数据作为对象一起写入从而无需使用元数据数据库。 MinIO 以内联、严格一致的操作执行所有功能，包括擦除代码、位 rotrot 检查、加密等。 每个 MinIO 集群都是分布式 MinIO 服务器的集合，每个节点一个进程。 MinIO 作为单个进程在用户空间中运行，并使用轻量级的协同例程来实现高并发。 将驱动器分组到擦除集（默认情况下，每组 16 个驱动器），然后使用确定性哈希算法将对象放置在这些擦除集上。 MinIO 专为大规模、多数据中心云存储服务而设计。 每个租户都运行自己的 MinIO 集群，该集群与其他租户完全隔离，从而使租户能够免受升级、更新和安全事件的任何干扰。 每个租户通过联合跨地理区域的集群来独立扩展。  ","version":null,"tagName":"h3"},{"title":"以 HwameiStor 为底座搭建 MinIO 的优势​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#以-hwameistor-为底座搭建-minio-的优势","content":"以 HwameiStor 为底座搭建 MinIO 存储方案，构建智、稳、敏全面增强本地存储，具备以下优势。 自动化运维管理 可以自动发现、识别、管理、分配磁盘。 根据亲和性，智能调度应用和数据。自动监测磁盘状态，并及时预警。 高可用的数据 使用跨节点副本同步数据， 实现高可用。发生问题时，会自动将应用调度到高可用数据节点上，保证应用的连续性。 丰富的数据卷类型 聚合 HDD、SSD、NVMe 类型的磁盘，提供非低延时，高吞吐的数据服务。 灵活动态的线性扩展 可以根据集群规模大小进行动态的扩容，灵活满足应用的数据持久化需求。 丰富的应用场景，广泛适配企业需求，适配高可用架构中间件 类似 Kafka、ElasticSearch、Redis 等这类中间件自身具备高可用架构，同时对数据的 IO 访问有很高要求。 HwameiStor 提供的基于 LVM 的单副本本地数据卷，可以很好地满足它们的要求。 为应用提供高可用数据卷 MySQL 等 OLTP 数据库要求底层存储提供高可用的数据存储，当发生问题时可快速恢复数据，同时也要求保证高性能的数据访问。 HwameiStor 提供的双副本的高可用数据卷，可以很好地满足此类需求。 自动化运维传统存储软件 MinIO、Ceph 等存储软件，需要使用 Kubernetes 节点上的磁盘，可以采用 PVC/PV 的方式， 通过 CSI 驱动自动化地使用 HwameiStor 的单副本本地卷，快速响应业务系统提出的部署、扩容、迁移等需求，实现基于 Kubernetes 的自动化运维。 ","version":null,"tagName":"h3"},{"title":"测试环境​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#测试环境","content":"按照以下步骤依次部署 Kubernetes 集群、HwameiStor 本地存储和 MinIO。 ","version":null,"tagName":"h2"},{"title":"部署 Kubernetes 集群​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#部署-kubernetes-集群","content":"本次测试使用了三台虚拟机节点部署了 Kubernetes 集群：1 Master + 2 Worker 节点，kubelet 版本为 1.22.0。  ","version":null,"tagName":"h3"},{"title":"部署 HwameiStor 本地存储​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#部署-hwameistor-本地存储","content":"在 Kubernetes 上部署 HwameiStor 本地存储。  两台 Worker 节点各配置了五块磁盘（SDB、SDC、SDD、SDE、SDF）用于 HwameiStor 本地磁盘管理。   查看 local storage node 状态。  创建了 storagClass。  ","version":null,"tagName":"h3"},{"title":"分布式多租户源码部署安装（minio operator）​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#分布式多租户源码部署安装minio-operator","content":"本节说明如何部署 minio operator，如何创建租户，如何配置 HwameiStor 本地卷。 ","version":null,"tagName":"h2"},{"title":"部署 minio operator​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#部署-minio-operator","content":"参照以下步骤部署 minio operator。 复制 minio operator 仓库到本地。 git clone &lt;https://github.com/minio/operator.git&gt; 进入 helm operator 目录：/root/operator/helm/operator。 部署 minio-operator 实例。 helm install minio-operator \\ --namespace minio-operator \\ --create-namespace \\ --generate-name . --set persistence.storageClass=local-storage-hdd-lvm . 检查 minio-operator 资源运行情况。 ","version":null,"tagName":"h3"},{"title":"创建租户​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#创建租户","content":"参照以下步骤创建一个租户。 进入 /root/operator/examples/kustomization/base 目录。如下修改 tenant.yaml。 进入 /root/operator/helm/tenant/ 目录。如下修改 values.yaml 文件。 进入 /root/operator/examples/kustomization/tenant-lite 目录。如下修改 kustomization.yaml 文件。 如下修改 tenant.yaml 文件。 如下修改 tenantNamePatch.yaml 文件。 创建租户： kubectl apply –k . 检查租户 minio-t1 资源状态： 如要创建一个新的租户可以在 /root/operator/examples/kustomization 目录下建一个新的 tenant 目录（本案例为 tenant-lite-2）并对相应文件做对应修改。 执行 kubectl apply –k . 创建新的租户 minio-t2。 ","version":null,"tagName":"h3"},{"title":"配置 HwameiStor 本地卷​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#配置-hwameistor-本地卷","content":"依次运行以下命令来配置本地卷。 kubectl get statefulset.apps/minio-t1-pool-0 -nminio-tenant -oyaml   kubectl get pvc –A   kubectl get pvc export-minio6-0 -nminio-6 -oyaml   kubectl get pv   kubectl get pvc data0-minio-t1-pool-0-0 -nminio-tenant -oyaml   kubectl get lv   kubect get lvr   ","version":null,"tagName":"h3"},{"title":"HwameiStor 与 MinIo 测试验证​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#hwameistor-与-minio-测试验证","content":"完成上述配置之后，执行了基本功能测试和多租户隔离测试。 ","version":null,"tagName":"h2"},{"title":"基本功能测试​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#基本功能测试","content":"基本功能测试的步骤如下。 从浏览器登录 minio console：10.6.163.52:30401/login。 通过 kubectl minio proxy -n minio-operator 获取 JWT。 浏览及管理创建的租户信息。 登录 minio-t1 租户（用户名 minio，密码 minio123）。 浏览 bucket bk-1。 创建新的 bucket bk-1-1。 创建 path path-1-2。 上传文件成功： 上传文件夹成功： 创建只读用户： ","version":null,"tagName":"h3"},{"title":"多租户隔离测试​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#多租户隔离测试","content":"执行以下步骤进行多租户隔离测试。 登录 minio-t2 租户。 此时只能看到 minio-t2 内容，minio-t1 的内容被屏蔽。 创建 bucket。 创建 path。 上传文件。 创建用户。 配置用户 policy。 删除 bucket。 ","version":null,"tagName":"h3"},{"title":"结论​","type":1,"pageTitle":"HwameiStor 对 Minio 的支持","url":"/cn/blog/minio#结论","content":"本次测试是在 Kubernetes 1.22 平台上部署了 MinIO 分布式对象存储并对接 HwameiStor 本地存储。在此环境中完成了基本能力测试、系统安全测试及运维管理测试。 全部测试成功通过，证实了 HwameiStor 能够完美适配 MinIO 存储方案。 ","version":null,"tagName":"h2"},{"title":"Welcome","type":0,"sectionRef":"#","url":"/cn/blog/welcome","content":"Welcome to the Hwameistor blog space. Here you can keep up with the progress of the Hwameistor open source project and recent hot topics. We also plan to include release notes for major releases, guidance articles, community-related events, and possibly some development tips, and interesting topics within the team. If you are interested in contributing to this open source project and would like to join the discussion or make some guest blog posts, please contact us. GitHub address is: https://github.com/hwameistor","keywords":"","version":null},{"title":"API 管理","type":0,"sectionRef":"#","url":"/cn/docs/apis","content":"","keywords":"","version":"下一个"},{"title":"CRD 对象类​","type":1,"pageTitle":"API 管理","url":"/cn/docs/apis#crd-对象类","content":"HwameiStor 在 Kubernetes 已有的 PV 和 PVC 对象类基础上，HwameiStor 定义了更丰富的对象类，把 PV/PVC 和本地数据盘关联起来。 名称\t缩写\tKind\t功能clusters\thmcluster\tCluster\tHwameiStor 集群 events\tevt\tEvent\tHwameiStor 集群的审计日志 localdiskclaims\tldc\tLocalDiskClaim\t筛选并分配本地数据盘 localdisknodes\tldn\tLocalDiskNode\t裸磁盘类型数据卷的存储节点 localdisks\tld\tLocalDisk\t节点上数据盘，自动识别空闲可用的数据盘 localdiskvolumes\tldv\tLocalDiskVolume\t裸磁盘类型数据卷 localstoragenodes\tlsn\tLocalStorageNode\tLVM 类型数据卷的存储节点 localvolumeconverts\tlvconvert\tLocalVolumeConvert\t将普通LVM类型数据卷转化为高可用LVM类型数据卷 localvolumeexpands\tlvexpand\tLocalVolumeExpand\t扩容LVM类型数据卷的容量 localvolumegroups\tlvg\tLocalVolumeGroup\tLVM 类型数据卷组 localvolumemigrates\tlvmigrate\tLocalVolumeMigrate\t迁移LVM类型数据卷 localvolumereplicas\tlvr\tLocalVolumeReplica\tLVM 类型数据卷的副本 localvolumereplicasnapshotrestores\tlvrsrestore,lvrsnaprestore\tLocalVolumeReplicaSnapshotRestore\t恢复 LVM 类型数据卷副本的快照 localvolumereplicasnapshots\tlvrs\tLocalVolumeReplicaSnapshot\tLVM 类型数据卷副本的快照 localvolumes\tlv\tLocalVolume\tLVM 类型数据卷 localvolumesnapshotrestores\tlvsrestore,lvsnaprestore\tLocalVolumeSnapshotRestore\t恢复 LVM 类型数据卷快照 localvolumesnapshots\tlvs\tLocalVolumeSnapshot\tLVM 类型数据卷快照 resizepolicies ResizePolicy\tPVC自动扩容策略 ","version":"下一个","tagName":"h2"},{"title":"裸磁盘数据卷","type":0,"sectionRef":"#","url":"/cn/docs/apps/disk","content":"裸磁盘数据卷 HwameiStor 提供的另一种类型数据卷是裸磁盘数据卷。 这种数据卷是基于节点上面的裸磁盘并将其直接挂载给容器使用。 因此这种类型的数据卷提供了更高效的数据读写性能，将磁盘的性能 完全释放。 以下步骤演示了如何创建并使用裸磁盘数据卷： 准备裸磁盘存储节点 需要保证该存储节点有可用磁盘，如果没有，可以参考裸磁盘存储节点扩容。 通过以下命令查看是否有空闲磁盘： $ kubectl get localdisknodes NAME FREECAPACITY TOTALCAPACITY TOTALDISK STATUS AGE k8s-worker-2 1073741824 1073741824 1 Ready 19d 准备 StorageClass 使用以下命令创建一个名称为 hwameistor-storage-disk-ssd 的 StorageClass： $ cat &lt;&lt; EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hwameistor-storage-disk-ssd parameters: diskType: SSD provisioner: disk.hwameistor.io allowVolumeExpansion: false reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer EOF 创建数据卷 PVC 使用以下命令创建一个名称为 hwameistor-disk-volume 的 PVC： $ cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hwameistor-disk-volume spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: hwameistor-storage-disk-ssd EOF ","keywords":"","version":"下一个"},{"title":"高可用卷","type":0,"sectionRef":"#","url":"/cn/docs/apps/ha","content":"","keywords":"","version":"下一个"},{"title":"查看 StorageClass​","type":1,"pageTitle":"高可用卷","url":"/cn/docs/apps/ha#查看-storageclass","content":"StorageClass &quot;hwameistor-storage-lvm-hdd-ha&quot; 使用参数 replicaNumber: &quot;2&quot; 开启高可用功能： $ kubectl apply -f examples/sc_ha.yaml $ kubectl get sc hwameistor-storage-lvm-hdd-ha -o yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hwameistor-storage-lvm-hdd-ha parameters: replicaNumber: &quot;2&quot; convertible: &quot;false&quot; csi.storage.k8s.io/fstype: xfs poolClass: HDD poolType: REGULAR striped: &quot;true&quot; volumeKind: LVM provisioner: lvm.hwameistor.io reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true  ","version":"下一个","tagName":"h2"},{"title":"创建 StatefulSet​","type":1,"pageTitle":"高可用卷","url":"/cn/docs/apps/ha#创建-statefulset","content":"在 HwameiStor 和 StorageClass 就绪后, 一条命令就能创建 MySQL 容器和它的数据卷： $ kubectl apply -f exapmles/sts-mysql_ha.yaml  请注意 volumeClaimTemplates 使用 storageClassName: hwameistor-storage-lvm-hdd-ha： spec: volumeClaimTemplates: - metadata: name: data labels: app: sts-mysql-ha app.kubernetes.io/name: sts-mysql-ha spec: storageClassName: hwameistor-storage-lvm-hdd-ha accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 1Gi  ","version":"下一个","tagName":"h2"},{"title":"查看 MySQL Pod 和 PVC/PV​","type":1,"pageTitle":"高可用卷","url":"/cn/docs/apps/ha#查看-mysql-pod-和-pvcpv","content":"在这个例子里，MySQL 容器被调度到了节点 k8s-worker-1。 $ kubectl get po -l app=sts-mysql-ha -o wide NAME READY STATUS RESTARTS AGE IP NODE sts-mysql-ha-0 2/2 Running 0 3m08s 10.1.15.151 k8s-worker-1 $ kubectl get pvc -l app=sts-mysql-ha NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE data-sts-mysql-ha-0 Bound pvc-5236ee6f-8212-4628-9876-1b620a4c4c36 1Gi RWO hwameistor-storage-lvm-hdd 3m Filesystem  Attentions: 1) 默认情况下双副本需要两个节点同时满足pod亲和性与污点容忍度。 如果您只是想进行数据备份，可以配置pvc 注解已忽略亲和性与污点。 这种情况下只需要满足一个节点更够调度即可 apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: hwameistor.io/skip-affinity-annotations: true  ","version":"下一个","tagName":"h2"},{"title":"查看 LocalVolume 和 LocalVolumeReplica 对象​","type":1,"pageTitle":"高可用卷","url":"/cn/docs/apps/ha#查看-localvolume-和-localvolumereplica-对象","content":"通过查看和 PV 同名的 LocalVolume(LV), 可以看到本地卷创建在了节点 k8s-worker-1 和节点 k8s-worker-2。 $ kubectl get lv pvc-5236ee6f-8212-4628-9876-1b620a4c4c36 NAME POOL REPLICAS CAPACITY ACCESSIBILITY STATE RESOURCE PUBLISHED AGE pvc-5236ee6f-8212-4628-9876-1b620a4c4c36 LocalStorage_PoolHDD 1 1073741824 Ready -1 k8s-worker-1 3m  LocalVolumeReplica (LVR) 进一步显示每个节点上的后端逻辑卷设备： $ kubectl get lvr NAME CAPACITY NODE STATE SYNCED DEVICE AGE 5236ee6f-8212-4628-9876-1b620a4c4c36-d2kn55 1073741824 k8s-worker-1 Ready true /dev/LocalStorage_PoolHDD-HA/5236ee6f-8212-4628-9876-1b620a4c4c36 4m 5236ee6f-8212-4628-9876-1b620a4c4c36-glm7rf 1073741824 k8s-worker-2 Ready true /dev/LocalStorage_PoolHDD-HA/5236ee6f-8212-4628-9876-1b620a4c4c36 4m  ","version":"下一个","tagName":"h2"},{"title":"LVM 数据卷","type":0,"sectionRef":"#","url":"/cn/docs/apps/lvm","content":"","keywords":"","version":"下一个"},{"title":"查看 StorageClass​","type":1,"pageTitle":"LVM 数据卷","url":"/cn/docs/apps/lvm#查看-storageclass","content":"首先确认 HwameiStor Operator 创建了 StorageClass。然后从中选一个合适的用于创建单副本数据卷。 $ kubectl get sc hwameistor-storage-lvm-hdd -o yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hwameistor-storage-lvm-hdd parameters: convertible: &quot;false&quot; csi.storage.k8s.io/fstype: xfs poolClass: HDD poolType: REGULAR replicaNumber: &quot;1&quot; striped: &quot;true&quot; volumeKind: LVM provisioner: lvm.hwameistor.io reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true  如果这个 storageClass 没有在安装时生成，可以运行以下的 yaml 文件重新生成它： $ kubectl apply -f examples/sc-local.yaml  ","version":"下一个","tagName":"h2"},{"title":"创建 StatefulSet​","type":1,"pageTitle":"LVM 数据卷","url":"/cn/docs/apps/lvm#创建-statefulset","content":"在 HwameiStor 和 StorageClass 就绪后, 一条命令就能创建 MySQL 容器和它的数据卷: $ kubectl apply -f sts-mysql_local.yaml  请注意 volumeClaimTemplates 使用 storageClassName: hwameistor-storage-lvm-hdd: spec: volumeClaimTemplates: - metadata: name: data labels: app: sts-mysql-local app.kubernetes.io/name: sts-mysql-local spec: storageClassName: hwameistor-storage-lvm-hdd accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 1Gi  请注意，PVC 容量的最小值需要超过 4096 个块，例如使用 4KB 块时为 16MB。 ","version":"下一个","tagName":"h2"},{"title":"查看 MySQL 容器和 PVC/PV​","type":1,"pageTitle":"LVM 数据卷","url":"/cn/docs/apps/lvm#查看-mysql-容器和-pvcpv","content":"在这个例子里，MySQL 容器被调度到了节点 k8s-worker-3。 $ kubectl get po -l app=sts-mysql-local -o wide NAME READY STATUS RESTARTS AGE IP NODE sts-mysql-local-0 2/2 Running 0 3m08s 10.1.15.154 k8s-worker-3 $ kubectl get pvc -l app=sts-mysql-local NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE data-sts-mysql-local-0 Bound pvc-accf1ddd-6f47-4275-b520-dc317c90f80b 1Gi RWO hwameistor-storage-lvm-hdd 3m Filesystem  ","version":"下一个","tagName":"h2"},{"title":"查看 LocalVolume 对象​","type":1,"pageTitle":"LVM 数据卷","url":"/cn/docs/apps/lvm#查看-localvolume-对象","content":"通过查看和 PV 同名的 LocalVolume(LV), 可以看到本地卷创建在了节点 k8s-worker-3上： $ kubectl get lv pvc-accf1ddd-6f47-4275-b520-dc317c90f80b NAME POOL REPLICAS CAPACITY ACCESSIBILITY STATE RESOURCE PUBLISHED AGE pvc-accf1ddd-6f47-4275-b520-dc317c90f80b LocalStorage_PoolHDD 1 1073741824 Ready -1 k8s-worker-3 3m  ","version":"下一个","tagName":"h2"},{"title":"[可选] 扩展 MySQL 应用成一个三节点的集群​","type":1,"pageTitle":"LVM 数据卷","url":"/cn/docs/apps/lvm#可选-扩展-mysql-应用成一个三节点的集群","content":"HwameiStor 支持 StatefulSet 的横向扩展. StatefulSet容器都会挂载一个独立的本地卷： $ kubectl scale sts/sts-mysql-local --replicas=3 $ kubectl get po -l app=sts-mysql-local -o wide NAME READY STATUS RESTARTS AGE IP NODE sts-mysql-local-0 2/2 Running 0 4h38m 10.1.15.154 k8s-worker-3 sts-mysql-local-1 2/2 Running 0 19m 10.1.57.44 k8s-worker-2 sts-mysql-local-2 0/2 Init:0/2 0 14s 10.1.42.237 k8s-worker-1 $ kubectl get pvc -l app=sts-mysql-local -o wide NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE data-sts-mysql-local-0 Bound pvc-accf1ddd-6f47-4275-b520-dc317c90f80b 1Gi RWO hwameistor-storage-lvm-hdd 3m07s Filesystem data-sts-mysql-local-1 Bound pvc-a4f8b067-9c1d-450f-aff4-5807d61f5d88 1Gi RWO hwameistor-storage-lvm-hdd 2m18s Filesystem data-sts-mysql-local-2 Bound pvc-47ee308d-77da-40ec-b06e-4f51499520c1 1Gi RWO hwameistor-storage-lvm-hdd 2m18s Filesystem $ kubectl get lv NAME POOL REPLICAS CAPACITY ACCESSIBILITY STATE RESOURCE PUBLISHED AGE pvc-47ee308d-77da-40ec-b06e-4f51499520c1 LocalStorage_PoolHDD 1 1073741824 Ready -1 k8s-worker-1 2m50s pvc-a4f8b067-9c1d-450f-aff4-5807d61f5d88 LocalStorage_PoolHDD 1 1073741824 Ready -1 k8s-worker-2 2m50s pvc-accf1ddd-6f47-4275-b520-dc317c90f80b LocalStorage_PoolHDD 1 1073741824 Ready -1 k8s-worker-3 3m40s  ","version":"下一个","tagName":"h2"},{"title":"社区","type":0,"sectionRef":"#","url":"/cn/docs/community","content":"","keywords":"","version":"下一个"},{"title":"GitHub​","type":1,"pageTitle":"社区","url":"/cn/docs/community#github","content":"发起一个 Issue ","version":"下一个","tagName":"h2"},{"title":"Slack​","type":1,"pageTitle":"社区","url":"/cn/docs/community#slack","content":"加入用户讨论组 #user加入开发者讨论组 #developer ","version":"下一个","tagName":"h2"},{"title":"Blog​","type":1,"pageTitle":"社区","url":"/cn/docs/community#blog","content":"博客定期发布动态。 ","version":"下一个","tagName":"h2"},{"title":"社区例会​","type":1,"pageTitle":"社区","url":"/cn/docs/community#社区例会","content":"加入社区开发者的例会讨论。 ","version":"下一个","tagName":"h2"},{"title":"商业支持​","type":1,"pageTitle":"社区","url":"/cn/docs/community#商业支持","content":"电话：(+86) 400 002 6898 Email：info@daocloud.io 微信扫码交谈：  ","version":"下一个","tagName":"h2"},{"title":"贡献","type":0,"sectionRef":"#","url":"/cn/docs/contribute/CONTRIBUTING","content":"","keywords":"","version":"下一个"},{"title":"社区准则​","type":1,"pageTitle":"贡献","url":"/cn/docs/contribute/CONTRIBUTING#社区准则","content":"HwameiStor 是一个列在 CNCF 全景图中的沙箱项目。 我们遵循 CNCF 社区行为准则。 ","version":"下一个","tagName":"h2"},{"title":"要求​","type":1,"pageTitle":"贡献","url":"/cn/docs/contribute/CONTRIBUTING#要求","content":"HwameiStor 的源代码是用 golang 编写的， 并且使用 git 进行管理。 为了方便安装 HwameiStor，您可能需要先安装 helm。 ","version":"下一个","tagName":"h2"},{"title":"参与进来​","type":1,"pageTitle":"贡献","url":"/cn/docs/contribute/CONTRIBUTING#参与进来","content":"请查看 Issue，是否有你感兴趣的任务。 特别是，如果你刚开始参与，你可以查找带有help wanted或 kind/bug等标签的 Issue，这些是云原生社区中的标准标签。 如果您对其中任何 Issue 感兴趣，请留下评论让我们知道！ 注意： 在开始提交新功能或大幅修改之前，请先提交一个 Issue 或 Discussion。 ","version":"下一个","tagName":"h2"},{"title":"下一步​","type":1,"pageTitle":"贡献","url":"/cn/docs/contribute/CONTRIBUTING#下一步","content":"如果您计划对代码进行贡献，审阅者可能需要您： 遵循良好的编码准则。填写完善的 PR Description 和 Commit Message。将大幅修改拆分为一系列逻辑上较小的补丁，每个补丁包含一个小的、容易理解的修改，这样可以在总体上保证每个 PR 依次递进，项目稳步发展。 如果您希望改进文档，请阅读文档。 ","version":"下一个","tagName":"h2"},{"title":"HwameiStor 成员角色","type":0,"sectionRef":"#","url":"/cn/docs/contribute/membership","content":"","keywords":"","version":"下一个"},{"title":"角色摘要​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#角色摘要","content":"这是我们在 HwameiStor 社区中使用的一组角色，每个角色的一般责任、加入或保留该角色所需的要求，以及角色在权限和特权方面的具体表现。 角色\t责任\t要求\t特权 协作者\t对项目的非正式贡献者\t无 GitHub HwameiStor 组织的外部协作者 可以提交 PR 和 Issue 在 HwameiStor 团队目标上具有读取和评论权限 成员\t社区定期活跃的贡献者 至少向 HwameiStor 代码库推送了一个PR GitHub HwameiStor 组织的成员 HwameiStor 团队目标上的编辑权限 HwameiStor 代码库的审查权限，允许处理 Issue。 审阅者\t帮助改进代码和文档的内容专家\t对 HwameiStor 文档具有丰富经验的贡献者。\t与成员相同，另外还有： 维护者和管理者优先批准他们审查的内容。 维护者\t批准其他成员的贡献\t对某个领域具有丰富经验和活跃的审阅者和贡献者\t与成员相同，另外还有： 能够在 GitHub 上批准代码更改 在工作组决策过程中拥有投票权 负责确保将发布说明和升级说明添加到具有用户可见更改的 PR 中 管理者\t管理和控制权限\t由 HwameiStor 组织任命 对各种与 HwameiStor 相关的资源拥有管理者特权 ","version":"下一个","tagName":"h2"},{"title":"协作者​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#协作者","content":"个人可以作为外部协作者（具有只读权限）添加到 HwameiStor GitHub 组织的代码库中，而无需成为成员。 这使他们可以被分配 Issue 和 PR，直到成为成员为止，但不会自动对他们的 PR 运行测试，也不能与 PR 机器人交互。 ","version":"下一个","tagName":"h2"},{"title":"要求​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#要求","content":"正在进行一些对项目有益的贡献，需要能够将 PR 或 Issue 分配给贡献者。 ","version":"下一个","tagName":"h3"},{"title":"成员​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#成员","content":"成熟的社区成员应该展示他们对本文档中的原则的遵守，熟悉项目的组织、角色、政策、程序、约定等，以及技术和/或写作能力。 成员是社区中持续活跃的贡献者。他们可以被分配 Issue 和 PR，参加工作组会议，并且他们的 PR 会自动运行预提交测试。 成员被期望继续积极参与社区贡献。 鼓励所有成员帮助减轻代码审查的负担，尽管每个 PR 在被接受到源代码库之前必须由一个或多个官方审阅者和维护者进行审查。 ","version":"下一个","tagName":"h2"},{"title":"要求​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#要求-1","content":"在过去的 6 个月内向 HwameiStor 代码库推送了至少一个 PR。持续为一个或多个领域做出贡献。 成员被期望在项目中持续参与。 如果一个人在 180 天内没有为项目做出贡献，他可能会失去成员资格。持续的贡献包括： 成功合并 PR为 Issue 或 PR 进行分类在 Issue 或 PR 上发表评论关闭 Issue 或 PR ","version":"下一个","tagName":"h3"},{"title":"成为成员​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#成为成员","content":"如果您有兴趣成为成员并满足上述要求，您可以将自己添加到members.yaml的成员列表中加入组织。 完成后，提交一个带有更改的 PR，并填写 PR 模板中请求的所有信息。 ","version":"下一个","tagName":"h3"},{"title":"职责和特权​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#职责和特权","content":"对分配给他们的 Issue 和 PR 做出响应对自己贡献的代码负责（除非明确转让所有权） 代码经过充分测试测试结果始终通过处理代码被接受后发现的错误或 Issue 频繁贡献代码的成员应主动为他们活跃的领域进行代码审查。 ","version":"下一个","tagName":"h3"},{"title":"审阅者​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#审阅者","content":"审阅者被信任只批准符合贡献指南中所描述的验收标准的内容。 ","version":"下一个","tagName":"h2"},{"title":"要求​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#要求-2","content":"要成为 审阅者，贡献者必须满足以下 要求： 成为 HwameiStor 社区的成员。在 HwameiStor.io 代码库进行 5 次重要贡献。重要贡献包括以下示例： 新内容内容审查内容改进 展示对文档质量和使用我们的样式指南的坚定承诺。由 HwameiStor 维护者或工作组负责人担保。 ","version":"下一个","tagName":"h3"},{"title":"职责​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#职责","content":"审查 hwameistor/hwameistor 代码库中的 PR。确保相关技术工作组被添加为审阅者，并确保维护者或管理者已经批准了 PR。 ","version":"下一个","tagName":"h3"},{"title":"特权​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#特权","content":"审查者批准的内容会被维护者或管理者优先处理。审阅者可以使用 /lgtm 标签通知维护者加快审批已审阅内容的发布。 审阅者无法将内容合并到 hwameistor/hwameistor 的 main 分支；只有维护者和管理者可以将内容合并到 main 分支。 ","version":"下一个","tagName":"h3"},{"title":"维护者​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#维护者","content":"维护者审核并批准代码贡献。虽然代码审查关注的是代码质量和正确性，但批准则关注对贡献的整体接受， 包括向后/向前兼容性，遵循 API 和标志约定，微妙的性能和正确性问题，与系统其他部分的交互等。 维护者状态适用于代码库的一部分，并在 CODEOWNERS 文件中反映出来。 ","version":"下一个","tagName":"h2"},{"title":"要求​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#要求-3","content":"以下适用于成为维护者的代码库的部分： 成为成员至少 3 个月对代码库做出至少 30 次重要的 PR 贡献必须通过贡献代码、进行审查、处理 Issue 等方式保持对社区的积极参与熟悉代码库由一个工作组负责人赞助，并没有其他负责人的反对意见 如果维护者在项目中长时间处于不活跃状态，该人将成为名誉维护者。名誉维护者失去了批准代码贡献的能力，但保留其一年的投票权。 一年后，名誉维护者恢复成为没有投票权的普通成员。 维护者通过以下方式为其负责的项目部分做出贡献： 成功合并 PR处理 Issue 或 PR关闭 Issue 或 PR ","version":"下一个","tagName":"h3"},{"title":"职责和特权​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#职责和特权-1","content":"以下适用于成为维护者的代码库的部分： 维护者状态可能是接受大型代码贡献的前提条件展示出良好的技术判断力通过代码审查负责项目质量控制 关注代码质量和正确性，包括测试和重构关注贡献的整体接受，例如与其他功能的依赖关系、向后/向前兼容性、API 和标志定义等 预期根据社区期望，对审查请求做出响应可以批准代码贡献的接受在需要做出决策时，这个职位的维护者有一票。 ","version":"下一个","tagName":"h3"},{"title":"管理者​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#管理者","content":"管理者负责项目的行政方面。 ","version":"下一个","tagName":"h2"},{"title":"要求​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#要求-4","content":"由 HwameiStor 组织任命。 ","version":"下一个","tagName":"h3"},{"title":"职责和特权​","type":1,"pageTitle":"HwameiStor 成员角色","url":"/cn/docs/contribute/membership#职责和特权-2","content":"管理 HwameiStor 项目的各种基础设施支持尽管管理者可能有权利覆盖任何政策并忽略某些规定，但我们希望管理者通常遵守项目的总体规则。 例如，除非绝对必要，管理者不应批准和/或提交他们没有资格的 PR。 ","version":"下一个","tagName":"h3"},{"title":"HwameiStor 能力、安全及运维测试","type":0,"sectionRef":"#","url":"/cn/blog/test","content":"","keywords":"","version":null},{"title":"TiDB 简介​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#tidb-简介","content":"TiDB 是一款同时支持在线事务处理 (OLTP) 与在线分析处理 (OATP) 的融合型分布式数据库产品，具备水平扩缩容、金融级高可用、实时 HTAP（即同时支持 OLTP 和 OATP）、云原生的分布式数据库，兼容 MySQL 5.7 协议和 MySQL 生态等重要特性。TiDB 的目标是为用户提供一站式的 OLTP、OLAP、HTAP 解决方案，适合高可用、强一致要求较高、数据规模较大等各种应用场景。 ","version":null,"tagName":"h2"},{"title":"TiDB 整体架构​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#tidb-整体架构","content":"TiDB 分布式数据库将整体架构拆分成了多个模块，各模块之间互相通信，组成完整的 TiDB 系统。对应的架构图如下：  TiDB Server SQL 层对外暴露 MySQL 协议的连接端点，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 LVS、HAProxy 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。 PD (Placement Driver) Server 整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。 存储节点 TiKV Server：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。 TiFlash：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。 ","version":null,"tagName":"h3"},{"title":"TiDB 数据库的存储​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#tidb-数据库的存储","content":" 键值对 (Key-Value Pair) TiKV 的选择是 Key-Value 模型，并且提供有序遍历方法。TiKV 数据存储的两个关键点： 这是一个巨大的 Map（可以类比一下 C++ 的 std::map），也就是存储的是 Key-Value Pairs。 这个 Map 中的 Key-Value pair 按照 Key 的二进制顺序有序，也就是可以 Seek 到某一个 Key 的位置，然后不断地调用 Next 方法以递增的顺序获取比这个 Key 大的 Key-Value。 本地存储（Rocks DB） 任何持久化的存储引擎，数据终归要保存在磁盘上，TiKV 也不例外。但是 TiKV 没有选择直接向磁盘上写数据，而是把数据保存在 RocksDB 中，具体的数据落地由 RocksDB 负责。这样做的原因是开发一个单机存储引擎工作量很大，特别是要做一个高性能的单机引擎，需要做各种细致的优化，而 RocksDB 是由 Facebook 开源的一个非常优秀的单机 KV 存储引擎，可以满足 TiKV 对单机引擎的各种要求。这里可以简单地认为 RocksDB 是一个单机的持久化 Key-Value Map。 Raft 协议 TiKV 选择了 Raft 算法来保证单机失效的情况下数据不丢失不出错。简单来说，就是把数据复制到多台机器上，这样某一台机器无法提供服务时，其他机器上的副本还能提供服务。这个数据复制方案可靠并且高效，能处理副本失效的情况。 Region TiKV 选择了按照 Key 划分 Range。某一段连续的 Key 都保存在一个存储节点上。将整个 Key-Value 空间分成很多段，每一段是一系列连续的 Key，称为一个 Region。尽量让每个 Region 中保存的数据不超过一定的大小，目前在 TiKV 中默认是不超过 96MB。每一个 Region 都可以用 [StartKey，EndKey] 这样的左闭右开区间来描述。 MVCC TiKV实现了多版本并发控制 (MVCC)。 分布式 ACID 事务 TiKV 的事务采用的是 Google 在 BigTable 中使用的事务模型：Percolator。 ","version":null,"tagName":"h3"},{"title":"搭建测试环境​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#搭建测试环境","content":"","version":null,"tagName":"h2"},{"title":"Kubernetes 集群​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#kubernetes-集群","content":"本次测试使用三台虚拟机节点部署 Kubernetes 集群，包括 1 个 master 节点和 2 个 worker节点。Kubelete 版本为 1.22.0。  ","version":null,"tagName":"h3"},{"title":"HwameiStor 本地存储​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#hwameistor-本地存储","content":"在 Kubernetes 集群上部署 HwameiStor 本地存储 在两台 worker 节点上分别为 HwameiStor 配置一块 100G 的本地磁盘 sdb 创建 storagClass ","version":null,"tagName":"h3"},{"title":"在 Kubernetes 上部署 TiDB​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#在-kubernetes-上部署-tidb","content":"可以使用 TiDB Operator 在 Kubernetes 上部署 TiDB。TiDB Operator 是 Kubernetes 上的 TiDB 集群自动运维系统，提供包括部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或私有部署的 Kubernetes 集群上。 TiDB 与 TiDB Operator 版本的对应关系如下： TiDB 版本\t适用的 TiDB Operator 版本dev\tdev TiDB &gt;= 5.4\t1.3 5.1 &lt;= TiDB &lt; 5.4\t1.3（推荐），1.2 3.0 &lt;= TiDB &lt; 5.1\t1.3（推荐），1.2，1.1 2.1 &lt;= TiDB &lt; 3.0\t1.0（停止维护） 部署 TiDB Operator​ 安装 TiDB CRDs kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/crd.yaml 安装 TiDB Operator helm repo add pingcap https://charts.pingcap.org/ kubectl create namespace tidb-admin helm install --namespace tidb-admin tidb-operator pingcap/tidb-operator --version v1.3.2 \\ --set operatorImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-operator:v1.3.2 \\ --set tidbBackupManagerImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-backup-manager:v1.3.2 \\ --set scheduler.kubeSchedulerImageName=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler 检查 TiDB Operator 组件 部署 TiDB 集群​ kubectl create namespace tidb-cluster &amp;&amp; \\ kubectl -n tidb-cluster apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/basic/tidb-cluster.yaml kubectl -n tidb-cluster apply -f https://raw.githubusercontent.com /pingcap/tidb-operator/master/examples/basic/tidb-monitor.yaml   连接 TiDB 集群​ yum -y install mysql-client   kubectl port-forward -n tidb-cluster svc/basic-tidb 4000 &gt; pf4000.out &amp;     检查并验证 TiDB 集群状态​ 创建 Hello_world 表 create table hello_world (id int unsigned not null auto_increment primary key, v varchar(32)); 查询 TiDB 版本号 select tidb_version()\\G; 查询 Tikv 存储状态 select * from information_schema.tikv_store_status\\G;  HwameiStor 存储配置​ 从 storageClass local-storage-hdd-lvm 分别为 tidb-tikv 及 tidb-pd 创建一个 PVC:    kubectl get po basic-tikv-0 -oyaml   kubectl get po basic-pd-0 -oyaml   ","version":null,"tagName":"h3"},{"title":"测试内容​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#测试内容","content":"","version":null,"tagName":"h2"},{"title":"数据库 SQL 基本能力测试​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#数据库-sql-基本能力测试","content":"完成部署数据库集群后，执行了以下基本能力测试，全部通过。 分布式事务​ 测试目的：支持在多种隔离级别下，实现分布式数据操作的完整性约束即 ACID属性 测试步骤： 创建测试数据库 CREATE DATABASE testdb 创建测试用表 CREATE TABLE t_test ( id int AUTO_INCREMENT, name varchar(32), PRIMARY KEY (id) ) 运行测试脚本 测试结果：支持在多种隔离级别下，实现分布式数据操作的完整性约束即 ACID 属性 对象隔离​ 测试目的：测试不同 schema 实现对象隔离 测试脚本： create database if not exists testdb; use testdb create table if not exists t_test ( id bigint, name varchar(200), sale_time datetime default current_timestamp, constraint pk_t_test primary key (id) ); insert into t_test(id,name) values (1,'a'),(2,'b'),(3,'c'); create user 'readonly'@'%' identified by &quot;readonly&quot;; grant select on testdb.* to readonly@'%'; select * from testdb.t_test; update testdb.t_test set name='aaa'; create user 'otheruser'@'%' identified by &quot;otheruser&quot;;  测试结果：支持创建不同 schema 实现对象隔离 表操作支持​ 测试目的：测试是否支持创建、删除和修改表数据、DML、列、分区表 测试步骤：连接数据库后按步骤执行测试脚本 测试脚本： # 创建和删除表 drop table if exists t_test; create table if not exists t_test ( id bigint default '0', name varchar(200) default '' , sale_time datetime default current_timestamp, constraint pk_t_test primary key (id) ); # 删除和修改 insert into t_test(id,name) values (1,'a'),(2,'b'),(3,'c'),(4,'d'),(5,'e'); update t_test set name='aaa' where id=1; update t_test set name='bbb' where id=2; delete from t_dml where id=5; # 修改、增加、删除列 alter table t_test modify column name varchar(250); alter table t_test add column col varchar(255); insert into t_test(id,name,col) values(10,'test','new_col'); alter table t_test add column colwithdefault varchar(255) default 'aaaa'; insert into t_test(id,name) values(20,'testdefault'); insert into t_test(id,name,colwithdefault ) values(10,'test','non-default '); alter table t_test drop column colwithdefault; # 分区表类型（仅摘录部分脚本） CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT NOT NULL, store_id INT NOT NULL )  测试结果：支持创建、删除和修改表数据、DML、列、分区表 索引支持​ 测试目的：验证多种类型的索引（唯一、聚簇、分区、Bidirectional indexes、Expression-based indexes、哈希索引等等）以及索引重建操作。 测试脚本： alter table t_test add unique index udx_t_test (name); # 默认就是主键聚簇索引 ADMIN CHECK TABLE t_test; create index time_idx on t_test(sale_time); alter table t_test drop index time_idx; admin show ddl jobs; admin show ddl job queries 156; create index time_idx on t_test(sale_time);  测试结果：支持创建、删除、组合、单列、唯一索引 表达式​ 测试目的：验证分布式数据库的表达式支持 if、casewhen、forloop、whileloop、loop exit when 等语句（上限 5 类） 前提条件：数据库集群已经部署完成。 测试脚本： SELECT CASE id WHEN 1 THEN 'first' WHEN 2 THEN 'second' ELSE 'OTHERS' END AS id_new FROM t_test; SELECT IF(id&gt;2,'int2+','int2-') from t_test;  测试结果：支持 if、case when、for loop、while loop、loop exit when 等语句（上限 5 类） 执行计划解析​ 测试目的：验证分布式数据库的执行计划解析支持 前提条件：数据库集群已经部署完成。 测试脚本： explain analyze select * from t_test where id NOT IN (1,2,4); explain analyze select * from t_test a where EXISTS (select * from t_test b where a.id=b.id and b.id&lt;3); explain analyze SELECT IF(id&gt;2,'int2+','int2-') from t_test;  测试结果：支持执行计划的解析 执行计划绑定​ 测试目的：验证分布式数据库的执行计划绑定功能 测试步骤： 查看 sql 语句的当前执行计划 使用绑定特性 查看该 sql 语句绑定后的执行计划 删除绑定 测试脚本： explain select * from employees3 a join employees4 b on a.id = b.id where a.lname='Johnson'; explain select /*+ hash_join(a,b) */ * from employees3 a join employees4 b on a.id = b.id where a.lname='Johnson';  测试结果：没有使用 hint 时可能不是 hash_join，使用 hint 后一定是 hash_join。 常用函数​ 测试目的：验证分布式数据库的标准的数据库函数(支持的函数类型） 测试结果：支持标准的数据库函数 显式/隐式事务​ 测试目的：验证分布式数据库的事务支持 测试结果：支持显示与隐式事务 字符集​ 测试目的：验证分布式数据库的数据类型支持 测试结果：目前只支持 UTF-8 mb4 字符集 锁支持​ 测试目的：验证分布式数据库的锁实现 测试结果：描述了锁的实现方式，R-R/R-W/W-W 情况下阻塞情况，死锁处理方式 隔离级别​ 测试目的：验证分布式数据库的事务隔离级别 测试结果：支持 si 隔离级别，支持 rc 隔离级别（4.0 GA 版本） 分布式复杂查询​ 测试目的：验证分布式数据库的分布式复杂查询能力 测试结果：支持跨节点 join 等分布式复杂查询、操作等，支持窗口函数、层次查询 ","version":null,"tagName":"h3"},{"title":"系统安全测试​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#系统安全测试","content":"这部分测试系统安全，完成数据库集群部署后，以下安全测试全部通过。 账号管理与权限测试​ 测试目的：验证分布式数据库的账号权限管理 测试脚本： select host,user,authentication_string from mysql.user; create user tidb IDENTIFIED by 'tidb'; select host,user,authentication_string from mysql.user; set password for tidb =password('tidbnew'); select host,user,authentication_string,Select_priv from mysql.user; grant select on *.* to tidb; flush privileges ; select host,user,authentication_string,Select_priv from mysql.user; grant all privileges on *.* to tidb; flush privileges ; select * from mysql.user where user='tidb'; revoke select on *.* from tidb; flush privileges ; revoke all privileges on *.* from tidb; flush privileges ; grant select(id) on test.TEST_HOTSPOT to tidb; drop user tidb;  测试结果： 支持创建、修改删除账号，并配置和密码，支持安全、审计和数据管理三权分立 根据不同账号，对数库各个级别权限控制包括：实例/库/表/列级别 访问控制​ 测试目的：验证分布式数据库的权限访问控制，数据库数据通过赋予基本增删改查访问权限控制 测试脚本： mysql -u root -h 172.17.49.222 -P 4000 drop user tidb; drop user tidb1; create user tidb IDENTIFIED by 'tidb'; grant select on tidb.* to tidb; grant insert on tidb.* to tidb; grant update on tidb.* to tidb; grant delete on tidb.* to tidb; flush privileges; show grants for tidb; exit; mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'select * from aa;' mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'insert into aa values(2);' mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'update aa set id=3;' mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'delete from aa where id=3;'  测试结果：数据库数据通过赋予基本增删改查访问权限控制。 白名单​ 测试目的：验证分布式数据库的白名单功能 测试脚本： mysql -u root -h 172.17.49.102 -P 4000 drop user tidb; create user tidb@'127.0.0.1' IDENTIFIED by 'tidb'; flush privileges; select * from mysql.user where user='tidb'; mysql -u tidb -h 127.0.0.1 -P 4000 -ptidb mysql -u tidb -h 172.17.49.102 -P 4000 -ptidb  测试结果：支持 IP 白名单功能，支持 IP 段通配操作 操作日志记录​ 测试目的：验证分布式数据库的操作监控能力 测试脚本：kubectl -ntidb-cluster logs tidb-test-pd-2 --tail 22 测试结果：记录用户通过运维管理控制台或者 API 执行的关键操作或者错误操作 ","version":null,"tagName":"h3"},{"title":"运维管理测试​","type":1,"pageTitle":"HwameiStor 能力、安全及运维测试","url":"/cn/blog/test#运维管理测试","content":"这部分测试系统运维，完成数据库集群部署后，以下运维管理测试全部通过。 数据导入导出​ 测试目的：验证分布式数据库的数据导入导出的工具支持 测试脚本： select * from sbtest1 into outfile '/sbtest1.csv'; load data local infile '/sbtest1.csv' into table test100;  测试结果：支持按表、schema、database 级别的逻辑导出导入 慢日志查询​ 测试目的：获取慢查询的 SQL 信息 前提条件：SQL 执行时间需大于配置的慢查询记录阈值,且 SQL 执行完毕 测试步骤： 调整慢查询阈值到 100ms 执行 sql 查看 log/系统表/dashboard 中的慢查询信息 测试脚本： show variables like 'tidb_slow_log_threshold'; set tidb_slow_log_threshold=100; select query_time, query from information_schema.slow_query where is_internal = false order by query_time desc limit 3;  测试结果：可以获取慢查询信息 有关测试详情，请查阅 TiDB on hwameiStor 部署及测试记录。 ","version":null,"tagName":"h3"},{"title":"故障恢复","type":0,"sectionRef":"#","url":"/cn/docs/fast_failover","content":"","keywords":"","version":"下一个"},{"title":"使用方式​","type":1,"pageTitle":"故障恢复","url":"/cn/docs/fast_failover#使用方式","content":"HwameiStor 为两类情况提供了应用故障快速恢复机制： 节点出现故障 在这种情况下，该节点上的应用均无法正常运行。对于使用 HwameiStor 数据卷的应用，需要及时地将 Pod 重新调度到新的健康节点。 您可以通过下列方式进行故障恢复： 为该节点打标签（Label）： kubectl label node &lt;nodeName&gt; hwameistor.io/failover=start 当故障恢复完成后，上面的标签会变成： hwameistor.io/failover=completed 应用 Pod 出现故障 在这种情况下，您可以为该 Pod 打标签（Label）对 Pod 进行故障恢复： kubectl label pod &lt;podName&gt; hwameistor.io/failover=start 当故障恢复完成后，旧的 Pod 会被删除，新的 Pod 会在新的节点上启动并正常运行。 ","version":"下一个","tagName":"h2"},{"title":"通过 HwameiStor-Operator 安装","type":0,"sectionRef":"#","url":"/cn/docs/install/operator","content":"","keywords":"","version":"下一个"},{"title":"安装步骤​","type":1,"pageTitle":"通过 HwameiStor-Operator 安装","url":"/cn/docs/install/operator#安装步骤","content":"添加 hwameistor-operator Helm Repo helm repo add hwameistor-operator https://hwameistor.io/hwameistor-operator helm repo update hwameistor-operator 通过 hwameistor-operator 部署 HwameiStor 注意 如果没有可用的干净磁盘，Operator 就不会自动创建 StorageClass。 Operator 会在安装过程中自动纳管磁盘，可用的磁盘会被添加到 LocalStorage 的 pool 里。 如果可用磁盘是在安装后提供的，则需要手动下发 LocalDiskClaim 将磁盘纳管到 LocalStorageNode 里。 一旦 LocalStorageNode 的 pool 里有磁盘，Operator 就会自动创建 StorageClass。 也就是说，如果没有容量，就不会自动创建 StorageClass。 helm install hwameistor-operator hwameistor-operator/hwameistor-operator -n hwameistor --create-namespace  可选参数： 磁盘预留 可用的干净磁盘默认会被纳管并且添加到 LocalStorageNode 的 pool 里。 如果您想在安装前预留一部分磁盘留作他用，您可以通过 helm 的 values 来设置磁盘预留配置。 方式 1： helm install hwameistor-operator hwameistor-operator/hwameistor-operator -n hwameistor --create-namespace \\ --set diskReserve\\[0\\].nodeName=node1 \\ --set diskReserve\\[0\\].devices={/dev/sdc\\,/dev/sdd} \\ --set diskReserve\\[1\\].nodeName=node2 \\ --set diskReserve\\[1\\].devices={/dev/sdc\\,/dev/sde} 这个例子展示了在 helm install 时通过--set 选项来设置磁盘预留配置，可能比较棘手。我们更建议把磁盘预留的配置写到一个文件里。 方式 2： diskReserve: - nodeName: node1 devices: - /dev/sdc - /dev/sdd - nodeName: node2 devices: - /dev/sdc - /dev/sde 比如，您可以把如上的 helm values 写到一个叫 diskReserve.yaml 的文件里，并在 helm install 时 apply。 helm install hwameistor-operator hwameistor-operator/hwameistor-operator -n hwameistor --create-namespace -f diskReserve.yaml 开启验证： helm install hwameistor-operator hwameistor-operator/hwameistor-operator -n hwameistor --create-namespace \\ --set apiserver.authentication.enable=true \\ --set apiserver.authentication.accessId={用户名} \\ --set apiserver.authentication.secretKey={密码} 您也可以在安装后通过修改 deployment/apiserver 来开启验证。 使用国内源： helm install hwameistor-operator hwameistor-operator/hwameistor-operator -n hwameistor --create-namespace \\ --set global.hwameistorImageRegistry=ghcr.m.daocloud.io \\ --set global.k8sImageRegistry=m.daocloud.io/registry.k8s.io  ","version":"下一个","tagName":"h2"},{"title":"卸载 (仅用于测试环境)","type":0,"sectionRef":"#","url":"/cn/docs/install/uninstall","content":"","keywords":"","version":"下一个"},{"title":"卸载但保留已有数据卷​","type":1,"pageTitle":"卸载 (仅用于测试环境)","url":"/cn/docs/install/uninstall#卸载但保留已有数据卷","content":"如果想要卸载 HwameiStor 的系统组件，但是保留已经创建的数据卷并服务于数据应用，采用下列方式： $ kubectl get cluster.hwameistor.io NAME AGE cluster-sample 21m $ kubectl delete clusters.hwameistor.io hwameistor-cluster  最终，所有的 HwameiStor 系统组件（Pods）将被删除。用下列命令检查，结果为空。 kubectl -n hwameistor get pod  ","version":"下一个","tagName":"h2"},{"title":"卸载并删除已有数据卷​","type":1,"pageTitle":"卸载 (仅用于测试环境)","url":"/cn/docs/install/uninstall#卸载并删除已有数据卷","content":"危险 在卸载之前，请确认所有数据都可以被删除。 如果想要卸载 HwameiStor 所有组件，并删除所有数据卷及数据，采用下列方式： 清理有状态数据应用。 删除应用。 删除数据卷 PVC。 相关的 PV、LV、LVR、LVG 都将被删除. 清理 HwameiStor 系统组件。 删除 HwameiStor 组件。 kubectl delete clusters.hwameistor.io hwameistor-cluster 删除 HwameiStor 系统空间。 kubectl delete ns hwameistor 删除 CRD、Hook 以及 RBAC。 kubectl get crd,mutatingwebhookconfiguration,clusterrolebinding,clusterrole -o name \\ | grep hwameistor \\ | xargs -t kubectl delete 删除 StorageClass。 kubectl get sc -o name \\ | grep hwameistor-storage-lvm- \\ | xargs -t kubectl delete 删除 hwameistor-operator。 helm uninstall hwameistor-operator -n hwameistor 最后，您仍然需要清理每个节点上的 LVM 配置，并采用额外的系统工具 （例如 wipefs）清除磁盘上的所有数据。 wipefs -a /dev/sdx blkid /dev/sdx  ","version":"下一个","tagName":"h2"},{"title":"准备工作","type":0,"sectionRef":"#","url":"/cn/docs/install/prereq","content":"","keywords":"","version":"下一个"},{"title":"Kubernetes 平台​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#kubernetes-平台","content":"Kubernetes 1.18+部署 CoreDNS ","version":"下一个","tagName":"h2"},{"title":"不支持的平台​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#不支持的平台","content":"OpenshiftRancher 注意 暂时不支持以上平台，但是计划未来支持。 ","version":"下一个","tagName":"h3"},{"title":"主机配置​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#主机配置","content":"","version":"下一个","tagName":"h2"},{"title":"支持的 Linux 发行版​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#支持的-linux-发行版","content":"CentOS/RHEL 7.4+Rocky Linux 8.4+Ubuntu 18+麒麟 V10 ","version":"下一个","tagName":"h3"},{"title":"支持的处理器架构​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#支持的处理器架构","content":"x86_64ARM64 ","version":"下一个","tagName":"h3"},{"title":"必需的软件依赖​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#必需的软件依赖","content":"已安装 LVM2 对于高可用功能，需要安装和当前运行的 kernel 版本一致的 kernel-devel 高可用功能模块在部分内核版本的节点上无法自动安装，需要手动安装 点击查看已确认可适配的内核版本 5.4.247-1.el7.elrepo 5.14.0-284.11.1.el9_2 5.15.0-204.147.6.2.el8uek --------------------------------------------------- 5.8.0-1043-azure 5.8.0-1042-azure 5.8.0-1041-azure 5.4.17-2102.205.7.2.el7uek 5.4.17-2011.0.7.el8uek 5.4.0-91 5.4.0-90 5.4.0-89 5.4.0-88 5.4.0-86 5.4.0-84 5.4.0-1064-azure 5.4.0-1063-azure 5.4.0-1062-azure 5.4.0-1061-azure 5.4.0-1060-aws 5.4.0-1059-azure 5.4.0-1059-aws 5.4.0-1058-azure 5.4.0-1058-aws 5.4.0-1057-aws 5.4.0-1056-aws 5.4.0-1055-aws 5.3.18-57.3 5.3.18-22.2 5.14.0-1.7.1.el9 5.11.0-1022-azure 5.11.0-1022-aws 5.11.0-1021-azure 5.11.0-1021-aws 5.11.0-1020-azure 5.11.0-1020-aws 5.11.0-1019-aws 5.11.0-1017-aws 5.11.0-1016-aws 5.10.0-8 5.10.0-7 5.10.0-6 4.9.215-36.el7 4.9.212-36.el7 4.9.206-36.el7 4.9.199-35.el7 4.9.188-35.el7 4.4.92-6.30.1 4.4.74-92.38.1 4.4.52-2.1 4.4.27-572.565306 4.4.0-217 4.4.0-216 4.4.0-214 4.4.0-213 4.4.0-210 4.4.0-1133-aws 4.4.0-1132-aws 4.4.0-1131-aws 4.4.0-1128-aws 4.4.0-1121-aws 4.4.0-1118-aws 4.19.19-5.0.8 4.19.0-8 4.19.0-6 4.19.0-5 4.19.0-16 4.18.0-80.1.2.el8_0 4.18.0-348.el8 4.18.0-305.el8 4.18.0-240.1.1.el8_3 4.18.0-193.el8 4.18.0-147.el8 4.15.0-163 4.15.0-162 4.15.0-161 4.15.0-159 4.15.0-158 4.15.0-156 4.15.0-112-lowlatency 4.15.0-1113-azure 4.15.0-1040-azure 4.15.0-1036-azure 4.14.35-2047.502.5.el7uek 4.14.35-1902.4.8.el7uek 4.14.35-1818.3.3.el7uek 4.14.248-189.473.amzn2 4.14.128-112.105.amzn2 4.13.0-1018-azure 4.12.14-95.3.1 4.12.14-25.25.1 4.12.14-197.29 4.12.14-120.1 4.1.12-124.49.3.1.el7uek 4.1.12-124.26.3.el6uek 4.1.12-124.21.1.el6uek 3.10.0-957.el7 3.10.0-862.el7 3.10.0-693.el7 3.10.0-693.21.1.el7 3.10.0-693.17.1.el7 3.10.0-514.6.2.el7 3.10.0-514.36.5.el7 3.10.0-327.el7 3.10.0-229.1.2.el7 3.10.0-123.20.1.el7 3.10.0-1160.el7 3.10.0-1127.el7 3.10.0-1062.el7 3.10.0-1049.el7 3.0.101-108.13.1 2.6.32-754.el6 2.6.32-696.el6 2.6.32-696.30.1.el6 2.6.32-696.23.1.el6 2.6.32-642.1.1.el6 2.6.32-573.1.1.el6 2.6.32-504.el6 数据卷扩容功能需要安装文件系统大小调整工具。使用 xfs 作为默认文件系统。因此节点上面需要安装 xfs_growfs。 CentOS/RHEL、Rocky 和 KylinUbuntu yum install -y lvm2 yum install -y kernel-devel-$(uname -r) yum install -y xfsprogs  ","version":"下一个","tagName":"h3"},{"title":"Secure Boot​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#secure-boot","content":"高可用功能暂时不支持 Secure Boot，确认 Secure Boot 是 disabled 状态： $ mokutil --sb-state SecureBoot disabled $ dmesg | grep secureboot [ 0.000000] secureboot: Secure boot disabled  ","version":"下一个","tagName":"h3"},{"title":"数据盘​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#数据盘","content":"HwameiStor 支持物理硬盘 (HDD)、固态硬盘 (SSD) 和 NVMe 闪存盘. 测试环境里，每个主机必须要有至少一块空闲的 10GiB 数据盘。 生产环境里，建议每个主机至少要有一块空闲的 200GiB 数据盘，而且建议使用固态硬盘 (SSD)。 注意 对于虚拟机环境，请确保每台虚拟机已经启用磁盘序列号的功能，这会帮助 HwameiStor 更好的识别管理主机上的磁盘。 为了避免磁盘识别冲突，请确保提供的虚拟磁盘序列号不能重复。 ","version":"下一个","tagName":"h3"},{"title":"网络​","type":1,"pageTitle":"准备工作","url":"/cn/docs/install/prereq#网络","content":"生产环境里，开启高可用模式后，建议使用有冗余保护的万兆 TCP/IP 网络。 ","version":"下一个","tagName":"h3"},{"title":"什么是 HwameiStor","type":0,"sectionRef":"#","url":"/cn/docs/intro","content":"","keywords":"","version":"下一个"},{"title":"功能特性​","type":1,"pageTitle":"什么是 HwameiStor","url":"/cn/docs/intro#功能特性","content":"自动化运维管理 可以自动发现、识别、管理、分配磁盘。 根据亲和性，智能调度应用和数据。自动监测磁盘状态，并及时预警。 高可用的数据 使用跨节点副本同步数据， 实现高可用。发生问题时，会自动将应用调度到高可用数据节点上，保证应用的连续性。 丰富的数据卷类型 聚合 HDD、SSD、NVMe 类型的磁盘，提供非低延时，高吞吐的数据服务。 灵活动态的线性扩展 可以根据集群规模大小进行动态的扩容，灵活满足应用的数据持久化需求。 ","version":"下一个","tagName":"h2"},{"title":"常见问题","type":0,"sectionRef":"#","url":"/cn/docs/faqs","content":"","keywords":"","version":"下一个"},{"title":"Q1：HwameiStor 本地存储调度器 scheduler 在 Kubernetes 平台中是如何工作的？​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q1hwameistor-本地存储调度器-scheduler-在-kubernetes-平台中是如何工作的","content":"HwameiStor 的调度器是以 Pod 的形式部署在 HwameiStor 的命名空间。  当应用（Deployment 或 StatefulSet）被创建后，应用的 Pod 会被自动部署到已配置好具备 HwameiStor 本地存储能力的 Worker 节点上。 ","version":"下一个","tagName":"h2"},{"title":"Q2: HwameiStor 如何应对应用多副本工作负载的调度？与传统通用型共享存储有什么不同？​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q2-hwameistor-如何应对应用多副本工作负载的调度与传统通用型共享存储有什么不同","content":"HwameiStor 建议使用有状态的 StatefulSet 用于多副本的工作负载。 有状态应用 StatefulSet 会将复制的副本部署到同一 Worker 节点，但会为每一个 Pod 副本创建一个对应的 PV 数据卷。 如果需要部署到不同节点分散 workload，需要通过 pod affinity 手动配置。  由于无状态应用 Deployment 不能共享 block 数据卷，所以建议使用单副本。 ","version":"下一个","tagName":"h2"},{"title":"Q3: 如何运维一个 Kubernetes 节点?​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q3-如何运维一个-kubernetes-节点","content":"HwameiStor 提供了数据卷驱逐和迁移功能。在移除或者重启一个 Kubernetes 节点的时候， 可以将该节点上的 Pod 和数据卷自动迁移到其他可用节点上，并确保 Pod 持续运行并提供服务。 ","version":"下一个","tagName":"h2"},{"title":"移除节点​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#移除节点","content":"为了确保 Pod 的持续运行，以及保证 HwameiStor 本地数据持续可用，在移除 Kubernetes 节点之前， 需要将该节点上的 Pod 和本地数据卷迁移至其他可用节点。可以通过下列步骤进行操作： 排空节点 kubectl drain NODE --ignore-daemonsets=true. --ignore-daemonsets=true 该命令可以将节点上的 Pod 驱逐，并重新调度。同时，也会自动触发 HwameiStor 的数据卷驱逐行为。 HwameiStor 会自动将该节点上的所有数据卷副本迁移到其他节点，并确保数据仍然可用。 检查迁移进度。 kubectl get localstoragenode NODE -o yaml apiVersion: hwameistor.io/v1alpha1 kind: LocalStorageNode metadata: name: NODE spec: hostname: NODE storageIP: 10.6.113.22 topogoly: region: default zone: default status: ... pools: LocalStorage_PoolHDD: class: HDD disks: - capacityBytes: 17175674880 devPath: /dev/sdb state: InUse type: HDD freeCapacityBytes: 16101933056 freeVolumeCount: 999 name: LocalStorage_PoolHDD totalCapacityBytes: 17175674880 totalVolumeCount: 1000 type: REGULAR usedCapacityBytes: 1073741824 usedVolumeCount: 1 volumeCapacityBytesLimit: 17175674880 ## **** 确保 volumes 字段为空 **** ## volumes: state: Ready 同时，HwameiStor 会自动重新调度被驱逐的 Pod，将它们调度到有效数据卷所在的节点上，并确保 Pod 正常运行。 从集群中移除节点 kubectl delete nodes NODE  ","version":"下一个","tagName":"h3"},{"title":"重启节点​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#重启节点","content":"重启节点通常需要很长的时间才能将节点恢复正常。在这期间，该节点上的所有 Pod 和本地数据都无法正常运行。 这种情况对于一些应用（例如，数据库）来说，会产生巨大的代价，甚至不可接受。 HwameiStor 可以立即将 Pod 调度到其他数据卷所在的可用节点，并持续运行。 对于使用 HwameiStor 多副本数据卷的 Pod，这一过程会非常迅速，大概需要 10 秒 （受 Kubernetes 的原生调度机制影响）；对于使用单副本数据卷的 Pod， 这个过程所需时间依赖于数据卷迁移所需时间，受用户数据量大小影响。 如果用户希望将数据卷保留在该节点上，在节点重启后仍然可以访问，可以在节点上添加下列标签， 阻止系统迁移该节点上的数据卷。系统仍然会立即将 Pod 调度到其他有数据卷副本的节点上。 添加一个标签（可选）。 如果在节点重新启动期间不需要迁移数据卷，你可以在排空（drain）节点之前将以下标签添加到该节点。 kubectl label node NODE hwameistor.io/eviction=disable 排空节点。 kubectl drain NODE --ignore-daemonsets=true. --ignore-daemonsets=true 如果执行了第 1 步，待第 2 步成功后，用户即可重启节点。如果没有执行第 1 步，待第 2 步成功后，用户察看数据迁移是否完成（方法如同“移除节点”的第 2 步）。 待数据迁移完成后，即可重启节点。 在前两步成功之后，用户可以重启节点，并等待节点系统恢复正常。 节点恢复至 Kubernetes 的正常状态。 kubectl uncordon NODE  ","version":"下一个","tagName":"h3"},{"title":"对于传统通用型共享存储​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#对于传统通用型共享存储","content":"有状态应用 statefulSet 会将复制的副本优先部署到其他节点以分散 workload，但会为每一个 Pod 副本创建一个对应的 PV 数据卷。 只有当副本数超过 Worker 节点数的时候会出现多个副本在同一个节点。 无状态应用 deployment 会将复制的副本优先部署到其他节点以分散 workload，并且所有的 Pod 共享一个 PV 数据卷 （目前仅支持 NFS）。只有当副本数超过 Worker 节点数的时候会出现多个副本在同一个节点。对于 block 存储， 由于数据卷不能共享，所以建议使用单副本。 ","version":"下一个","tagName":"h3"},{"title":"Q4: LocalStorageNode 查看出现报错如何处理？​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q4-localstoragenode-查看出现报错如何处理","content":"当查看 LocalStorageNode出现如下报错：  可能的错误原因： 节点没有安装 LVM2，可通过如下命令进行安装： rpm -qa | grep lvm2 # 确认 LVM2 是否安装 yum install lvm2 # 在每个节点上确认 LVM 已安装 确认节点上对应磁盘的 GPT 分区： blkid /dev/sd* # 确认磁盘分区是否干净 wipefs -a /dev/sd* # 磁盘清理  ","version":"下一个","tagName":"h2"},{"title":"Q5: 使用 Hwameistor-operator 安装后为什么没有自动创建StorageClasses​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q5-使用-hwameistor-operator-安装后为什么没有自动创建storageclasses","content":"可能的原因： 节点没有可自动纳管的剩余裸盘，可通过如下命令进行检查： kubectl get ld # 检查磁盘 kubectl get lsn &lt;node-name&gt; -o yaml # 检查磁盘是否被正常纳管 hwameistor相关组件【不包含drbd-adapter】没有正常工作，可通过如下命令进行检查： drbd-adapter 组件只有在 HA 启用时候才生效，如果没有启用，可以忽略相关错误 kubectl get pod -n hwameistor # 确认 pod 是否运行正常 kubectl get hmcluster -o yaml # 查看 health 字段  ","version":"下一个","tagName":"h2"},{"title":"Q6: 如何手动扩容存储池容量？​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q6-如何手动扩容存储池容量","content":"什么时候需要手动扩容: 需要使用磁盘分区 (Issue #1387)不同的磁盘使用了相同的序列号 (Issue #1450,Issue #1449) 执行 lsblk -o +SERIAL 命令查看磁盘序列号。 手动扩容步骤: 创建并扩容存储池 vgcreate LocalStorage_PoolHDD /dev/sdb LocalStorage_PoolHDD 是 HDD 磁盘类型的存储池名称，其他可选名称有LocalStorage_PoolSSD 用于 SSD 类型，LocalStorage_PoolNVMe 用于 NVMe 类型。 如果需要使用一个磁盘分区来扩容存储池，可以使用下面的命令: vgcreate LocalStorage_PoolHDD /dev/sdb1 如果存储池已经存在，可以使用下面的命令来扩容存储池: vgextend LocalStorage_PoolHDD /dev/sdb1 检查节点的存储池状态并确认磁盘已经添加到存储池中: $ kubectl get lsn node1 -o yaml apiVersion: hwameistor.io/v1alpha1 kind: LocalStorageNode ... pools: LocalStorage_PoolHDD: class: HDD disks: - capacityBytes: 17175674880 devPath: /dev/sdb ...  ","version":"下一个","tagName":"h2"},{"title":"Q7: 如何手动回收数据卷？​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q7-如何手动回收数据卷","content":"什么时候需要手动回收数据卷: StorageClass 的回收策略配置为 Retain，删除 PVC 后卷未能自动回收异常情况下删除 PVC 后数据卷没有自动回收 手动回收数据卷步骤: 查看 LV（数据卷） 与 PVC 的映射表，找到确定不再使用的 PVC，则对应 LV 应该是需要被回收的: $ kubectl get lv | awk '{print $1}' | grep -v NAME | xargs -I {} kubectl get lv {} -o jsonpath='{.metadata.name} -&gt; {.spec.pvcNamespace}/{.spec.pvcName}{&quot;\\n&quot;}' pvc-be53be2a-1c4b-430e-a45b-05777c791957 -&gt; default/data-nginx-sts-0 查看 PVC 是否存在，如存在则删除 查看 与 LV 同名的 PV 是否存在，如存在则删除 编辑 LV，修改 spec.delete=true $ kubectl edit lv pvc-be53be2a-1c4b-430e-a45b-05777c791957 ... spec: delete: true  ","version":"下一个","tagName":"h2"},{"title":"Q8: 为什么会有 LocalVolume 资源残留？​","type":1,"pageTitle":"常见问题","url":"/cn/docs/faqs#q8-为什么会有-localvolume-资源残留","content":"在先删除PV再删除PVC的情况下，LocalVolume资源不会被正常回收，需要在开启HonorPVReclaimPolicy特性后，才能正常回收。 注意 参考文档: https://kubernetes.io/blog/2021/12/15/kubernetes-1-23-prevent-persistentvolume-leaks-when-deleting-out-of-order/ 开启HonorPVReclaimPolicy步骤: 修改kube-controller-manager: $ vi /etc/kubernetes/manifests/kube-controller-manager.yaml ... spec: containers: - command: - kube-controller-manager - --allocate-node-cidrs=false - --feature-gates=HonorPVReclaimPolicy=true 修改csi-provisioner: $ kubectl edit -n hwameistor deployment.apps/hwameistor-local-storage-csi-controller ... containers: - args: - --v=5 - --csi-address=$(CSI_ADDRESS) - --leader-election=true - --feature-gates=Topology=true - --strict-topology - --extra-create-metadata=true - --feature-gates=HonorPVReclaimPolicy=true env: - name: CSI_ADDRESS value: /csi/csi.sock image: k8s.m.daocloud.io/sig-storage/csi-provisioner:v3.5.0 检查配置是否生效: 可以查看现有pv的finalizers是否包含external-provisioner.volume.kubernetes.io/finalizer $ kubectl get pv pvc-a7b7e3ba-f837-45ba-b243-dec7d8aaed53 -o yaml ... apiVersion: v1 kind: PersistentVolume metadata: annotations: pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com creationTimestamp: &quot;2021-11-17T19:28:56Z&quot; finalizers: - kubernetes.io/pv-protection - external-attacher/lvm-hwameistor-io - external-provisioner.volume.kubernetes.io/finalizer  ","version":"下一个","tagName":"h2"},{"title":"查看安装系统的状态","type":0,"sectionRef":"#","url":"/cn/docs/install/post_check","content":"","keywords":"","version":"下一个"},{"title":"查看 HwameiStor 系统组件​","type":1,"pageTitle":"查看安装系统的状态","url":"/cn/docs/install/post_check#查看-hwameistor-系统组件","content":"以下 Pod 必须在系统中正常运行。 $ kubectl -n hwameistor get pod NAME READY STATUS RESTARTS AGE drbd-adapter-k8s-master-rhel7-gtk7t 0/2 Completed 0 23m drbd-adapter-k8s-node1-rhel7-gxfw5 0/2 Completed 0 23m drbd-adapter-k8s-node2-rhel7-lv768 0/2 Completed 0 23m hwameistor-admission-controller-dc766f976-mtlvw 1/1 Running 0 23m hwameistor-apiserver-86d6c9b7c8-v67gg 1/1 Running 0 23m hwameistor-auditor-54f46fcbc6-jb4f4 1/1 Running 0 23m hwameistor-exporter-6498478c57-kr8r4 1/1 Running 0 23m hwameistor-failover-assistant-cdc6bd665-56wbw 1/1 Running 0 23m hwameistor-local-disk-csi-controller-6587984795-fztcd 2/2 Running 0 23m hwameistor-local-disk-manager-7gg9x 2/2 Running 0 23m hwameistor-local-disk-manager-kqkng 2/2 Running 0 23m hwameistor-local-disk-manager-s66kn 2/2 Running 0 23m hwameistor-local-storage-csi-controller-5cdff98f55-jj45w 6/6 Running 0 23m hwameistor-local-storage-mfqks 2/2 Running 0 23m hwameistor-local-storage-pnfpx 2/2 Running 0 23m hwameistor-local-storage-whg9t 2/2 Running 0 23m hwameistor-pvc-autoresizer-86dc79d57-s2l68 1/1 Running 0 23m hwameistor-scheduler-6db69957f-r58j6 1/1 Running 0 23m hwameistor-ui-744cd78d84-vktjq 1/1 Running 0 23m hwameistor-volume-evictor-5db99cf979-4674n 1/1 Running 0 23m  信息 local-disk-manager 和 local-storage 组件是以 DaemonSets 方式进行部署的，必须在每个节点上运行。 ","version":"下一个","tagName":"h2"},{"title":"查看 HwameiStor CRD（即 API）​","type":1,"pageTitle":"查看安装系统的状态","url":"/cn/docs/install/post_check#查看-hwameistor-crd即-api","content":"以下 HwameiStor CRD 必须安装在系统上。 $ kubectl api-resources --api-group hwameistor.io NAME SHORTNAMES APIVERSION NAMESPACED KIND clusters hmcluster hwameistor.io/v1alpha1 false Cluster events evt hwameistor.io/v1alpha1 false Event localdiskclaims ldc hwameistor.io/v1alpha1 false LocalDiskClaim localdisknodes ldn hwameistor.io/v1alpha1 false LocalDiskNode localdisks ld hwameistor.io/v1alpha1 false LocalDisk localdiskvolumes ldv hwameistor.io/v1alpha1 false LocalDiskVolume localstoragenodes lsn hwameistor.io/v1alpha1 false LocalStorageNode localvolumeconverts lvconvert hwameistor.io/v1alpha1 false LocalVolumeConvert localvolumeexpands lvexpand hwameistor.io/v1alpha1 false LocalVolumeExpand localvolumegroups lvg hwameistor.io/v1alpha1 false LocalVolumeGroup localvolumemigrates lvmigrate hwameistor.io/v1alpha1 false LocalVolumeMigrate localvolumereplicas lvr hwameistor.io/v1alpha1 false LocalVolumeReplica localvolumereplicasnapshotrestores lvrsrestore,lvrsnaprestore hwameistor.io/v1alpha1 false LocalVolumeReplicaSnapshotRestore localvolumereplicasnapshots lvrs hwameistor.io/v1alpha1 false LocalVolumeReplicaSnapshot localvolumes lv hwameistor.io/v1alpha1 false LocalVolume localvolumesnapshotrestores lvsrestore,lvsnaprestore hwameistor.io/v1alpha1 false LocalVolumeSnapshotRestore localvolumesnapshots lvs hwameistor.io/v1alpha1 false LocalVolumeSnapshot resizepolicies hwameistor.io/v1alpha1 false ResizePolicy  想了解具体的 CRD 信息，请查阅 CRD。 ","version":"下一个","tagName":"h2"},{"title":"查看 LocalDiskNode 和 localDisks​","type":1,"pageTitle":"查看安装系统的状态","url":"/cn/docs/install/post_check#查看-localdisknode-和-localdisks","content":"HwameiStor 自动扫描每个节点上的磁盘，并为每一块磁盘生成一个 CRD 资源 LocalDisk (LD)。 没有被使用的磁盘，其状态被标记为 PHASE: Available。 $ kubectl get localdisknodes NAME FREECAPACITY TOTALCAPACITY TOTALDISK STATUS AGE k8s-master Ready 28h k8s-node1 Ready 28h k8s-node2 Ready 28h $ kubectl get localdisks NAME NODEMATCH DEVICEPATH PHASE AGE localdisk-2307de2b1c5b5d051058bc1d54b41d5c k8s-node1 /dev/sdb Available 28h localdisk-311191645ea00c62277fe709badc244e k8s-node2 /dev/sdb Available 28h localdisk-37a20db051af3a53a1c4e27f7616369a k8s-master /dev/sdb Available 28h localdisk-b57b108ad2ccc47f4b4fab6f0b9eaeb5 k8s-node2 /dev/sda Bound 28h localdisk-b682686c65667763bda58e391fbb5d20 k8s-master /dev/sda Bound 28h localdisk-da121e8f0dabac9ee1bcb6ed69840d7b k8s-node1 /dev/sda Bound 28h  ","version":"下一个","tagName":"h2"},{"title":"查看 LocalStorageNodes 及存储池​","type":1,"pageTitle":"查看安装系统的状态","url":"/cn/docs/install/post_check#查看-localstoragenodes-及存储池","content":"HwameiStor 为每个存储节点创建一个 CRD 资源 LocalStorageNode (LSN)。 每个 LSN 将会记录该存储节点的状态，及节点上的所有存储资源，包括存储池、数据卷、及相关配置信息。 $ kubectl get lsn NAME IP STATUS AGE 10-6-234-40 10.6.234.40 Ready 3m52s 10-6-234-41 10.6.234.41 Ready 3m54s 10-6-234-42 10.6.234.42 Ready 3m55s $ kubectl get lsn 10-6-234-41 -o yaml apiVersion: hwameistor.io/v1alpha1 kind: LocalStorageNode metadata: creationTimestamp: &quot;2023-04-11T06:46:52Z&quot; generation: 1 name: 10-6-234-41 resourceVersion: &quot;13575433&quot; uid: 4986f7b8-6fe1-43f1-bdca-e68b6fa53f92 spec: hostname: 10-6-234-41 storageIP: 10.6.234.41 topogoly: region: default zone: default status: pools: LocalStorage_PoolHDD: class: HDD disks: - capacityBytes: 10733223936 devPath: /dev/sdb state: InUse type: HDD - capacityBytes: 1069547520 devPath: /dev/sdc state: InUse type: HDD - capacityBytes: 1069547520 devPath: /dev/sdd state: InUse type: HDD - capacityBytes: 1069547520 devPath: /dev/sde state: InUse type: HDD - capacityBytes: 1069547520 devPath: /dev/sdf state: InUse type: HDD - capacityBytes: 1069547520 devPath: /dev/sdg state: InUse type: HDD freeCapacityBytes: 16080961536 freeVolumeCount: 1000 name: LocalStorage_PoolHDD totalCapacityBytes: 16080961536 totalVolumeCount: 1000 type: REGULAR usedCapacityBytes: 0 usedVolumeCount: 0 volumeCapacityBytesLimit: 16080961536 state: Ready  ","version":"下一个","tagName":"h2"},{"title":"查看 StorageClass​","type":1,"pageTitle":"查看安装系统的状态","url":"/cn/docs/install/post_check#查看-storageclass","content":"HwameiStor Operator 在完成 HwameiStor 系统组件安装和系统初始化之后，会根据系统配置 （例如：是否开启 HA 模块、磁盘类型等）自动创建相应的 StorageClass 用于创建数据卷。 $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE hwameistor-storage-lvm-hdd lvm.hwameistor.io Delete WaitForFirstConsumer false 23h hwameistor-storage-lvm-hdd-convertible lvm.hwameistor.io Delete WaitForFirstConsumer false 23h hwameistor-storage-lvm-hdd-ha lvm.hwameistor.io Delete WaitForFirstConsumer false 23h  ","version":"下一个","tagName":"h2"},{"title":"准入控制器","type":0,"sectionRef":"#","url":"/cn/docs/modules/admission_controller","content":"","keywords":"","version":"下一个"},{"title":"识别 HwameiStor 数据卷​","type":1,"pageTitle":"准入控制器","url":"/cn/docs/modules/admission_controller#识别-hwameistor-数据卷","content":"准入控制器可以获取 Pod 使用的所有 PVC，并检查每个 PVC存储制备器。 如果制备器的名称后缀是 *.hwameistor.io，表示 Pod 正在使用 HwameiStor 提供的数据卷。 ","version":"下一个","tagName":"h2"},{"title":"验证资源​","type":1,"pageTitle":"准入控制器","url":"/cn/docs/modules/admission_controller#验证资源","content":"准入控制器只验证 Pod 资源，并在创建资源时就进行验证。 信息 为确保 HwameiStor 的 Pod 可以顺利启动，不会校验 HwameiStor 所在的命名空间下的 Pod。 ","version":"下一个","tagName":"h2"},{"title":"数据加载管理器","type":0,"sectionRef":"#","url":"/cn/docs/modules/dlm","content":"","keywords":"","version":"下一个"},{"title":"适用场景​","type":1,"pageTitle":"数据加载管理器","url":"/cn/docs/modules/dlm#适用场景","content":"数据加载管理器支持多种数据加载协议：s3、nfs、ftp、http、ssh 在AI数据训练场景中，可以更快的将数据加载到本地缓存卷中。 特别是当数据集支持s3协议拉取时，可以结合p2p技术，大幅提升数据加载速度。 ","version":"下一个","tagName":"h2"},{"title":"与 数据集管理器 一起使用​","type":1,"pageTitle":"数据加载管理器","url":"/cn/docs/modules/dlm#与-数据集管理器-一起使用","content":"DataSet Manager 是 HwameiStor 的一个组件，必须与数据集管理器 模块配合使用. ","version":"下一个","tagName":"h2"},{"title":"图形界面","type":0,"sectionRef":"#","url":"/cn/docs/modules/gui","content":"图形界面 HwameiStor 提供了一个图形化用户界面。用户通过该 web 界面可以直观、方便地管理 HwameiStor 存储系统。该模块可通过 Operator 进行部署。","keywords":"","version":"下一个"},{"title":"数据集管理器","type":0,"sectionRef":"#","url":"/cn/docs/modules/dsm","content":"数据集管理器 数据集管理器 是 DataStor 的一个模块，DataStor 是 AI 场景下云原生的本地存储系统加速解决方案，为 AI 应用所需的数据集提供高性能的本地缓存卷 支持的卷管理器：LVM。 支持的存储介质：HDD、SSD、NVMe。","keywords":"","version":"下一个"},{"title":"驱逐器","type":0,"sectionRef":"#","url":"/cn/docs/modules/evictor","content":"","keywords":"","version":"下一个"},{"title":"如何使用​","type":1,"pageTitle":"驱逐器","url":"/cn/docs/modules/evictor#如何使用","content":"请参考 Eviction ","version":"下一个","tagName":"h2"},{"title":"指标采集器","type":0,"sectionRef":"#","url":"/cn/docs/modules/exporter","content":"指标采集器 指标采集器是 HwameiStor 的一个系统指标服务器。它服务采集系统资源的相关指标， 例如磁盘、数据卷、存储空间、数据卷操作等等，并将这些指标提供给 Prometheus 模块。该模块由 Operator 进行部署。","keywords":"","version":"下一个"},{"title":"LDA 控制器","type":0,"sectionRef":"#","url":"/cn/docs/modules/lda_controller","content":"LDA 控制器 LDA 控制器提供了一个单独的 CRD - localdiskactions，用于匹配 localdisk，并执行指定的 action。 其代码示例如下： apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskAction metadata: name: forbidden-1 spec: action: reserve rule: minCapacity: 1024 devicePath: /dev/rbd* --- apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskAction metadata: name: forbidden-2 spec: action: reserve rule: maxCapacity: 1048576 devicePath: /dev/sd* 以上的 yaml 表示： 比 1024 字节更小且 devicePath 满足 /dev/rbd* 匹配条件的 Localdisk 将被预留比 1048576 字节更大且 devicePath 满足 /dev/sd* 匹配条件的 Localdisk 将被预留 请注意，当前支持的 action 仅 reserve。","keywords":"","version":"下一个"},{"title":"调度器","type":0,"sectionRef":"#","url":"/cn/docs/modules/scheduler","content":"","keywords":"","version":"下一个"},{"title":"安装​","type":1,"pageTitle":"调度器","url":"/cn/docs/modules/scheduler#安装","content":"调度器应在集群中以 HA 模式部署，这是生产环境中的最佳实践。 ","version":"下一个","tagName":"h2"},{"title":"本地存储","type":0,"sectionRef":"#","url":"/cn/docs/modules/ls","content":"","keywords":"","version":"下一个"},{"title":"适用场景​","type":1,"pageTitle":"本地存储","url":"/cn/docs/modules/ls#适用场景","content":"HwameiStor 提供两种本地数据卷：LVM、Disk。本地存储作为 HwameiStor 的一部分，负责提供 LVM 本地数据卷，包括高可用 LVM 数据卷、非高可用 LVM 数据卷。 非高可用的 LVM 本地数据卷，适用下列场景和应用： 具备高可用特性的数据库，例如 MongoDB 等具备高可用特性的消息中间件，例如 Kafka、RabbitMQ 等具备高可用特性的键值存储系统，例如 Redis 等其他具备高可用功能的应用 高可用的 LVM 本地数据卷，适用下列场景和应用： 数据库，例如 MySQL、PostgreSQL 等其他需要数据高可用特性的应用 ","version":"下一个","tagName":"h2"},{"title":"功能与路线图​","type":1,"pageTitle":"本地存储","url":"/cn/docs/modules/ls#功能与路线图","content":"功能路线图提供了本地存储版本发布及特性追踪功能。 ","version":"下一个","tagName":"h2"},{"title":"反馈​","type":1,"pageTitle":"本地存储","url":"/cn/docs/modules/ls#反馈","content":"如果有任何问题、意见、建议，请反馈至：Issues ","version":"下一个","tagName":"h2"},{"title":"本地磁盘管理器","type":0,"sectionRef":"#","url":"/cn/docs/modules/ldm","content":"","keywords":"","version":"下一个"},{"title":"基本概念​","type":1,"pageTitle":"本地磁盘管理器","url":"/cn/docs/modules/ldm#基本概念","content":"LocalDisk (LD): 这是 LDM 抽象的磁盘资源，一个 LD 代表了节点上的一块物理磁盘。 LocalDiskClaim (LDC): 这是系统使用磁盘的方式，通过创建 LDC 对象来向系统申请磁盘。用户可以添加一些对磁盘的描述来选择磁盘。 目前，LDC 支持以下对磁盘的描述选项： NodeNameCapacityDiskType（例如 HDD/SSD） ","version":"下一个","tagName":"h2"},{"title":"用法​","type":1,"pageTitle":"本地磁盘管理器","url":"/cn/docs/modules/ldm#用法","content":"查看 LocalDisk 信息 $ kubectl get localdisk NAME NODEMATCH PHASE 10-6-118-11-sda 10-6-118-11 Available 10-6-118-11-sdb 10-6-118-11 Available 该命令用于获取集群中磁盘资源信息，获取的信息总共有三列，含义分别如下： NAME： 代表磁盘在集群中的名称。NODEMATCH： 表明磁盘所在的节点名称。PHASE： 表明这个磁盘当前的状态。 通过 kubectl get localdisk &lt;name&gt; -o yaml 查看更多关于某块磁盘的信息。 申请可用磁盘 创建 LocalDiskClaim cat &lt;&lt; EOF | kubectl apply -f - apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskClaim metadata: name: &lt;localDiskClaimName&gt; spec: description: # 比如：HDD,SSD,NVMe diskType: &lt;diskType&gt; # 磁盘所在节点 nodeName: &lt;nodeName&gt; # 使用磁盘的系统名称 比如：local-storage,local-disk-manager owner: &lt;ownerName&gt; EOF 该命令用于创建一个磁盘使用的申请请求。在这个 yaml 文件里面，您可以在 description 字段添加对申请磁盘的描述，比如磁盘类型、磁盘的容量等等。 查看 LocalDiskClaim 信息 $ kubectl get localdiskclaim &lt;name&gt; LDC 被处理完成后，将立即被系统自动清理。如果 owner 是 local-storage，处理后的结果可以在对应的 LocalStorageNode 里查看。 ","version":"下一个","tagName":"h2"},{"title":"磁盘扩展","type":0,"sectionRef":"#","url":"/cn/docs/nodes_and_disks/disk_expansion","content":"","keywords":"","version":"下一个"},{"title":"准备新的存储磁盘​","type":1,"pageTitle":"磁盘扩展","url":"/cn/docs/nodes_and_disks/disk_expansion#准备新的存储磁盘","content":"从 HwameiStor 中选择需要扩容的节点，将新增磁盘插入该节点的磁盘槽位。 本例中，所用的新增存储节点和磁盘信息如下所示： name: k8s-worker-4devPath: /dev/sdcdiskType: SSD 在新增磁盘被插入到 HwameiStor 存储节点 k8s-worker-4 后，检查该节点上的新磁盘状态，如下： 检查新增磁盘是否成功插入节点，并被正确识别 ssh root@k8s-worker-4 lsblk | grep sdc 输出类似于： sdc 8:32 0 20G 1 disk 检查 HwameiStor 是否为新增磁盘正确创建资源 LocalDisk，并且状态为 Unclaimed kubectl get localdisk | grep k8s-worker-4 | grep sdc 输出类似于： k8s-worker-4-sdc k8s-worker-4 Available  ","version":"下一个","tagName":"h2"},{"title":"将新增磁盘加入到节点的存储池​","type":1,"pageTitle":"磁盘扩展","url":"/cn/docs/nodes_and_disks/disk_expansion#将新增磁盘加入到节点的存储池","content":"通过创建资源 LocalDiskClaim，将新增磁盘加入节点的存储池。 完成下列操作后，新磁盘应该被自动加入节点的 SSD 存储池中。 如果该节点上没有 SSD 存储池，HwameiStor 会为其自动创建，并将新磁盘加入其中。 $ kubectl apply -f - &lt;&lt;EOF apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskClaim metadata: name: k8s-worker-4-expand spec: nodeName: k8s-worker-4 owner: local-storage description: localDiskNames: - k8s-worker-4-sdc EOF  ","version":"下一个","tagName":"h2"},{"title":"后续检查​","type":1,"pageTitle":"磁盘扩展","url":"/cn/docs/nodes_and_disks/disk_expansion#后续检查","content":"完成上述步骤后，检查新增磁盘及其存储池的状态，确保节点和 HwameiStor 系统的正常运行。具体如下： kubectl get localstoragenode k8s-worker-4  输出类似于： apiVersion: hwameistor.io/v1alpha1 kind: LocalStorageNode metadata: name: k8s-worker-4 spec: hostname: k8s-worker-4 storageIP: 10.6.182.103 topogoly: region: default zone: default status: pools: LocalStorage_PoolSSD: class: SSD disks: - capacityBytes: 214744170496 devPath: /dev/sdb state: InUse type: SSD - capacityBytes: 214744170496 devPath: /dev/sdc state: InUse type: SSD freeCapacityBytes: 429488340992 freeVolumeCount: 1000 name: LocalStorage_PoolSSD totalCapacityBytes: 429488340992 totalVolumeCount: 1000 type: REGULAR usedCapacityBytes: 0 usedVolumeCount: 0 volumeCapacityBytesLimit: 429488340992 volumes: state: Ready  ","version":"下一个","tagName":"h2"},{"title":"CAS 存储","type":0,"sectionRef":"#","url":"/cn/docs/terms/cas","content":"","keywords":"","version":"下一个"},{"title":"CAS 的优势​","type":1,"pageTitle":"CAS 存储","url":"/cn/docs/terms/cas#cas-的优势","content":"","version":"下一个","tagName":"h2"},{"title":"敏捷性​","type":1,"pageTitle":"CAS 存储","url":"/cn/docs/terms/cas#敏捷性","content":"CAS 中的每个存储卷都有一个容器化存储控制器和相应的容器化副本。因此，围绕这些组件的资源维护和调整是真正敏捷化的。 Kubernetes 的滚动升级功能实现了存储控制器和存储副本的无缝升级。CPU 和内存等资源可以使用容器 cgroup 进行优化。 ","version":"下一个","tagName":"h3"},{"title":"存储策略的粒度​","type":1,"pageTitle":"CAS 存储","url":"/cn/docs/terms/cas#存储策略的粒度","content":"对存储软件进行容器化，并将存储控制器专用于每个卷，可以实现存储策略的最大粒度。使用 CAS 体系结构，您可以按卷配置所有存储策略。 此外，您可以监视每个卷的存储参数，并动态更新存储策略，以实现每个工作负载的预期结果。随着卷存储策略中粒度的增加，对存储吞吐量、IOPS 和延迟的控制也会增加。 ","version":"下一个","tagName":"h3"},{"title":"避免绑定​","type":1,"pageTitle":"CAS 存储","url":"/cn/docs/terms/cas#避免绑定","content":"避免被云服务商绑定是许多 Kubernetes 用户的共同目标。然而，有状态应用的数据通常仍然依赖于云服务商及其技术，或者依赖于底层的传统共享存储系统 NAS 或 SAN。 而使用 CAS 方法后，存储控制器可以在每个工作负载的后台进行数据迁移，使得实时迁移变得更简单。 换句话说，CAS 的控制粒度以无中断的方式简化了有状态工作负载从一个 Kubernetes 集群到另一个集群的迁移。 ","version":"下一个","tagName":"h3"},{"title":"原生云​","type":1,"pageTitle":"CAS 存储","url":"/cn/docs/terms/cas#原生云","content":"CAS 将存储软件容器化，并使用 Kubernetes CRD 来表示底层存储资源，如磁盘和存储池。这种模式使存储能够无缝地集成到其他云主机的工具中。 可以使用 Prometheus、Grafana、Fluentd、Weavescope、Jaeger 等云主机工具来调配、监控和管理存储资源。 另外，CAS 中卷的存储和性能是可伸缩的。由于每个卷都有自己的存储控制器，因此可以在节点存储容量的允许范围内弹性伸缩。 随着给定 Kubernetes 集群中容器应用数量的增加，会添加更多节点，从而提高存储容量和性能的总体可用性，使存储可用于新的应用容器。 ","version":"下一个","tagName":"h3"},{"title":"更小的影响范围​","type":1,"pageTitle":"CAS 存储","url":"/cn/docs/terms/cas#更小的影响范围","content":"由于 CAS 体系结构是按工作负载划分的，并且组件是松散耦合的，所以 CAS 的影响范围比典型的分布式存储体系结构小得多。 CAS 可以通过从存储控制器到存储副本的同步复制来提供高可用性。维护副本所需的元数据简化为副本节点的信息以及有关副本状态的信息。 如果某个节点出现故障，存储控制器将通过第二个或第三个副本在正运行且数据可用的节点上轮转。 因此使用 CAS 时，影响范围要小得多，影响仅局限在该节点上有副本的卷。 ","version":"下一个","tagName":"h3"},{"title":"磁盘节点扩展","type":0,"sectionRef":"#","url":"/cn/docs/nodes_and_disks/disk_nodes","content":"","keywords":"","version":"下一个"},{"title":"步骤​","type":1,"pageTitle":"磁盘节点扩展","url":"/cn/docs/nodes_and_disks/disk_nodes#步骤","content":"","version":"下一个","tagName":"h2"},{"title":"1. 准备新的存储节点​","type":1,"pageTitle":"磁盘节点扩展","url":"/cn/docs/nodes_and_disks/disk_nodes#1-准备新的存储节点","content":"在 Kubernetes 集群中新增一个节点，或者，选择一个已有的集群节点（非 HwameiStor 节点）。 本例中，所用的新增存储节点和磁盘信息如下所示： name: k8s-worker-2devPath: /dev/sdbdiskType: SSD disk 新增节点已经成功加入 Kubernetes 集群之后，检查并确保下列 Pod 正常运行在该节点上，以及相关资源存在于集群中： $ kubectl get node NAME STATUS ROLES AGE VERSION k8s-master-1 Ready master 96d v1.24.3-2+63243a96d1c393 k8s-worker-1 Ready worker 96h v1.24.3-2+63243a96d1c393 k8s-worker-2 Ready worker 96h v1.24.3-2+63243a96d1c393 $ kubectl -n hwameistor get pod -o wide | grep k8s-worker-2 hwameistor-local-disk-manager-sfsf1 2/2 Running 0 19h 10.6.128.150 k8s-worker-2 &lt;none&gt; &lt;none&gt; # 检查 LocalDiskNode 资源 $ kubectl get localdisknode k8s-worker-2 NAME FREECAPACITY TOTALCAPACITY TOTALDISK STATUS AGE k8s-worker-2 Ready 21d  ","version":"下一个","tagName":"h3"},{"title":"2. 添加新增存储节点到 HwameiStor 系统​","type":1,"pageTitle":"磁盘节点扩展","url":"/cn/docs/nodes_and_disks/disk_nodes#2-添加新增存储节点到-hwameistor-系统","content":"首先，需要将磁盘 sdb 的 owner 信息修改成 local-disk-manager，具体如下： $ kubectl edit ld localdisk-2307de2b1c5b5d051058bc1d54b41d5c apiVersion: hwameistor.io/v1alpha1 kind: LocalDisk metadata: name: localdisk-2307de2b1c5b5d051058bc1d54b41d5c spec: devicePath: /dev/sdb nodeName: k8s-worker-2 + owner: local-disk-manager ...  为增加存储节点创建资源 LocalStorageClaim，以此为新增存储节点构建存储池。这样，节点就已经成功加入 HwameiStor 系统。具体如下： $ kubectl apply -f - &lt;&lt;EOF apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskClaim metadata: name: k8s-worker-2 spec: nodeName: k8s-worker-2 owner: local-disk-manager description: diskType: SSD EOF  ","version":"下一个","tagName":"h3"},{"title":"3. 后续检查​","type":1,"pageTitle":"磁盘节点扩展","url":"/cn/docs/nodes_and_disks/disk_nodes#3-后续检查","content":"完成上述步骤后，检查新增存储节点及其存储池的状态，确保节点和 HwameiStor 系统的正常运行。具体如下： $ kubectl get localdisknode k8s-worker-2 -o yaml apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskNode metadata: name: k8s-worker-2 spec: nodeName: k8s-worker-2 status: pools: LocalDisk_PoolSSD: class: SSD disks: - capacityBytes: 214744170496 devPath: /dev/sdb state: Available type: SSD freeCapacityBytes: 214744170496 freeVolumeCount: 1 totalCapacityBytes: 214744170496 totalVolumeCount: 1 type: REGULAR usedCapacityBytes: 0 usedVolumeCount: 0 volumeCapacityBytesLimit: 214744170496 volumes: state: Ready  ","version":"下一个","tagName":"h3"},{"title":"审计日志","type":0,"sectionRef":"#","url":"/cn/docs/system_audit","content":"","keywords":"","version":"下一个"},{"title":"使用方式​","type":1,"pageTitle":"审计日志","url":"/cn/docs/system_audit#使用方式","content":"审计日志通过 CRD 的方式存入系统中，为每一个资源创建一个 CR 来记录其操作历史。该 CRD 如下： apiVersion: hwameistor.io/v1alpha1 kind: Event name: spec: resourceType: &lt;Cluster | Node | StoragePool | Volume&gt; resourceName: records: - action: actionContent: # in JSON format time: state: stateContent: # in JSON format  例如，我们可以看看数据卷的审计信息： apiVersion: hwameistor.io/v1alpha1 kind: Event metadata: creationTimestamp: &quot;2023-08-08T15:52:55Z&quot; generation: 5 name: volume-pvc-34e3b086-2d95-4980-beb6-e175fd79a847 resourceVersion: &quot;10221888&quot; uid: d3ebaffb-eddb-4c84-93be-efff350688af spec: resourceType: Volume resourceName: pvc-34e3b086-2d95-4980-beb6-e175fd79a847 records: - action: Create actionContent: '{&quot;requiredCapacityBytes&quot;:5368709120,&quot;volumeQoS&quot;:{},&quot;poolName&quot;:&quot;LocalStorage_PoolHDD&quot;,&quot;replicaNumber&quot;:2,&quot;convertible&quot;:true,&quot;accessibility&quot;:{&quot;nodes&quot;:[&quot;k8s-node1&quot;,&quot;k8s-master&quot;],&quot;zones&quot;:[&quot;default&quot;],&quot;regions&quot;:[&quot;default&quot;]},&quot;pvcNamespace&quot;:&quot;default&quot;,&quot;pvcName&quot;:&quot;mysql-data-volume&quot;,&quot;volumegroup&quot;:&quot;db890e34-a092-49ac-872b-f2a422439c81&quot;}' time: &quot;2023-08-08T15:52:55Z&quot; - action: Mount actionContent: '{&quot;allocatedCapacityBytes&quot;:5368709120,&quot;replicas&quot;:[&quot;pvc-34e3b086-2d95-4980-beb6-e175fd79a847-krp927&quot;,&quot;pvc-34e3b086-2d95-4980-beb6-e175fd79a847-wm7p56&quot;],&quot;state&quot;:&quot;Ready&quot;,&quot;publishedNode&quot;:&quot;k8s-node1&quot;,&quot;fsType&quot;:&quot;xfs&quot;,&quot;rawblock&quot;:false}' time: &quot;2023-08-08T15:53:07Z&quot; - action: Unmount actionContent: '{&quot;allocatedCapacityBytes&quot;:5368709120,&quot;usedCapacityBytes&quot;:33783808,&quot;totalInode&quot;:2621120,&quot;usedInode&quot;:3,&quot;replicas&quot;:[&quot;pvc-34e3b086-2d95-4980-beb6-e175fd79a847-krp927&quot;,&quot;pvc-34e3b086-2d95-4980-beb6-e175fd79a847-wm7p56&quot;],&quot;state&quot;:&quot;Ready&quot;,&quot;publishedNode&quot;:&quot;k8s-node1&quot;,&quot;fsType&quot;:&quot;xfs&quot;,&quot;rawblock&quot;:false}' time: &quot;2023-08-08T16:03:03Z&quot; - action: Delete actionContent: '{&quot;requiredCapacityBytes&quot;:5368709120,&quot;volumeQoS&quot;:{},&quot;poolName&quot;:&quot;LocalStorage_PoolHDD&quot;,&quot;replicaNumber&quot;:2,&quot;convertible&quot;:true,&quot;accessibility&quot;:{&quot;nodes&quot;:[&quot;k8s-node1&quot;,&quot;k8s-master&quot;],&quot;zones&quot;:[&quot;default&quot;],&quot;regions&quot;:[&quot;default&quot;]},&quot;pvcNamespace&quot;:&quot;default&quot;,&quot;pvcName&quot;:&quot;mysql-data-volume&quot;,&quot;volumegroup&quot;:&quot;db890e34-a092-49ac-872b-f2a422439c81&quot;,&quot;config&quot;:{&quot;version&quot;:1,&quot;volumeName&quot;:&quot;pvc-34e3b086-2d95-4980-beb6-e175fd79a847&quot;,&quot;requiredCapacityBytes&quot;:5368709120,&quot;convertible&quot;:true,&quot;resourceID&quot;:2,&quot;readyToInitialize&quot;:true,&quot;initialized&quot;:true,&quot;replicas&quot;:[{&quot;id&quot;:1,&quot;hostname&quot;:&quot;k8s-node1&quot;,&quot;ip&quot;:&quot;10.6.113.101&quot;,&quot;primary&quot;:true},{&quot;id&quot;:2,&quot;hostname&quot;:&quot;k8s-master&quot;,&quot;ip&quot;:&quot;10.6.113.100&quot;,&quot;primary&quot;:false}]},&quot;delete&quot;:true}' time: &quot;2023-08-08T16:03:38Z&quot;  ","version":"下一个","tagName":"h2"},{"title":"LVM 节点扩展","type":0,"sectionRef":"#","url":"/cn/docs/nodes_and_disks/lvm_nodes","content":"","keywords":"","version":"下一个"},{"title":"步骤​","type":1,"pageTitle":"LVM 节点扩展","url":"/cn/docs/nodes_and_disks/lvm_nodes#步骤","content":"","version":"下一个","tagName":"h2"},{"title":"1. 准备新的存储节点​","type":1,"pageTitle":"LVM 节点扩展","url":"/cn/docs/nodes_and_disks/lvm_nodes#1-准备新的存储节点","content":"在 Kubernetes 集群中新增一个节点，或者，选择一个已有的集群节点（非 HwameiStor 节点）。 该节点必须满足 Prerequisites 要求的所有条件。 本例中，所用的新增存储节点和磁盘信息如下所示： name: k8s-worker-4devPath: /dev/sdbdiskType: SSD disk 新增节点已经成功加入 Kubernetes 集群之后，检查并确保下列 Pod 正常运行在该节点上，以及相关资源存在于集群中： $ kubectl get node NAME STATUS ROLES AGE VERSION k8s-master-1 Ready master 96d v1.24.3-2+63243a96d1c393 k8s-worker-1 Ready worker 96h v1.24.3-2+63243a96d1c393 k8s-worker-2 Ready worker 96h v1.24.3-2+63243a96d1c393 k8s-worker-3 Ready worker 96d v1.24.3-2+63243a96d1c393 k8s-worker-4 Ready worker 1h v1.24.3-2+63243a96d1c393 $ kubectl -n hwameistor get pod -o wide | grep k8s-worker-4 hwameistor-local-disk-manager-c86g5 2/2 Running 0 19h 10.6.182.105 k8s-worker-4 &lt;none&gt; &lt;none&gt; hwameistor-local-storage-s4zbw 2/2 Running 0 19h 192.168.140.82 k8s-worker-4 &lt;none&gt; &lt;none&gt; # 检查 LocalStorageNode 资源 $ kubectl get localstoragenode k8s-worker-4 NAME IP ZONE REGION STATUS AGE k8s-worker-4 10.6.182.103 default default Ready 8d  ","version":"下一个","tagName":"h3"},{"title":"2. 添加新增存储节点到 HwameiStor 系统​","type":1,"pageTitle":"LVM 节点扩展","url":"/cn/docs/nodes_and_disks/lvm_nodes#2-添加新增存储节点到-hwameistor-系统","content":"为增加存储节点创建资源 LocalStorageClaim，以此为新增存储节点构建存储池。这样，节点就已经成功加入 HwameiStor 系统。具体如下： $ kubectl apply -f - &lt;&lt;EOF apiVersion: hwameistor.io/v1alpha1 kind: LocalDiskClaim metadata: name: k8s-worker-4 spec: nodeName: k8s-worker-4 owner: local-storage description: diskType: SSD EOF  ","version":"下一个","tagName":"h3"},{"title":"3. 后续检查​","type":1,"pageTitle":"LVM 节点扩展","url":"/cn/docs/nodes_and_disks/lvm_nodes#3-后续检查","content":"完成上述步骤后，检查新增存储节点及其存储池的状态，确保节点和 HwameiStor 系统的正常运行。具体如下： $ kubectl get localstoragenode k8s-worker-4 -o yaml apiVersion: hwameistor.io/v1alpha1 kind: LocalStorageNode metadata: name: k8s-worker-4 spec: hostname: k8s-worker-4 storageIP: 10.6.182.103 topogoly: region: default zone: default status: pools: LocalStorage_PoolSSD: class: SSD disks: - capacityBytes: 214744170496 devPath: /dev/sdb state: InUse type: SSD freeCapacityBytes: 214744170496 freeVolumeCount: 1000 name: LocalStorage_PoolSSD totalCapacityBytes: 214744170496 totalVolumeCount: 1000 type: REGULAR usedCapacityBytes: 0 usedVolumeCount: 0 volumeCapacityBytesLimit: 214744170496 volumes: state: Ready  ","version":"下一个","tagName":"h3"},{"title":"CRD 和 CR","type":0,"sectionRef":"#","url":"/cn/docs/terms/crd_and_cr","content":"","keywords":"","version":"下一个"},{"title":"CRD​","type":1,"pageTitle":"CRD 和 CR","url":"/cn/docs/terms/crd_and_cr#crd","content":"CRD 是 Custom Resource Definition 的缩写，是 Kubernetes 内置原生的一个资源类型。它是自定义资源 (CR) 的定义，用来描述什么是自定义资源。 CRD 可以向 Kubernetes 集群注册一种新资源，用于拓展 Kubernetes 集群的能力。有了 CRD，就可以自定义底层基础设施的抽象，根据业务需求来定制资源类型，利用 Kubernetes 已有的资源和能力，通过乐高积木的模式定义出更高层次的抽象。 ","version":"下一个","tagName":"h2"},{"title":"CR​","type":1,"pageTitle":"CRD 和 CR","url":"/cn/docs/terms/crd_and_cr#cr","content":"CR 是 Custom Resource 的缩写，它实际是 CRD 的一个实例，是符合 CRD 中字段格式定义的一个资源描述。 ","version":"下一个","tagName":"h2"},{"title":"CRDs + Controllers​","type":1,"pageTitle":"CRD 和 CR","url":"/cn/docs/terms/crd_and_cr#crds--controllers","content":"我们都知道 Kubernetes 的扩展能力很强大，但仅有 CRD 并没有什么用，还需要有控制器 (Custom Controller) 的支撑，才能体现出 CRD 的价值，Custom Controller 可以监听 CR 的 CRUD 事件来实现自定义业务逻辑。 在 Kubernetes 中，可以说是 CRDs + Controllers = Everything。 另请参考 Kubernetes 官方文档： CustomResourceCustomResourceDefinition ","version":"下一个","tagName":"h2"},{"title":"Kubernetes 存储","type":0,"sectionRef":"#","url":"/cn/docs/terms/k8s_storage","content":"","keywords":"","version":"下一个"},{"title":"容器存储接口​","type":1,"pageTitle":"Kubernetes 存储","url":"/cn/docs/terms/k8s_storage#容器存储接口","content":"容器存储接口 (CSI) 是在 Kubernetes 这种容器编排体系上将任意文件存储和块存储系统暴露给容器化工作负载的一个标准。 使用 CSI 后，像 HwameiStor 这样的第三方存储提供商可以写入和部署新的存储卷插件，例如 HwameiStor LocalDisk 和 LocalVolumeReplica，而无需修改 Kubernetes 核心代码。 当集群管理员安装 HwameiStor 时，所需的 HwameiStor CSI 驱动程序组件也将安装到 Kubernetes 集群中。 //在 CSI 之前，Kubernetes 支持使用 out-of-tree 资源调配器（也称为外部资源调配器）添加新的存储提供商。Kubernetes 树状卷 (in-tree) 早于外部资源调配器。而 Kubernetes 社区也在努力使用基于 CSI 的卷来替代树状卷。  ","version":"下一个","tagName":"h2"},{"title":"存储类和动态资源调配​","type":1,"pageTitle":"Kubernetes 存储","url":"/cn/docs/terms/k8s_storage#存储类和动态资源调配","content":"StorageClass 为管理员提供了一种描述存储“类别”的方法。不同的类别可能会映射到集群管理员确定的服务质量水平、 备份策略或任意策略。在某些存储系统中，StorageClass 这个概念称为“配置文件”。 有了动态资源调配功能后，集群管理员无需预先调配存储资源。它会在用户请求时自动调配存储资源。 动态卷资源调配的实现基于 StorageClass 这个抽象概念。集群管理员可以根据需要，定义尽可能多的 StorageClass 对象， 每个对象都会指定一个调配卷的插件（也称为调配器），并在资源调配时配置传递给该调配器的一组参数。 集群管理员可以在集群内定义和公开（来自相同或不同存储系统的）多种存储类型，每种类型都可以自定义参数。 这种设计还确保最终用户不必担心存储配置的复杂性和细微差别，但仍然能够从多个存储选项中进行选择。 安装 HwameiStor 后将附带了两个默认存储类，允许用户创建本地卷 (HwameiStor LocalVolume) 或副本 (HwameiStor LocalVolumeReplica)。 集群管理员可以启用所需的存储引擎，然后为数据引擎创建存储类。 ","version":"下一个","tagName":"h2"},{"title":"持久卷声明​","type":1,"pageTitle":"Kubernetes 存储","url":"/cn/docs/terms/k8s_storage#持久卷声明","content":"PersistentVolumeClaim (PVC) 是由集群管理员提供的 StorageClass 服务的用户存储请求。在容器上运行的应用可以请求某种类型的存储。 例如，容器可以指定所需的存储大小或存取数据的方式（只读、单次读/写、多次读写等）。 除了存储大小和存取模式之外，管理员还可以创建存储类，为 PV 提供自定义属性，例如磁盘类型（HDD 和 SSD）、性能水平或存储层级（常规存储或冷存储）。 ","version":"下一个","tagName":"h2"},{"title":"持久卷​","type":1,"pageTitle":"Kubernetes 存储","url":"/cn/docs/terms/k8s_storage#持久卷","content":"PersistentVolume (PV) 由存储提供商在用户请求 PVC 时动态调配。PV 包含容器如何消耗存储的详细信息。 Kubernetes 和卷驱动程序使用 PV 中的细节将存储附加/解除附加到容器运行的节点，并将存储挂载/解除挂载到容器。 HwameiStor 控制面动态设置 HwameiStor 本地卷和副本，并帮助在集群中创建 PV 对象。 ","version":"下一个","tagName":"h2"},{"title":"有状态和无状态​","type":1,"pageTitle":"Kubernetes 存储","url":"/cn/docs/terms/k8s_storage#有状态和无状态","content":"Kubernetes 提供了几个内置的有状态和无状态负载资源，让应用开发人员定义在 Kubernetes 上运行的负载。 通过创建 Kubernetes 无状态/有状态负载，并使用 PVC 将其连接到 PV，可以运行有状态负载。 例如，可以用 YAML 创建引用 PVC 的 MySQL 无状态负载。无状态负载引用的 MySQL PVC 应该使用请求的大小和 StorageClass 创建。 一旦 HwameiStor 控制平面为所需的 StorageClass 和容量提供了 PV，则声明设置为已满足。然后，Kubernetes 将挂载 PV 并启动 MySQL 无状态负载。 ","version":"下一个","tagName":"h2"},{"title":"LVM","type":0,"sectionRef":"#","url":"/cn/docs/terms/lvm","content":"","keywords":"","version":"下一个"},{"title":"LVM 主要构成​","type":1,"pageTitle":"LVM","url":"/cn/docs/terms/lvm#lvm-主要构成","content":"物理存储介质 (PM, physical media)：LVM 存储介质可以是分区、磁盘、RAID 阵列或 SAN 磁盘。 物理卷 (PV, physical volume)：物理卷是 LVM 的基本存储逻辑块，但与基本的物理存储介质（如分区、磁盘等）比较， 却包含有与 LVM 相关的管理参数，创建物理卷可以用磁盘分区，也可以用磁盘本身。磁盘设备必须初始化为 LVM 物理卷，才能与 LVM 结合使用。 卷组 (VG, Volume Group)：LVM 卷组由一个或多个物理卷组成。 逻辑卷 (LV, logical volume)：LV 建立在 VG 之上，可以在 LV 之上建立文件系统。 物理范围 (PE, physical extents)：PV 物理卷中可以分配的最小存储单元，PE 的大小是可以指定的，默认为 4MB。 逻辑范围 (LE, logical extents)：LV 逻辑卷中可以分配的最小存储单元，在同一个卷组中，LE 的大小与 PE 是相同的， 并且一一对应。 ","version":"下一个","tagName":"h2"},{"title":"LVM 优点​","type":1,"pageTitle":"LVM","url":"/cn/docs/terms/lvm#lvm-优点","content":"使用卷组，使多个硬盘空间看起来像是一个大的硬盘使用逻辑卷，可以跨多个硬盘空间的分区 sdb1 sdb2 sdc1 sdd2 sdf使用逻辑卷，可以在空间不足时动态调整它的大小在调整逻辑卷大小时，不需要考虑逻辑卷在硬盘上的位置，不用担心没有可用的连续空间可以在线对 LV、VG 进行创建、删除、调整大小等操作，LVM 上的文件系统也需要重新调整大小允许创建快照，可以用来保存文件系统的备份RAID + LVM 结合使用：LVM 是软件的卷管理方式，而 RAID 是磁盘管理的方法。对于重要的数据， 使用 RAID 来保护物理磁盘不会因为故障而中断业务，再用 LVM 来实现对卷的良性管理，更好地利用磁盘资源。 ","version":"下一个","tagName":"h2"},{"title":"使用 LVM 的基本步骤​","type":1,"pageTitle":"LVM","url":"/cn/docs/terms/lvm#使用-lvm-的基本步骤","content":"物理磁盘被格式化为 PV，即空间被划分为一个个的 PE。PV 包含多个 PE。不同的 PV 加入到同一个 VG 中，即不同 PV 的 PE 全部进入到 VG 的 PE 池内。VG 包含多个 PV。在 VG 中创建 LV 逻辑卷，这个创建过程基于 PE，所以组成 LV 的 PE 可能来自不同的物理磁盘。LV 基于 PE 创建。LV 直接可以格式化后挂载使用。LV 的扩缩实际上就是增加或减少组成该 LV 的 PE 数量，其过程不会丢失原始数据。格式化 LV，并挂载使用。 ","version":"下一个","tagName":"h2"},{"title":"LV 扩容​","type":1,"pageTitle":"LVM","url":"/cn/docs/terms/lvm#lv-扩容","content":"首先，确定是否有可用的扩容空间，因为空间是从 VG 里面创建的，并且 LV 不能跨 VG 扩容。若 VG 没有了容量，需要先扩 VG。步骤如下： $ vgs VG #PV #LV #SN Attr VSize VFree vg-sdb1 1 8 1 wz--n- &lt;16.00g &lt;5.39g $ lvextend -L +100M -r /dev/vg-sdb1/lv-sdb1 #将 /dev/vg-sdb1/lv-sdb 扩容 100M  ","version":"下一个","tagName":"h2"},{"title":"VG 扩容​","type":1,"pageTitle":"LVM","url":"/cn/docs/terms/lvm#vg-扩容","content":"如果 VG 卷组中的空间不够了，需要添加新的磁盘，依次运行以下命令： $ pvcreate /dev/sdc $ vgextend vg-sdb1 /dev/sdb3  ","version":"下一个","tagName":"h2"},{"title":"LV 快照​","type":1,"pageTitle":"LVM","url":"/cn/docs/terms/lvm#lv-快照","content":"LVM 机制提供了对 LV 做快照的功能，以此来获得文件系统的状态一致性备份。LVM 采用写时复制技术 (Copy-On-Write, COW)， 不用停止服务或将逻辑卷设为只读就可以进行备份，使用 LVM 快照功能既可以获得一致备份，又不会影响服务器的可用性。 LVM 采用的写时复制，是指创建 LVM 快照时，仅复制原始卷中数据的元数据。换句话说，也就是在创建 LVM 逻辑卷时， 并不会发生数据的物理复制。再换句话说，仅复制元数据，不复制物理数据，因此快照的创建几乎是实时的。 当原始卷上执行写入操作时，快照会跟踪原始卷中块的变更，这时原始卷上将要变更的数据会在变更之前拷贝到快照预留的空间。 ","version":"下一个","tagName":"h2"},{"title":"卷","type":0,"sectionRef":"#","url":"/cn/docs/terms/volume","content":"卷 容器中的文件在磁盘上是临时存放的，这给容器中运行的较重要的应用程序带来一些问题。 问题一：当容器崩溃时文件会丢失。kubelet 会重新启动容器，但容器会以干净的状态重启。问题二：在同一个 Pod 中运行多个容器并共享文件时出现此问题。 Kubernetes 卷 (Volume) 这一抽象概念能够解决这两个问题。 Kubernetes 支持很多类型的卷。Pod 可以同时使用任意数目的卷类型。临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 当 Pod 不再存在时，Kubernetes 也会销毁临时卷；不过 Kubernetes 不会销毁持久卷。对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。 卷的核心是一个目录，其中可能保存了数据，Pod 中的容器可以访问该目录中的数据。所采用的特定卷类型将决定该目录如何形成、使用何种介质保存数据以及目录中存放的内容。 使用卷时，在 .spec.volumes 字段中设置为 Pod 提供的卷，并在 .spec.containers[*].volumeMounts 字段中声明卷在容器中的挂载位置。 参考 Kubernetes 官方文档： 卷持久卷临时卷","keywords":"","version":"下一个"},{"title":"PV 和 PVC","type":0,"sectionRef":"#","url":"/cn/docs/terms/pv_pvc","content":"PV 和 PVC PV（PersistentVolume，持久卷）是对存储资源的抽象，将存储定义为一种容器应用可以使用的资源。 PV 由管理员创建和配置，与存储提供商的具体实现直接相关，例如文件存储、块存储、对象存储或 DRBD 等， 通过插件式的机制进行管理，供应用访问和使用。除 EmptyDir 类型的存储卷，PV 的生命周期独立于使用它的 Pod。 PVC（PersistentVolumeClaim，持久卷声明）是用户对存储资源的一个申请。就像 Pod 消耗 Node 的资源一样，PVC 会消耗 PV 的资源。 PVC 可以申请存储空间的大小 (Size) 和访问模式（例如 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany）。 使用 PVC 申请的存储空间仍然不满足应用对存储设备的各种需求。在很多情况下，应用程序对存储设备的特性和性能都有不同的要求， 包括读写速度、并发性能、数据冗余等要求。这就需要使用资源对象 StorageClass，用于标记存储资源和性能，根据 PVC 的需求动态供给合适的 PV 资源。 StorageClass 和存储资源动态供应的机制经完善后，实现了存储卷的按需创建，在共享存储的自动化管理进程中实现了重要的一步。 另请参考 Kubernetes 官方文档： 持久卷存储类动态卷供应","keywords":"","version":"下一个"},{"title":"CSI 接口","type":0,"sectionRef":"#","url":"/cn/docs/terms/csi","content":"","keywords":"","version":"下一个"},{"title":"扩展 CSI 和 Kubernetes​","type":1,"pageTitle":"CSI 接口","url":"/cn/docs/terms/csi#扩展-csi-和-kubernetes","content":"为了实现在 Kubernetes 上扩展卷的功能，应该扩展几个组件，包括 CSI 规范、“in-tree” 卷插件、external-provisioner 和 external-attacher。 ","version":"下一个","tagName":"h2"},{"title":"扩展 CSI 规范​","type":1,"pageTitle":"CSI 接口","url":"/cn/docs/terms/csi#扩展-csi-规范","content":"最新的 CSI 0.2.0 仍未定义扩展卷的功能。应引入新的 3 个 RPC：RequiresFSResize、ControllerResizeVolume 和 NodeResizeVolume。 service Controller { rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {} …… rpc RequiresFSResize (RequiresFSResizeRequest) returns (RequiresFSResizeResponse) {} rpc ControllerResizeVolume (ControllerResizeVolumeRequest) returns (ControllerResizeVolumeResponse) {} } service Node { rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {} …… rpc NodeResizeVolume (NodeResizeVolumeRequest) returns (NodeResizeVolumeResponse) {} }  ","version":"下一个","tagName":"h2"},{"title":"扩展 “In-Tree” 卷插件​","type":1,"pageTitle":"CSI 接口","url":"/cn/docs/terms/csi#扩展-in-tree-卷插件","content":"除了扩展的 CSI 规范之外，Kubernetes 中的 csiPlugin 接口还应实现 expandablePlugin。csiPlugin 接口将扩展代表 ExpanderController 的 PersistentVolumeClaim。 type ExpandableVolumePlugin interface { VolumePlugin ExpandVolumeDevice(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, error) RequiresFSResize() bool }  ","version":"下一个","tagName":"h2"},{"title":"实现卷驱动程序​","type":1,"pageTitle":"CSI 接口","url":"/cn/docs/terms/csi#实现卷驱动程序","content":"最后，为了抽象化实现的复杂性，应将单独的存储提供程序管理逻辑硬编码为以下功能，这些功能在 CSI 规范中已明确定义： CreateVolumeDeleteVolumeControllerPublishVolumeControllerUnpublishVolumeValidateVolumeCapabilitiesListVolumesGetCapacityControllerGetCapabilitiesRequiresFSResizeControllerResizeVolume ","version":"下一个","tagName":"h3"},{"title":"展示​","type":1,"pageTitle":"CSI 接口","url":"/cn/docs/terms/csi#展示","content":"以具体的用户案例来演示此功能。 为 CSI 存储供应商创建存储类 allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: csi-qcfs parameters: csiProvisionerSecretName: orain-test csiProvisionerSecretNamespace: default provisioner: csi-qcfsplugin reclaimPolicy: Delete volumeBindingMode: Immediate 在 Kubernetes 集群上部署包括存储供应商 csi-qcfsplugin 在内的 CSI 卷驱动 创建 PVC qcfs-pvc，它将由存储类 csi-qcfs 动态配置 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: qcfs-pvc namespace: default .... spec: accessModes: - ReadWriteOnce resources: requests: storage: 300Gi storageClassName: csi-qcfs 创建 MySQL 5.7 实例以使用 PVC qcfs-pvc 为了反映完全相同的生产级别方案，实际上有两种不同类型的工作负载，包括： 批量插入使 MySQL 消耗更多的文件系统容量浪涌查询请求 通过编辑 pvc qcfs-pvc 配置动态扩展卷容量 ","version":"下一个","tagName":"h2"},{"title":"概览","type":0,"sectionRef":"#","url":"/cn/docs/use_cases/overview","content":"","keywords":"","version":"下一个"},{"title":"热备份机制​","type":1,"pageTitle":"概览","url":"/cn/docs/use_cases/overview#热备份机制","content":"","version":"下一个","tagName":"h2"},{"title":"单节点热备份​","type":1,"pageTitle":"概览","url":"/cn/docs/use_cases/overview#单节点热备份","content":"Raid 5 保障，可容忍 1 组磁盘故障。 控制流和数据流彼此独立，保证数据访问的稳定性。  ","version":"下一个","tagName":"h3"},{"title":"跨节点热备份​","type":1,"pageTitle":"概览","url":"/cn/docs/use_cases/overview#跨节点热备份","content":"Raid 5 + 主备副本保障。 规划了 HA 专用网络逻辑接口 dce-storage 在节点间同步存储流量。跨节点同步复制数据，保证数据的热备份。  ","version":"下一个","tagName":"h3"},{"title":"数据再平衡​","type":1,"pageTitle":"概览","url":"/cn/docs/use_cases/overview#数据再平衡","content":"通过数据卷迁移技术实现数据在集群中的均衡放置。在线移动数据到有更多富裕空间的节点上。  ","version":"下一个","tagName":"h2"},{"title":"数据卷类型变更​","type":1,"pageTitle":"概览","url":"/cn/docs/use_cases/overview#数据卷类型变更","content":"为了支持某些特殊场景，允许单副本数据卷变更为多副本，支持跨节点热备份。  ","version":"下一个","tagName":"h2"},{"title":"数据卷扩容","type":0,"sectionRef":"#","url":"/cn/docs/volumes/expand","content":"","keywords":"","version":"下一个"},{"title":"查看 StorageClass 是否使用了参数 allowVolumeExpansion: true​","type":1,"pageTitle":"数据卷扩容","url":"/cn/docs/volumes/expand#查看-storageclass-是否使用了参数-allowvolumeexpansion-true","content":"$ kubectl get pvc data-sts-mysql-local-0 -o jsonpath='{.spec.storageClassName}' hwameistor-storage-lvm-hdd $ kubectl get sc hwameistor-storage-lvm-hdd -o jsonpath='{.allowVolumeExpansion}' true  ","version":"下一个","tagName":"h2"},{"title":"修改 PVC 的大小​","type":1,"pageTitle":"数据卷扩容","url":"/cn/docs/volumes/expand#修改-pvc-的大小","content":"$ kubectl edit pvc data-sts-mysql-local-0 ... spec: resources: requests: storage: 2Gi ...  ","version":"下一个","tagName":"h2"},{"title":"观察扩容过程​","type":1,"pageTitle":"数据卷扩容","url":"/cn/docs/volumes/expand#观察扩容过程","content":"增加的容量越多，扩容所需时间越长。可以在 PVC 的事件日志中观察整个扩容的过程. $ kubectl describe pvc data-sts-mysql-local-0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ExternalExpanding 34s volume_expand Ignoring the PVC: didn't find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. Warning VolumeResizeFailed 33s external-resizer lvm.hwameistor.io resize volume &quot;pvc-b9fc8651-97b8-414c-8bcf-c8d2708c4ee8&quot; by resizer &quot;lvm.hwameistor.io&quot; failed: rpc error: code = Unknown desc = volume expansion not completed yet Normal Resizing 32s (x2 over 33s) external-resizer lvm.hwameistor.io External resizer is resizing volume pvc-b9fc8651-97b8-414c-8bcf-c8d2708c4ee8 Normal FileSystemResizeRequired 32s external-resizer lvm.hwameistor.io Require file system resize of volume on node Normal FileSystemResizeSuccessful 11s kubelet MountVolume.NodeExpandVolume succeeded for volume &quot;pvc-b9fc8651-97b8-414c-8bcf-c8d2708c4ee8&quot; k8s-worker-3  ","version":"下一个","tagName":"h2"},{"title":"观察扩容完成后的 PVC/PV​","type":1,"pageTitle":"数据卷扩容","url":"/cn/docs/volumes/expand#观察扩容完成后的-pvcpv","content":"$ kubectl get pvc data-sts-mysql-local-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-sts-mysql-local-0 Bound pvc-b9fc8651-97b8-414c-8bcf-c8d2708c4ee8 2Gi RWO hwameistor-storage-lvm-hdd 96m $ kubectl get pv pvc-b9fc8651-97b8-414c-8bcf-c8d2708c4ee8 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b9fc8651-97b8-414c-8bcf-c8d2708c4ee8 2Gi RWO Delete Bound default/data-sts-mysql-local-0 hwameistor-storage-lvm-hdd 96m  ","version":"下一个","tagName":"h2"},{"title":"Minio","type":0,"sectionRef":"#","url":"/cn/docs/use_cases/minio","content":"","keywords":"","version":"下一个"},{"title":"MinIO 简介​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#minio-简介","content":"MinIO 是一款高性能、分布式、兼容 S3 的多云对象存储系统套件。MinIO 原生支持 Kubernetes，能够支持所有公有云、私有云及边缘计算环境。 MinIO 是 GNU AGPL v3 开源的软件定义产品，能够很好地运行在标准硬件如 X86 等设备上。  MinIO 的架构设计从一开始就是针对性能要求很高的私有云标准，在实现对象存储所需要的全部功能的基础上追求极致的性能。 MinIO 具备易用性、高效性及高性能，能够以更简单的方式提供具有弹性伸缩能力的云原生对象存储服务。 MinIO 在传统对象存储场景（如辅助存储、灾难恢复和归档）方面表现出色，同时在机器学习、大数据、私有云、混合云等方面的存储技术上也独树一帜，包括数据分析、高性能应用负载、原生云应用等。 ","version":"下一个","tagName":"h2"},{"title":"MinIO 架构设计​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#minio-架构设计","content":"MinIO 为云原生架构设计，可以作为轻量级容器运行并由外部编排服务如 Kubernetes 管理。 MinIO 整个服务包约为不到 100 MB 的静态二进制文件，即使在很高负载下也可以高效利用 CPU 和内存资源并可以在共享硬件上共同托管大量租户。 对应的架构图如下：  MinIO 用作云原生应用程序的主要存储，与传统对象存储相比，云原生应用程序需要更高的吞吐量和更低的延迟，而这些都是 MinIO 能够达成的性能指标，读/写速度高达 183 GB/秒和 171 GB/秒。 MinIO 极致的高性能离不开底层存储基础平台。本地存储在众多的存储协议中具有最高的读写性能无疑能为 MinIO 提供性能保障。 HwameiStor 正是满足云原生时代要求的储存系统。它具有高性能、高可用、自动化、低成本、快速部署等优点，可以替代昂贵的传统 SAN 存储。 MinIO 可以在带有本地驱动器（JBOD/JBOF）的标准服务器上运行。 集群为完全对称的体系架构，即所有服务器的功能均相同，没有名称节点或元数据服务器。 MinIO 将数据和元数据作为对象一起写入从而无需使用元数据数据库。 MinIO 以内联、严格一致的操作执行所有功能，包括擦除代码、位 rotrot 检查、加密等。 每个 MinIO 集群都是分布式 MinIO 服务器的集合，每个节点一个进程。 MinIO 作为单个进程在用户空间中运行，并使用轻量级的协同例程来实现高并发。 将驱动器分组到擦除集（默认情况下，每组 16 个驱动器），然后使用确定性哈希算法将对象放置在这些擦除集上。 MinIO 专为大规模、多数据中心云存储服务而设计。 每个租户都运行自己的 MinIO 集群，该集群与其他租户完全隔离，从而使租户能够免受升级、更新和安全事件的任何干扰。 每个租户通过联合跨地理区域的集群来独立扩展。  ","version":"下一个","tagName":"h3"},{"title":"以 HwameiStor 为底座搭建 MinIO 的优势​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#以-hwameistor-为底座搭建-minio-的优势","content":"以 HwameiStor 为底座搭建 MinIO 存储方案，构建智、稳、敏全面增强本地存储，具备以下优势。 自动化运维管理 可以自动发现、识别、管理、分配磁盘。 根据亲和性，智能调度应用和数据。自动监测磁盘状态，并及时预警。 高可用的数据 使用跨节点副本同步数据， 实现高可用。发生问题时，会自动将应用调度到高可用数据节点上，保证应用的连续性。 丰富的数据卷类型 聚合 HDD、SSD、NVMe 类型的磁盘，提供非低延时，高吞吐的数据服务。 灵活动态的线性扩展 可以根据集群规模大小进行动态的扩容，灵活满足应用的数据持久化需求。 丰富的应用场景，广泛适配企业需求，适配高可用架构中间件 类似 Kafka、ElasticSearch、Redis 等这类中间件自身具备高可用架构，同时对数据的 IO 访问有很高要求。 HwameiStor 提供的基于 LVM 的单副本本地数据卷，可以很好地满足它们的要求。 为应用提供高可用数据卷 MySQL 等 OLTP 数据库要求底层存储提供高可用的数据存储，当发生问题时可快速恢复数据，同时也要求保证高性能的数据访问。 HwameiStor 提供的双副本的高可用数据卷，可以很好地满足此类需求。 自动化运维传统存储软件 MinIO、Ceph 等存储软件，需要使用 Kubernetes 节点上的磁盘，可以采用 PVC/PV 的方式， 通过 CSI 驱动自动化地使用 HwameiStor 的单副本本地卷，快速响应业务系统提出的部署、扩容、迁移等需求，实现基于 Kubernetes 的自动化运维。 ","version":"下一个","tagName":"h3"},{"title":"测试环境​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#测试环境","content":"按照以下步骤依次部署 Kubernetes 集群、HwameiStor 本地存储和 MinIO。 ","version":"下一个","tagName":"h2"},{"title":"部署 Kubernetes 集群​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#部署-kubernetes-集群","content":"本次测试使用了三台虚拟机节点部署了 Kubernetes 集群：1 Master + 2 Worker 节点，kubelet 版本为 1.22.0。  ","version":"下一个","tagName":"h3"},{"title":"部署 HwameiStor 本地存储​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#部署-hwameistor-本地存储","content":"在 Kubernetes 上部署 HwameiStor 本地存储。  两台 Worker 节点各配置了五块磁盘（SDB、SDC、SDD、SDE、SDF）用于 HwameiStor 本地磁盘管理。   查看 local storage node 状态。  创建了 storagClass。  ","version":"下一个","tagName":"h3"},{"title":"分布式多租户源码部署安装（minio operator）​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#分布式多租户源码部署安装minio-operator","content":"本节说明如何部署 minio operator，如何创建租户，如何配置 HwameiStor 本地卷。 ","version":"下一个","tagName":"h2"},{"title":"部署 minio operator​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#部署-minio-operator","content":"参照以下步骤部署 minio operator。 复制 minio operator 仓库到本地。 git clone &lt;https://github.com/minio/operator.git&gt; 进入 helm operator 目录：/root/operator/helm/operator。 部署 minio-operator 实例。 helm install minio-operator \\ --namespace minio-operator \\ --create-namespace \\ --generate-name . --set persistence.storageClass=local-storage-hdd-lvm . 检查 minio-operator 资源运行情况。 ","version":"下一个","tagName":"h3"},{"title":"创建租户​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#创建租户","content":"参照以下步骤创建一个租户。 进入 /root/operator/examples/kustomization/base 目录。如下修改 tenant.yaml。 进入 /root/operator/helm/tenant/ 目录。如下修改 values.yaml 文件。 进入 /root/operator/examples/kustomization/tenant-lite 目录。如下修改 kustomization.yaml 文件。 如下修改 tenant.yaml 文件。 如下修改 tenantNamePatch.yaml 文件。 创建租户： kubectl apply –k . 检查租户 minio-t1 资源状态： 如要创建一个新的租户可以在 /root/operator/examples/kustomization 目录下建一个新的 tenant 目录（本案例为 tenant-lite-2）并对相应文件做对应修改。 执行 kubectl apply –k . 创建新的租户 minio-t2。 ","version":"下一个","tagName":"h3"},{"title":"配置 HwameiStor 本地卷​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#配置-hwameistor-本地卷","content":"依次运行以下命令来配置本地卷。 kubectl get statefulset.apps/minio-t1-pool-0 -nminio-tenant -oyaml   kubectl get pvc –A   kubectl get pvc export-minio6-0 -nminio-6 -oyaml   kubectl get pv   kubectl get pvc data0-minio-t1-pool-0-0 -nminio-tenant -oyaml   kubectl get lv   kubect get lvr   ","version":"下一个","tagName":"h3"},{"title":"HwameiStor 与 MinIo 测试验证​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#hwameistor-与-minio-测试验证","content":"完成上述配置之后，执行了基本功能测试和多租户隔离测试。 ","version":"下一个","tagName":"h2"},{"title":"基本功能测试​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#基本功能测试","content":"基本功能测试的步骤如下。 从浏览器登录 minio console：10.6.163.52:30401/login。 通过 kubectl minio proxy -n minio-operator 获取 JWT。 浏览及管理创建的租户信息。 登录 minio-t1 租户（用户名 minio，密码 minio123）。 浏览 bucket bk-1。 创建新的 bucket bk-1-1。 创建 path path-1-2。 上传文件成功： 上传文件夹成功： 创建只读用户： ","version":"下一个","tagName":"h3"},{"title":"多租户隔离测试​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#多租户隔离测试","content":"执行以下步骤进行多租户隔离测试。 登录 minio-t2 租户。 此时只能看到 minio-t2 内容，minio-t1 的内容被屏蔽。 创建 bucket。 创建 path。 上传文件。 创建用户。 配置用户 policy。 删除 bucket。 ","version":"下一个","tagName":"h3"},{"title":"结论​","type":1,"pageTitle":"Minio","url":"/cn/docs/use_cases/minio#结论","content":"本次测试是在 Kubernetes 1.22 平台上部署了 MinIO 分布式对象存储并对接 HwameiStor 本地存储。在此环境中完成了基本能力测试、系统安全测试及运维管理测试。 全部测试成功通过，证实了 HwameiStor 能够完美适配 MinIO 存储方案。 ","version":"下一个","tagName":"h2"},{"title":"数据卷迁移","type":0,"sectionRef":"#","url":"/cn/docs/volumes/migrate","content":"","keywords":"","version":"下一个"},{"title":"基本概念​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#基本概念","content":"LocalVolumeGroup(LVG)（数据卷组）管理是 HwameiStor 中重要的一个功能。当应用 Pod 申请多个数据卷 PVC 时，为了保证 Pod 能正确运行，这些数据卷必须具有某些相同属性，例如：数据卷的副本数量，副本所在的节点。通过数据卷组管理功能正确地管理这些相关联的数据卷，是 HwameiStor 中非常重要的能力。 ","version":"下一个","tagName":"h2"},{"title":"前提条件​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#前提条件","content":"LocalVolumeMigrate 需要部署在 Kubernetes 系统中，需要部署应用满足下列条件： 支持 lvm 类型的卷 基于 LocalVolume 粒度迁移时，默认所属相同 LocalVolumeGroup 的数据卷不会一并迁移（若一并迁移，需要配置开关 MigrateAllVols：true） ","version":"下一个","tagName":"h2"},{"title":"步骤 1: 创建 StorageClass​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-1-创建-storageclass","content":"$ cd ../../deploy/ $ kubectl apply -f storageclass-lvm.yaml  ","version":"下一个","tagName":"h2"},{"title":"步骤 2: 创建 multiple PVC​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-2-创建-multiple-pvc","content":"$ kubectl apply -f pvc-multiple-lvm.yaml  ","version":"下一个","tagName":"h2"},{"title":"步骤 3: 部署多数据卷 Pod​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-3-部署多数据卷-pod","content":"$ kubectl apply -f nginx-multiple-lvm.yaml  ","version":"下一个","tagName":"h2"},{"title":"步骤 4: 解挂载多数据卷 Pod​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-4-解挂载多数据卷-pod","content":"kubectl -n hwameistor scale --current-replicas=1 --replicas=0 deployment/nginx-local-storage-lvm  ","version":"下一个","tagName":"h2"},{"title":"步骤 5: 创建迁移任务​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-5-创建迁移任务","content":"$ cat &lt;&lt; EOF | kubectl apply -f - apiVersion: hwameistor.io/v1alpha1 kind: LocalVolumeMigrate metadata: namespace: hwameistor name: &lt;localVolumeMigrateName&gt; annotations: hwameistor.io/replica-affinity: &lt;need/forbid&gt; spec: sourceNode: &lt;sourceNodeName&gt; targetNodesSuggested: - &lt;targetNodesName1&gt; - &lt;targetNodesName2&gt; volumeName: &lt;volName&gt; migrateAllVols: &lt;true/false&gt; EOF  在迁移时， 1）如果指定了targetNodesSuggested，系统会从指定的节点中，选择一个适合的进行迁移。如果都不合适，迁移操作失败; 2）如果不指定 targetNodesSuggested，系统会根据容量平衡原则自动选择一个适合的节点进行迁移。 3）如果不希望在迁移过程中考虑pod的亲和性 hwameistor.io/replica-affinity: forbid $ cat &lt;&lt; EOF | kubectl apply -f - apiVersion: hwameistor.io/v1alpha1 kind: LocalVolumeMigrate metadata: namespace: hwameistor name: &lt;localVolumeMigrateName&gt; spec: sourceNode: &lt;sourceNodeName&gt; targetNodesSuggested: [] volumeName: &lt;volName&gt; migrateAllVols: &lt;true/false&gt; EOF  ","version":"下一个","tagName":"h2"},{"title":"步骤 6: 查看迁移状态​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-6-查看迁移状态","content":"$ kubectl get LocalVolumeMigrate localvolumemigrate-1 -o yaml apiVersion: hwameistor.io/v1alpha1 kind: LocalVolumeMigrate metadata: generation: 1 name: localvolumemigrate-1 namespace: hwameistor resourceVersion: &quot;12828637&quot; uid: 78af7f1b-d701-4b03-84de-27fafca58764 spec: abort: false migrateAllVols: true sourceNode: k8s-172-30-40-61 targetNodesSuggested: - k8s-172-30-45-223 volumeName: pvc-1a0913ac-32b9-46fe-8258-39b4e3b696a4 status: originalReplicaNumber: 1 targetNode: k8s-172-30-45-223 state: Completed message:  ","version":"下一个","tagName":"h2"},{"title":"步骤 7: 查看迁移成功状态​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-7-查看迁移成功状态","content":"$ kubectl get lvr NAME CAPACITY NODE STATE SYNCED DEVICE AGE pvc-1a0913ac-32b9-46fe-8258-39b4e3b696a4-9cdkkn 1073741824 172-30-45-223 Ready true /dev/LocalStorage_PoolHDD-HA/pvc-1a0913ac-32b9-46fe-8258-39b4e3b696a4 77s pvc-d9d3ae9f-64af-44de-baad-4c69b9e0744a-7ppmrx 1073741824 172-30-45-223 Ready true /dev/LocalStorage_PoolHDD-HA/pvc-d9d3ae9f-64af-44de-baad-4c69b9e0744a 77s  ","version":"下一个","tagName":"h2"},{"title":"步骤 8: 迁移成功后，重新挂载数据卷 Pod​","type":1,"pageTitle":"数据卷迁移","url":"/cn/docs/volumes/migrate#步骤-8-迁移成功后重新挂载数据卷-pod","content":"$ kubectl -n hwameistor scale --current-replicas=0 --replicas=1 deployment/nginx-local-storage-lvm  ","version":"下一个","tagName":"h2"},{"title":"数据卷的自动扩容","type":0,"sectionRef":"#","url":"/cn/docs/volumes/pvc_autoresizing","content":"","keywords":"","version":"下一个"},{"title":"ResizePolicy​","type":1,"pageTitle":"数据卷的自动扩容","url":"/cn/docs/volumes/pvc_autoresizing#resizepolicy","content":"下面是一个示例 CR: apiVersion: hwameistor.io/v1alpha1 kind: ResizePolicy metadata: name: resizepolicy1 spec: warningThreshold: 60 resizeThreshold: 80 nodePoolUsageLimit: 90  warningThreshold、resizeThreshold、resizeThreshold 三个 int 类型的字段都表示一个百分比。 warningThreshold 目前暂时还没有关联任何告警动作，它是作为一个目标比例，即扩容完成后卷的使用率会在这个比例以下。resizeThreshold 指示了一个使用率，当卷的使用率达到这个比例时，扩容动作就会被触发。nodePoolUsageLimit 表示节点存储池使用率的上限，如果某个池的使用率达到了这个比例， 那么落在这个池的卷将不会自动扩容。 ","version":"下一个","tagName":"h2"},{"title":"匹配规则​","type":1,"pageTitle":"数据卷的自动扩容","url":"/cn/docs/volumes/pvc_autoresizing#匹配规则","content":"这是一个带有 label-selector 的示例 CR。 apiVersion: hwameistor.io/v1alpha1 kind: ResizePolicy metadata: name: example-policy spec: warningThreshold: 60 resizeThreshold: 80 nodePoolUsageLimit: 90 storageClassSelector: matchLabels: pvc-resize: auto namespaceSelector: matchLabels: pvc-resize: auto pvcSelector: matchLabels: pvc-resize: auto  ResizePolicy 有三个 label-selector： pvcSelector 表示被这个 selector 选中的 PVC 会依照选中它的 policy 自动扩容。namespaceSelector 表示被这个 selector 选中的 namespace 下的 PVC 会依照这个 policy 自动扩容。storageClassSelector 表示从被这个 selector 选中的 storageclass 创建出来的 PVC 会依照这个 policy 自动扩容。 这三个 selector 之间是“且”的关系，如果你在一个 ResizePolicy 里指明了多个 selector， 那么要符合全部的 selector 的 PVC 才会匹配这个 policy。如果 ResizePolicy 中没有指明任何 selector， 它就是一个集群 ResizePolicy，也就像是整个集群中所有 PVC 的默认 policy。 ","version":"下一个","tagName":"h2"},{"title":"数据卷克隆","type":0,"sectionRef":"#","url":"/cn/docs/volumes/volume_clone","content":"","keywords":"","version":"下一个"},{"title":"1. 创建克隆卷​","type":1,"pageTitle":"数据卷克隆","url":"/cn/docs/volumes/volume_clone#1-创建克隆卷","content":"可以创建 pvc，对数据卷进行克隆操作。具体如下： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hwameistor-lvm-volume-clone spec: storageClassName: hwameistor-storage-lvm-ssd dataSource: # 必须提供已经 Bound 的数据卷 name: data-sts-mysql-local-0 kind: PersistentVolumeClaim apiGroup: &quot;&quot; accessModes: - ReadWriteOnce resources: requests: storage: 1Gi  ","version":"下一个","tagName":"h2"},{"title":"2. 使用克隆卷​","type":1,"pageTitle":"数据卷克隆","url":"/cn/docs/volumes/volume_clone#2-使用克隆卷","content":"使用以下命令创建一个 nginx 应用并使用数据卷 hwameistor-lvm-volume-clone： cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: nginx namespace: default spec: containers: - name: nginx image: docker.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - name: data mountPath: /data ports: - containerPort: 80 volumes: - name: data persistentVolumeClaim: claimName: hwameistor-lvm-volume-clone EOF  ","version":"下一个","tagName":"h2"},{"title":"本地缓存卷","type":0,"sectionRef":"#","url":"/cn/docs/volumes/cache","content":"","keywords":"","version":"下一个"},{"title":"安装 Dragonfly​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#安装-dragonfly","content":"根据集群配置 /etc/hosts。 $ vi /etc/hosts host1-IP hostName1 host2-IP hostName2 host3-IP hostName3 要安装 Dragonfly 组件，请确保配置了默认存储类，因为创建存储卷需要它。 kubectl patch storageclass hwameistor-storage-lvm-hdd -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}' 使用 helm 安装 Dragonfly。 helm repo add dragonfly https://dragonflyoss.github.io/helm-charts/ helm install --create-namespace --namespace dragonfly-system dragonfly dragonfly/dragonfly --version 1.1.63 dragonfly-dfdaemon 配置。 kubectl -n dragonfly-system get ds kubectl -n dragonfly-system edit ds dragonfly-dfdaemon ... spec: spec: containers: - image: docker.io/dragonflyoss/dfdaemon:v2.1.45 ... securityContext: capabilities: add: - SYS_ADMIN privileged: true volumeMounts: ... - mountPath: /var/run name: host-run - mountPath: /mnt mountPropagation: Bidirectional name: host-mnt ... volumes: ... - hostPath: path: /var/run type: DirectoryOrCreate name: host-run - hostPath: path: /mnt type: DirectoryOrCreate name: host-mnt ... 安装 dfget 客户端命令行工具。在每个节点执行： wget https://github.com/dragonflyoss/Dragonfly2/releases/download/v2.1.44/dfget-2.1.44-linux-amd64.rpm rpm -ivh dfget-2.1.44-linux-amd64.rpm 为避免出现问题，请取消之前配置的默认存储类。 kubectl patch storageclass hwameistor-storage-lvm-hdd -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;false&quot;}}}'  ","version":"下一个","tagName":"h2"},{"title":"查看 Dragonfly​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#查看-dragonfly","content":"$ kubectl -n dragonfly-system get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dragonfly-dfdaemon-d2fzp 1/1 Running 0 19h 200.200.169.158 hwameistor-test-1 &lt;none&gt; &lt;none&gt; dragonfly-dfdaemon-p7smf 1/1 Running 0 19h 200.200.29.171 hwameistor-test-3 &lt;none&gt; &lt;none&gt; dragonfly-dfdaemon-tcwkr 1/1 Running 0 19h 200.200.39.71 hwameistor-test-2 &lt;none&gt; &lt;none&gt; dragonfly-manager-5479bf9bc9-tp4g5 1/1 Running 1 (19h ago) 19h 200.200.29.174 hwameistor-test-3 &lt;none&gt; &lt;none&gt; dragonfly-manager-5479bf9bc9-wpbr6 1/1 Running 0 19h 200.200.39.92 hwameistor-test-2 &lt;none&gt; &lt;none&gt; dragonfly-manager-5479bf9bc9-zvrdj 1/1 Running 0 19h 200.200.169.142 hwameistor-test-1 &lt;none&gt; &lt;none&gt; dragonfly-mysql-0 1/1 Running 0 19h 200.200.29.178 hwameistor-test-3 &lt;none&gt; &lt;none&gt; dragonfly-redis-master-0 1/1 Running 0 19h 200.200.169.137 hwameistor-test-1 &lt;none&gt; &lt;none&gt; dragonfly-redis-replicas-0 1/1 Running 0 19h 200.200.39.72 hwameistor-test-2 &lt;none&gt; &lt;none&gt; dragonfly-redis-replicas-1 1/1 Running 0 19h 200.200.29.130 hwameistor-test-3 &lt;none&gt; &lt;none&gt; dragonfly-redis-replicas-2 1/1 Running 0 19h 200.200.169.134 hwameistor-test-1 &lt;none&gt; &lt;none&gt; dragonfly-scheduler-0 1/1 Running 0 19h 200.200.169.190 hwameistor-test-1 &lt;none&gt; &lt;none&gt; dragonfly-scheduler-1 1/1 Running 0 19h 200.200.39.76 hwameistor-test-2 &lt;none&gt; &lt;none&gt; dragonfly-scheduler-2 1/1 Running 0 19h 200.200.29.163 hwameistor-test-3 &lt;none&gt; &lt;none&gt; dragonfly-seed-peer-0 1/1 Running 1 (19h ago) 19h 200.200.169.138 hwameistor-test-1 &lt;none&gt; &lt;none&gt; dragonfly-seed-peer-1 1/1 Running 0 19h 200.200.39.80 hwameistor-test-2 &lt;none&gt; &lt;none&gt; dragonfly-seed-peer-2 1/1 Running 0 19h 200.200.29.151 hwameistor-test-3 &lt;none&gt; &lt;none&gt;  ","version":"下一个","tagName":"h2"},{"title":"查看 DataSet​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#查看-dataset","content":"以 minio 为例： apiVersion: datastore.io/v1alpha1 kind: DataSet metadata: name: dataset-test spec: refresh: true type: minio minio: endpoint: Your service ip address:9000 bucket: BucketName/Dir # 根据你的数据集所在的目录级别定义 secretKey: minioadmin accessKey: minioadmin region: ap-southeast-2  ","version":"下一个","tagName":"h2"},{"title":"创建 DataSet​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#创建-dataset","content":"kubectl apply -f dataset.yaml  确认缓存卷已成功创建。 $ kubectl get lv dataset-test NAME POOL REPLICAS CAPACITY USED STATE PUBLISHED AGE dataset-test LocalStorage_PoolHDD 3 1073741824 906514432 Ready 20d $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE dataset-test 1Gi ROX Retain Bound default/hwameistor-dataset 20d  PV 的大小是根据你数据集的大小而决定的，您也可以手动配置。 ","version":"下一个","tagName":"h2"},{"title":"创建 PVC 绑定 PV​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#创建-pvc-绑定-pv","content":"apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hwameistor-dataset namespace: default spec: accessModes: - ReadOnlyMany resources: requests: storage: 1Gi # 数据集大小 volumeMode: Filesystem volumeName: dataset-test  确认 PVC 已经创建成功。 $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hwameistor-dataset Bound dataset-test 1Gi ROX 20d  ","version":"下一个","tagName":"h2"},{"title":"创建 StatefulSet​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#创建-statefulset","content":"kubectl apply -f sts-nginx-AI.yaml  apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-dataload namespace: default spec: serviceName: nginx-dataload replicas: 1 selector: matchLabels: app: nginx-dataload template: metadata: labels: app: nginx-dataload spec: hostNetwork: true hostPID: true hostIPC: true containers: - name: nginx image: docker.io/library/nginx:latest imagePullPolicy: IfNotPresent securityContext: privileged: true env: - name: DATASET_NAME value: dataset-test volumeMounts: - name: data mountPath: /data ports: - containerPort: 80 volumes: - name: data persistentVolumeClaim: claimName: hwameistor-dataset  信息 claimName使用绑定到数据集的 pvc 的名称。 env: DATASET_NAME=datasetName ","version":"下一个","tagName":"h2"},{"title":"查看 Nginx Pod​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#查看-nginx-pod","content":"$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-dataload-0 1/1 Running 0 3m58s $ kubectl logs nginx-dataload-0 hwameistor-dataloader Created custom resource Custom resource deleted, exiting DataLoad execution time: 1m20.24310857s  根据日志，加载数据耗时 1m20.24310857s。 ","version":"下一个","tagName":"h2"},{"title":"[可选] 将 Nginx 扩展为 3 节点集群​","type":1,"pageTitle":"本地缓存卷","url":"/cn/docs/volumes/cache#可选-将-nginx-扩展为-3-节点集群","content":"HwameiStor 缓存卷支持 StatefulSet 横向扩展。StatefulSet 的每个 Pod 都会附加并挂载一个绑定同一份数据集的 HwameiStor 缓存卷。 $ kubectl scale sts/sts-nginx-AI --replicas=3 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE nginx-dataload-0 1/1 Running 0 41m nginx-dataload-1 1/1 Running 0 37m nginx-dataload-2 1/1 Running 0 35m $ kubectl logs nginx-dataload-1 hwameistor-dataloader Created custom resource Custom resource deleted, exiting DataLoad execution time: 3.24310857s $ kubectl logs nginx-dataload-2 hwameistor-dataloader Created custom resource Custom resource deleted, exiting DataLoad execution time: 2.598923144s  根据日志，第二次和第三次加载数据只耗时 3.24310857s、2.598923144s 。对比首次加载速度得到了很大的提升。 ","version":"下一个","tagName":"h2"},{"title":"TiDB","type":0,"sectionRef":"#","url":"/cn/docs/use_cases/tidb","content":"","keywords":"","version":"下一个"},{"title":"TiDB 简介​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#tidb-简介","content":"TiDB 是一款同时支持在线事务处理 (OLTP) 与在线分析处理 (OATP) 的融合型分布式数据库产品，具备水平扩缩容、金融级高可用、实时 HTAP（即同时支持 OLTP 和 OATP）、云原生的分布式数据库，兼容 MySQL 5.7 协议和 MySQL 生态等重要特性。TiDB 的目标是为用户提供一站式的 OLTP、OLAP、HTAP 解决方案，适合高可用、强一致要求较高、数据规模较大等各种应用场景。 ","version":"下一个","tagName":"h2"},{"title":"TiDB 整体架构​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#tidb-整体架构","content":"TiDB 分布式数据库将整体架构拆分成了多个模块，各模块之间互相通信，组成完整的 TiDB 系统。对应的架构图如下：  TiDB Server SQL 层对外暴露 MySQL 协议的连接端点，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 LVS、HAProxy 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。 PD (Placement Driver) Server 整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。 存储节点 TiKV Server：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。 TiFlash：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。 ","version":"下一个","tagName":"h3"},{"title":"TiDB 数据库的存储​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#tidb-数据库的存储","content":" 键值对 (Key-Value Pair) TiKV 的选择是 Key-Value 模型，并且提供有序遍历方法。TiKV 数据存储的两个关键点： 这是一个巨大的 Map（可以类比一下 C++ 的 std::map），也就是存储的是 Key-Value Pairs。 这个 Map 中的 Key-Value pair 按照 Key 的二进制顺序有序，也就是可以 Seek 到某一个 Key 的位置，然后不断地调用 Next 方法以递增的顺序获取比这个 Key 大的 Key-Value。 本地存储（Rocks DB） 任何持久化的存储引擎，数据终归要保存在磁盘上，TiKV 也不例外。但是 TiKV 没有选择直接向磁盘上写数据，而是把数据保存在 RocksDB 中，具体的数据落地由 RocksDB 负责。这样做的原因是开发一个单机存储引擎工作量很大，特别是要做一个高性能的单机引擎，需要做各种细致的优化，而 RocksDB 是由 Facebook 开源的一个非常优秀的单机 KV 存储引擎，可以满足 TiKV 对单机引擎的各种要求。这里可以简单地认为 RocksDB 是一个单机的持久化 Key-Value Map。 Raft 协议 TiKV 选择了 Raft 算法来保证单机失效的情况下数据不丢失不出错。简单来说，就是把数据复制到多台机器上，这样某一台机器无法提供服务时，其他机器上的副本还能提供服务。这个数据复制方案可靠并且高效，能处理副本失效的情况。 Region TiKV 选择了按照 Key 划分 Range。某一段连续的 Key 都保存在一个存储节点上。将整个 Key-Value 空间分成很多段，每一段是一系列连续的 Key，称为一个 Region。尽量让每个 Region 中保存的数据不超过一定的大小，目前在 TiKV 中默认是不超过 96MB。每一个 Region 都可以用 [StartKey，EndKey] 这样的左闭右开区间来描述。 MVCC TiKV 实现了多版本并发控制 (MVCC)。 分布式 ACID 事务 TiKV 的事务采用的是 Google 在 BigTable 中使用的事务模型：Percolator。 ","version":"下一个","tagName":"h3"},{"title":"搭建测试环境​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#搭建测试环境","content":"","version":"下一个","tagName":"h2"},{"title":"Kubernetes 集群​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#kubernetes-集群","content":"本次测试使用三台虚拟机节点部署 Kubernetes 集群，包括 1 个 master 节点和 2 个 worker节点。kubelet 版本为 1.22.0。  ","version":"下一个","tagName":"h3"},{"title":"HwameiStor 本地存储​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#hwameistor-本地存储","content":"在 Kubernetes 集群上部署 HwameiStor 本地存储 在两台 worker 节点上分别为 HwameiStor 配置一块 100G 的本地磁盘 sdb 创建 storagClass ","version":"下一个","tagName":"h3"},{"title":"在 Kubernetes 上部署 TiDB​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#在-kubernetes-上部署-tidb","content":"可以使用 TiDB Operator 在 Kubernetes 上部署 TiDB。TiDB Operator 是 Kubernetes 上的 TiDB 集群自动运维系统，提供包括部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或私有部署的 Kubernetes 集群上。 TiDB 与 TiDB Operator 版本的对应关系如下： TiDB 版本\t适用的 TiDB Operator 版本dev\tdev TiDB &gt;= 5.4\t1.3 5.1 &lt;= TiDB &lt; 5.4\t1.3（推荐），1.2 3.0 &lt;= TiDB &lt; 5.1\t1.3（推荐），1.2，1.1 2.1 &lt;= TiDB &lt; 3.0\t1.0（停止维护） 部署 TiDB Operator​ 安装 TiDB CRDs kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/crd.yaml 安装 TiDB Operator helm repo add pingcap https://charts.pingcap.org/ kubectl create namespace tidb-admin helm install --namespace tidb-admin tidb-operator pingcap/tidb-operator --version v1.3.2 \\ --set operatorImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-operator:v1.3.2 \\ --set tidbBackupManagerImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-backup-manager:v1.3.2 \\ --set scheduler.kubeSchedulerImageName=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler 检查 TiDB Operator 组件 部署 TiDB 集群​ kubectl create namespace tidb-cluster &amp;&amp; \\ kubectl -n tidb-cluster apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/basic/tidb-cluster.yaml kubectl -n tidb-cluster apply -f https://raw.githubusercontent.com /pingcap/tidb-operator/master/examples/basic/tidb-monitor.yaml   连接 TiDB 集群​ yum -y install mysql-client   kubectl port-forward -n tidb-cluster svc/basic-tidb 4000 &gt; pf4000.out &amp;     检查并验证 TiDB 集群状态​ 创建 Hello_world 表 create table hello_world (id int unsigned not null auto_increment primary key, v varchar(32)); 查询 TiDB 版本号 select tidb_version()\\G; 查询 Tikv 存储状态 select * from information_schema.tikv_store_status\\G;  HwameiStor 存储配置​ 从 storageClass local-storage-hdd-lvm 分别为 tidb-tikv 及 tidb-pd 创建一个 PVC:    kubectl get po basic-tikv-0 -oyaml   kubectl get po basic-pd-0 -oyaml   ","version":"下一个","tagName":"h3"},{"title":"测试内容​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#测试内容","content":"","version":"下一个","tagName":"h2"},{"title":"数据库 SQL 基本能力测试​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#数据库-sql-基本能力测试","content":"完成部署数据库集群后，执行了以下基本能力测试，全部通过。 分布式事务​ 测试目的：支持在多种隔离级别下，实现分布式数据操作的完整性约束即 ACID属性 测试步骤： 创建测试数据库 CREATE DATABASE testdb 创建测试用表 CREATE TABLE t_test ( id int AUTO_INCREMENT, name varchar(32), PRIMARY KEY (id) ) 运行测试脚本 测试结果：支持在多种隔离级别下，实现分布式数据操作的完整性约束即 ACID 属性 对象隔离​ 测试目的：测试不同 schema 实现对象隔离 测试脚本： create database if not exists testdb; use testdb create table if not exists t_test ( id bigint, name varchar(200), sale_time datetime default current_timestamp, constraint pk_t_test primary key (id) ); insert into t_test(id,name) values (1,'a'),(2,'b'),(3,'c'); create user 'readonly'@'%' identified by &quot;readonly&quot;; grant select on testdb.* to readonly@'%'; select * from testdb.t_test; update testdb.t_test set name='aaa'; create user 'otheruser'@'%' identified by &quot;otheruser&quot;;  测试结果：支持创建不同 schema 实现对象隔离 表操作支持​ 测试目的：测试是否支持创建、删除和修改表数据、DML、列、分区表 测试步骤：连接数据库后按步骤执行测试脚本 测试脚本： # 创建和删除表 drop table if exists t_test; create table if not exists t_test ( id bigint default '0', name varchar(200) default '' , sale_time datetime default current_timestamp, constraint pk_t_test primary key (id) ); # 删除和修改 insert into t_test(id,name) values (1,'a'),(2,'b'),(3,'c'),(4,'d'),(5,'e'); update t_test set name='aaa' where id=1; update t_test set name='bbb' where id=2; delete from t_dml where id=5; # 修改、增加、删除列 alter table t_test modify column name varchar(250); alter table t_test add column col varchar(255); insert into t_test(id,name,col) values(10,'test','new_col'); alter table t_test add column colwithdefault varchar(255) default 'aaaa'; insert into t_test(id,name) values(20,'testdefault'); insert into t_test(id,name,colwithdefault ) values(10,'test','non-default '); alter table t_test drop column colwithdefault; # 分区表类型（仅摘录部分脚本） CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT NOT NULL, store_id INT NOT NULL )  测试结果：支持创建、删除和修改表数据、DML、列、分区表 索引支持​ 测试目的：验证多种类型的索引（唯一、聚簇、分区、Bidirectional indexes、Expression-based indexes、哈希索引等等）以及索引重建操作。 测试脚本： alter table t_test add unique index udx_t_test (name); # 默认就是主键聚簇索引 ADMIN CHECK TABLE t_test; create index time_idx on t_test(sale_time); alter table t_test drop index time_idx; admin show ddl jobs; admin show ddl job queries 156; create index time_idx on t_test(sale_time);  测试结果：支持创建、删除、组合、单列、唯一索引 表达式​ 测试目的：验证分布式数据库的表达式支持 if、casewhen、forloop、whileloop、loop exit when 等语句（上限 5 类） 前提条件：数据库集群已经部署完成。 测试脚本： SELECT CASE id WHEN 1 THEN 'first' WHEN 2 THEN 'second' ELSE 'OTHERS' END AS id_new FROM t_test; SELECT IF(id&gt;2,'int2+','int2-') from t_test;  测试结果：支持 if、case when、for loop、while loop、loop exit when 等语句（上限 5 类） 执行计划解析​ 测试目的：验证分布式数据库的执行计划解析支持 前提条件：数据库集群已经部署完成。 测试脚本： explain analyze select * from t_test where id NOT IN (1,2,4); explain analyze select * from t_test a where EXISTS (select * from t_test b where a.id=b.id and b.id&lt;3); explain analyze SELECT IF(id&gt;2,'int2+','int2-') from t_test;  测试结果：支持执行计划的解析 执行计划绑定​ 测试目的：验证分布式数据库的执行计划绑定功能 测试步骤： 查看 sql 语句的当前执行计划 使用绑定特性 查看该 sql 语句绑定后的执行计划 删除绑定 测试脚本： explain select * from employees3 a join employees4 b on a.id = b.id where a.lname='Johnson'; explain select /*+ hash_join(a,b) */ * from employees3 a join employees4 b on a.id = b.id where a.lname='Johnson';  测试结果：没有使用 hint 时可能不是 hash_join，使用 hint 后一定是 hash_join。 常用函数​ 测试目的：验证分布式数据库的标准的数据库函数(支持的函数类型） 测试结果：支持标准的数据库函数 显式/隐式事务​ 测试目的：验证分布式数据库的事务支持 测试结果：支持显示与隐式事务 字符集​ 测试目的：验证分布式数据库的数据类型支持 测试结果：目前只支持 UTF-8 mb4 字符集 锁支持​ 测试目的：验证分布式数据库的锁实现 测试结果：描述了锁的实现方式，R-R/R-W/W-W 情况下阻塞情况，死锁处理方式 隔离级别​ 测试目的：验证分布式数据库的事务隔离级别 测试结果：支持 si 隔离级别，支持 rc 隔离级别（4.0 GA 版本） 分布式复杂查询​ 测试目的：验证分布式数据库的分布式复杂查询能力 测试结果：支持跨节点 join 等分布式复杂查询、操作等，支持窗口函数、层次查询 ","version":"下一个","tagName":"h3"},{"title":"系统安全测试​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#系统安全测试","content":"这部分测试系统安全，完成数据库集群部署后，以下安全测试全部通过。 账号管理与权限测试​ 测试目的：验证分布式数据库的账号权限管理 测试脚本： select host,user,authentication_string from mysql.user; create user tidb IDENTIFIED by 'tidb'; select host,user,authentication_string from mysql.user; set password for tidb =password('tidbnew'); select host,user,authentication_string,Select_priv from mysql.user; grant select on *.* to tidb; flush privileges ; select host,user,authentication_string,Select_priv from mysql.user; grant all privileges on *.* to tidb; flush privileges ; select * from mysql.user where user='tidb'; revoke select on *.* from tidb; flush privileges ; revoke all privileges on *.* from tidb; flush privileges ; grant select(id) on test.TEST_HOTSPOT to tidb; drop user tidb;  测试结果： 支持创建、修改删除账号，并配置和密码，支持安全、审计和数据管理三权分立 根据不同账号，对数库各个级别权限控制包括：实例/库/表/列级别 访问控制​ 测试目的：验证分布式数据库的权限访问控制，数据库数据通过赋予基本增删改查访问权限控制 测试脚本： mysql -u root -h 172.17.49.222 -P 4000 drop user tidb; drop user tidb1; create user tidb IDENTIFIED by 'tidb'; grant select on tidb.* to tidb; grant insert on tidb.* to tidb; grant update on tidb.* to tidb; grant delete on tidb.* to tidb; flush privileges; show grants for tidb; exit; mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'select * from aa;' mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'insert into aa values(2);' mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'update aa set id=3;' mysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e 'delete from aa where id=3;'  测试结果：数据库数据通过赋予基本增删改查访问权限控制。 白名单​ 测试目的：验证分布式数据库的白名单功能 测试脚本： mysql -u root -h 172.17.49.102 -P 4000 drop user tidb; create user tidb@'127.0.0.1' IDENTIFIED by 'tidb'; flush privileges; select * from mysql.user where user='tidb'; mysql -u tidb -h 127.0.0.1 -P 4000 -ptidb mysql -u tidb -h 172.17.49.102 -P 4000 -ptidb  测试结果：支持 IP 白名单功能，支持 IP 段通配操作 操作日志记录​ 测试目的：验证分布式数据库的操作监控能力 测试脚本：kubectl -ntidb-cluster logs tidb-test-pd-2 --tail 22 测试结果：记录用户通过运维管理控制台或者 API 执行的关键操作或者错误操作 ","version":"下一个","tagName":"h3"},{"title":"运维管理测试​","type":1,"pageTitle":"TiDB","url":"/cn/docs/use_cases/tidb#运维管理测试","content":"这部分测试系统运维，完成数据库集群部署后，以下运维管理测试全部通过。 数据导入导出​ 测试目的：验证分布式数据库的数据导入导出的工具支持 测试脚本： select * from sbtest1 into outfile '/sbtest1.csv'; load data local infile '/sbtest1.csv' into table test100;  测试结果：支持按表、schema、database 级别的逻辑导出导入 慢日志查询​ 测试目的：获取慢查询的 SQL 信息 前提条件：SQL 执行时间需大于配置的慢查询记录阈值,且 SQL 执行完毕 测试步骤： 调整慢查询阈值到 100ms 执行 sql 查看 log/系统表/dashboard 中的慢查询信息 测试脚本： show variables like 'tidb_slow_log_threshold'; set tidb_slow_log_threshold=100; select query_time, query from information_schema.slow_query where is_internal = false order by query_time desc limit 3;  测试结果：可以获取慢查询信息 有关测试详情，请查阅 TiDB on hwameiStor 部署及测试记录。 ","version":"下一个","tagName":"h3"},{"title":"数据卷驱逐","type":0,"sectionRef":"#","url":"/cn/docs/volumes/volume_eviction","content":"","keywords":"","version":"下一个"},{"title":"驱逐节点​","type":1,"pageTitle":"数据卷驱逐","url":"/cn/docs/volumes/volume_eviction#驱逐节点","content":"在 Kubernetes 系统中，可以使用下列命令驱逐节点，将正在该节点上运行的 Pod 移除并迁移到其他节点上。 同时，也将 Pod 使用的 HwameiStor 数据卷从该节点迁移到其他节点，保证 Pod 可以在其他节点上正常运行。 kubectl drain k8s-node-1 --ignore-daemonsets=true  可以使用下列命令查看所关联的 HwameiStor 数据卷是否迁移成功。 kubectl get LocalStorageNode k8s-node-1 -o yaml  输出类似于： apiVersion: hwameistor.io/v1alpha1 kind: LocalStorageNode metadata: creationTimestamp: &quot;2022-10-11T07:41:58Z&quot; generation: 1 name: k8s-node-1 resourceVersion: &quot;6402198&quot; uid: c71cc6ac-566a-4e0b-8687-69679b07471f spec: hostname: k8s-node-1 storageIP: 10.6.113.22 topogoly: region: default zone: default status: ... pools: LocalStorage_PoolHDD: class: HDD disks: - capacityBytes: 17175674880 devPath: /dev/sdb state: InUse type: HDD freeCapacityBytes: 16101933056 freeVolumeCount: 999 name: LocalStorage_PoolHDD totalCapacityBytes: 17175674880 totalVolumeCount: 1000 type: REGULAR usedCapacityBytes: 1073741824 usedVolumeCount: 1 volumeCapacityBytesLimit: 17175674880 # ** 确保 volumes 为空 ** # volumes: state: Ready  同时，可以使用下列命令查看被驱逐节点上是否还有 HwameiStor 的数据卷。 kubectl get localvolumereplica  输出类似于： NAME CAPACITY NODE STATE SYNCED DEVICE AGE pvc-1427f36b-adc4-4aef-8d83-93c59064d113-957f7g 1073741824 k8s-node-3 Ready true /dev/LocalStorage_PoolHDD-HA/pvc-1427f36b-adc4-4aef-8d83-93c59064d113 20h pvc-1427f36b-adc4-4aef-8d83-93c59064d113-qlpbmq 1073741824 k8s-node-2 Ready true /dev/LocalStorage_PoolHDD-HA/pvc-1427f36b-adc4-4aef-8d83-93c59064d113 30m pvc-6ca4c0d4-da10-4e2e-83b2-19cbf5c5e3e4-scrxjb 1073741824 k8s-node-2 Ready true /dev/LocalStorage_PoolHDD/pvc-6ca4c0d4-da10-4e2e-83b2-19cbf5c5e3e4 30m pvc-f8f017f9-eb09-4fbe-9795-a6e2d6873148-5t782b 1073741824 k8s-node-2 Ready true /dev/LocalStorage_PoolHDD-HA/pvc-f8f017f9-eb09-4fbe-9795-a6e2d6873148 30m  在一些情况下，重启节点时，用户希望仍然保留数据卷在该节点上。可以通过在该节点上添加下列标签实现： kubectl label node k8s-node-1 hwameistor.io/eviction=disable  ","version":"下一个","tagName":"h2"},{"title":"驱逐 Pod​","type":1,"pageTitle":"数据卷驱逐","url":"/cn/docs/volumes/volume_eviction#驱逐-pod","content":"当 Kubernetes 节点负载过重时，系统会选择性地驱逐一些 Pod，从而释放一些系统资源，保证其他 Pod 正常运行。 如果被驱逐的 Pod 使用了 HwameiStor 数据卷，系统会捕捉到这个被驱逐的 Pod，自动将相关的 HwameiStor 数据卷迁移到其他节点，从而保证该 Pod 能正常运行。 ","version":"下一个","tagName":"h2"},{"title":"迁移 Pod​","type":1,"pageTitle":"数据卷驱逐","url":"/cn/docs/volumes/volume_eviction#迁移-pod","content":"运维人员可以主动迁移应用 Pod 和其使用的 HwameiStor 数据卷，从而平衡系统资源，保证系统平稳运行。 可以通过下列两种方式进行主动迁移： 方法 1 kubectl label pod mysql-pod hwameistor.io/eviction=start kubectl delete pod mysql-pod 方法 2 $ cat &lt;&lt; EOF | kubectl apply -f - apiVersion: hwameistor.io/v1alpha1 kind: LocalVolumeMigrate metadata: name: migrate-pvc-6ca4c0d4-da10-4e2e-83b2-19cbf5c5e3e4 spec: sourceNode: k8s-node-1 targetNodesSuggested: - k8s-node-2 - k8s-node-3 volumeName: pvc-6ca4c0d4-da10-4e2e-83b2-19cbf5c5e3e4 migrateAllVols: true EOF $ kubectl delete pod mysql-pod  ","version":"下一个","tagName":"h2"},{"title":"数据卷加密","type":0,"sectionRef":"#","url":"/cn/docs/volumes/volume_encrypt","content":"","keywords":"","version":"下一个"},{"title":"1. 创建一个用于加密数据卷的 Secret​","type":1,"pageTitle":"数据卷加密","url":"/cn/docs/volumes/volume_encrypt#1-创建一个用于加密数据卷的-secret","content":"具体如下： apiVersion: v1 data: key: H4sIAAAAAAAA/+xVP2/bPhDd9SluyGAHUJTgtzFTgPyKLkmDZiwK4kxebNYUSRwp10Xb715IkWzLf5II6eAC3iTe3eO9R/Le1PoJWpEB0AJthcl4J41LxAu0Av67jBlAVIyBdpZpmYgdWmlxQjbWIACBfUlpRlUUEDyn756XRVjm6/WtNMkUrFEo4Gz08OlW3t/c/T/OuLIkn4ylKLIcCkqqWJcUdTRuLOS9DfI63NTml8X5xQ8sbdZSUN49mWmD+c1Pp MOSBETihVF0551Jnot1193HZQYw885zxxSe0EbKAObVhNhRoiijXqMD5MDekgByOnjjUs2aqSnvp0VfsaKehE1vzVdCnlJ6DgqQMpVBbijXUWiAUNVnJ2BOFJrivchSlpRQbvb9zP45r4N7Q2pgiuTSuoJpSksBo0628XXi6n29derJGnNnp7DMMZjDKr6Ah1ozxShbgefG6cFFq3YiBWRMngVcb/Z37zVdjy7Ox+1isKioJJcEnP28+r3nhJ3XdLx8HrweRid4P YRN3UAMqGifMhuxNwN293XFrI/ZhocgBq8PoQ0kWyMp7xIaR3wIc5XQe0Wa/aBXVG8VZhj7z/QDGkv612OlFJEmPf6Lynbza98lybdyu+u4W/D6++5usDw4LWcYZ02w9LqytSldNb+dlnW8fPnkejCtemejx483n2/HPax2upWU2Ci5Pe3hy9dhrnN1cp0jdZ35Qk+Od0yfbOdkOyfbeZftvPbA/zXf+RMAAP//IK/8i+YNAAA= kind: Secret metadata: name: hwameistor-encrypt-secret namespace: hwameistor type: Opaque  ","version":"下一个","tagName":"h2"},{"title":"2. 创建一个 StorageClass​","type":1,"pageTitle":"数据卷加密","url":"/cn/docs/volumes/volume_encrypt#2-创建一个-storageclass","content":"使用以下命令创建一个 StorageClass 并指定使用上方创建的 Secret： apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage-hdd-encrypt provisioner: lvm.hwameistor.io volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Delete parameters: encryptSecret: hwameistor/hwameistor-encrypt-secret encryptType: LUKS replicaNumber: &quot;1&quot; poolClass: &quot;HDD&quot; poolType: &quot;REGULAR&quot; volumeKind: &quot;LVM&quot; striped: &quot;true&quot; csi.storage.k8s.io/fstype: &quot;xfs&quot;  ","version":"下一个","tagName":"h2"},{"title":"3. 创建 PVC 和 Deployment​","type":1,"pageTitle":"数据卷加密","url":"/cn/docs/volumes/volume_encrypt#3-创建-pvc-和-deployment","content":"apiVersion: v1 kind: PersistentVolumeClaim metadata: name: local-storage-pvc-encrypt spec: accessModes: - ReadWriteOnce storageClassName: local-storage-hdd-encrypt resources: requests: storage: 1Gi  apiVersion: apps/v1 kind: Deployment metadata: name: nginx-local-storage-lvm labels: app: nginx-local-storage-lvm spec: replicas: 1 selector: matchLabels: app: nginx-local-storage-lvm template: metadata: labels: app: nginx-local-storage-lvm name: nginx-local-storage-lvm spec: restartPolicy: Always terminationGracePeriodSeconds: 0 containers: - image: nginx:latest imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 command: - sh - -xc - | VOL=&quot;$( df | grep /usr/share/nginx/html | awk '{print $1,$NF}' )&quot; echo &quot;&lt;center&gt;&lt;h1&gt;Demo volume: ${VOL}&lt;/h1&gt;&lt;/center&gt;&quot; &gt; /usr/share/nginx/html/index.html nginx -g &quot;daemon off;&quot; volumeMounts: - name: html-root mountPath: /usr/share/nginx/html resources: limits: cpu: &quot;100m&quot; memory: &quot;100Mi&quot; volumes: - name: html-root persistentVolumeClaim: claimName: local-storage-pvc-encrypt  ","version":"下一个","tagName":"h2"},{"title":"4. 检查应用是否被正常创建​","type":1,"pageTitle":"数据卷加密","url":"/cn/docs/volumes/volume_encrypt#4-检查应用是否被正常创建","content":"# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-local-storage-lvm-79886d9dd-44fsg 1/1 Running 0 20m 100.111.156.91 k8s-node1  ","version":"下一个","tagName":"h2"},{"title":"5. 检查数据卷是否为加密卷​","type":1,"pageTitle":"数据卷加密","url":"/cn/docs/volumes/volume_encrypt#5-检查数据卷是否为加密卷","content":"通过 lsblk 命令可以查看数据卷的 TYPE 是否为 crypt： # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 160G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 159G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm └─centos-home 253:2 0 101.1G 0 lvm /home sdb 8:16 0 200G 0 disk └─LocalStorage_PoolHDD-pvc--2c097032--690d--4510--99ad--54119b6b650c 253:3 0 1G 0 lvm └─pvc-2c097032-690d-4510-99ad-54119b6b650c-encrypt 253:4 0 1008M 0 crypt /var/lib/kubelet/pods/4c2b76f3-a84f-4e62-88c8-a71abeb68efd/volumes/kubernetes.io~csi/pvc-2c097032-690d-4510-99ad-54119b6b650c/mount sr0 11:0 1 1024M 0 rom  通过 blkid 命令可以查看数据卷的 TYPE 是否为 crypto_LUKS： # blkid /dev/LocalStorage_PoolHDD/pvc-2c097032-690d-4510-99ad-54119b6b650c /dev/LocalStorage_PoolHDD/pvc-2c097032-690d-4510-99ad-54119b6b650c: UUID=&quot;a1910adf-f1dc-45a4-aeb3-6a8cf045bb9d&quot; TYPE=&quot;crypto_LUKS&quot;  ","version":"下一个","tagName":"h2"},{"title":"数据卷快照","type":0,"sectionRef":"#","url":"/cn/docs/volumes/volume_snapshot","content":"","keywords":"","version":"下一个"},{"title":"创建新的 VolumeSnapshotClass​","type":1,"pageTitle":"数据卷快照","url":"/cn/docs/volumes/volume_snapshot#创建新的-volumesnapshotclass","content":"默认情况下，HwameiStor 在安装过程中不会自动创建这样的 VolumeSnapshotClass，因此您需要手动创建 VolumeSnapshotClass。 示例 VolumeSnapshotClass 如下： kind: VolumeSnapshotClass apiVersion: snapshot.storage.k8s.io/v1 metadata: name: hwameistor-storage-lvm-snapshot annotations: snapshot.storage.kubernetes.io/is-default-class: &quot;true&quot; parameters: snapsize: &quot;1073741824&quot; driver: lvm.hwameistor.io deletionPolicy: Delete  snapsize：指定创建卷快照的大小。 注意 如果不指定 snapsize 参数，那么创建的快照大小和源卷的大小一致。 创建 VolumeSnapshotClass 后，您可以使用它来创建 VolumeSnapshot。 ","version":"下一个","tagName":"h2"},{"title":"使用 VolumeSnapshotClass 创建 VolumeSnapshot​","type":1,"pageTitle":"数据卷快照","url":"/cn/docs/volumes/volume_snapshot#使用-volumesnapshotclass-创建-volumesnapshot","content":"示例 VolumeSnapshot 如下： apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: snapshot-local-storage-pvc-lvm spec: volumeSnapshotClassName: hwameistor-storage-lvm-snapshot source: persistentVolumeClaimName: local-storage-pvc-lvm  persistentVolumeClaimName：指定要创建快照的 PVC。 创建 VolumeSnapshot 后，您可以使用如下命令检查 VolumeSnapshot。 $ kubectl get vs NAME READYTOUSE SOURCEPVC SOURCESNAPSHOTCONTENT RESTORESIZE SNAPSHOTCLASS SNAPSHOTCONTENT CREATIONTIME AGE snapshot-local-storage-pvc-lvm true local-storage-pvc-lvm 1Gi hwameistor-storage-lvm-snapshot snapcontent-0fc17697-68ea-49ce-8e4c-7a791e315110 53y 2m57s  创建 VolumeSnapshot 后，您可以使用如下命令检查 HwameiStor 本地卷快照。 $ kubectl get lvs NAME CAPACITY SOURCEVOLUME STATE MERGING INVALID AGE snapcontent-0fc17697-68ea-49ce-8e4c-7a791e315110 1073741824 pvc-967baffd-ce10-4739-b996-87c9ed24e635 Ready 5m31s  CAPACITY：快照的容量大小SOURCEVOLUME：快照的源卷名称MERGING：快照是否处于合并状态（一般由回滚操作触发）INVALID：快照是否失效（一般在快照容量写满触发）AGE：快照真实创建的时间（不同于 CR 创建的时间，这个时间是底层快照数据卷的创建时间） 创建 VolumeSnapshot 后，您可以对 VolumeSnapshot 进行还原、回滚操作。 ","version":"下一个","tagName":"h2"},{"title":"对卷快照进行还原操作​","type":1,"pageTitle":"数据卷快照","url":"/cn/docs/volumes/volume_snapshot#对卷快照进行还原操作","content":"可以创建 pvc，对卷快照进行还原操作。具体如下： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: local-storage-pvc-lvm-restore spec: storageClassName: local-storage-hdd-lvm dataSource: name: snapshot-local-storage-pvc-lvm kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 1Gi  ","version":"下一个","tagName":"h2"},{"title":"对卷快照进行回滚操作​","type":1,"pageTitle":"数据卷快照","url":"/cn/docs/volumes/volume_snapshot#对卷快照进行回滚操作","content":"注意 对快照进行回滚必须先停止源卷的 I/O，比如先停止应用，等待回滚操作完全结束， 并确认数据一致性之后再使用回滚后的数据卷。 可以通过创建资源 LocalVolumeSnapshotRestore，对卷快照进行回滚操作。具体如下： apiVersion: hwameistor.io/v1alpha1 kind: LocalVolumeSnapshotRestore metadata: name: rollback-test spec: sourceVolumeSnapshot: snapcontent-0fc17697-68ea-49ce-8e4c-7a791e315110 restoreType: &quot;rollback&quot;  sourceVolumeSnapshot：指定要进行回滚操作的本地卷快照。 对创建的 LocalVolumeSnapshotRestore 进行观察，可以通过状态了解整个回滚的过程。回滚结束后，对应的 LocalVolumeSnapshotRestore 会被删除。 $ kubectl get LocalVolumeSnapshotRestore -w NAME TARGETVOLUME SOURCESNAPSHOT STATE AGE restore-test2 pvc-967baffd-ce10-4739-b996-87c9ed24e635 snapcontent-0fc17697-68ea-49ce-8e4c-7a791e315110 Submitted 0s restore-test2 pvc-967baffd-ce10-4739-b996-87c9ed24e635 snapcontent-81a1f605-c28a-4e60-8c78-a3d504cbf6d9 InProgress 0s restore-test2 pvc-967baffd-ce10-4739-b996-87c9ed24e635 snapcontent-81a1f605-c28a-4e60-8c78-a3d504cbf6d9 Completed 2s  ","version":"下一个","tagName":"h2"},{"title":"数据卷 IO 限速","type":0,"sectionRef":"#","url":"/cn/docs/volumes/volume_provisioned_io","content":"","keywords":"","version":"下一个"},{"title":"要求 (如果您想限制非直接 IO)​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#要求-如果您想限制非直接-io","content":"cgroup v2 具有以下要求： 操作系统发行版启用 cgroup v2Linux 内核为 5.8 或更高版本 更多信息, 请参见 Kubernetes 官网。 ","version":"下一个","tagName":"h2"},{"title":"使用最大 IOPS 和吞吐量参数创建新的 StorageClass​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#使用最大-iops-和吞吐量参数创建新的-storageclass","content":"默认情况下，HwameiStor 在安装过程中不会自动创建这样的 StorageClass，因此您需要手动创建 StorageClass。 示例 StorageClass 如下： allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hwameistor-storage-lvm-hdd-sample parameters: convertible: &quot;false&quot; csi.storage.k8s.io/fstype: xfs poolClass: HDD poolType: REGULAR provision-iops-on-creation: &quot;100&quot; provision-throughput-on-creation: 1Mi replicaNumber: &quot;1&quot; striped: &quot;true&quot; volumeKind: LVM provisioner: lvm.hwameistor.io reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer  与 HwameiStor 安装程序创建的常规 StorageClass 相比，添加了以下参数： Provision-iops-on-creation：指定创建时卷的最大 IOPS。Provision-throughput-on-creation：它指定创建时卷的最大吞吐量。 创建 StorageClass 后，您可以使用它来创建 PVC。 ","version":"下一个","tagName":"h2"},{"title":"使用 StorageClass 创建 PVC​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#使用-storageclass-创建-pvc","content":"示例 PVC 如下： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-sample spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: hwameistor-storage-lvm-hdd-sample  创建 PVC 后，您可以创建 Deployment 来使用 PVC。 ","version":"下一个","tagName":"h2"},{"title":"创建带有 PVC 的 Deployment​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#创建带有-pvc-的-deployment","content":"示例 Deployment 如下：: apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: pod-sample name: pod-sample spec: replicas: 1 selector: matchLabels: app: pod-sample strategy: {} template: metadata: creationTimestamp: null labels: app: pod-sample spec: volumes: - name: data persistentVolumeClaim: claimName: pvc-sample containers: - command: - sleep - &quot;100000&quot; image: busybox name: busybox resources: {} volumeMounts: - name: data mountPath: /data status: {}  创建 Deployment 后，您可以使用以下命令测试卷的 IOPS 和吞吐量： 终端1: kubectl exec -it pod-sample-5f5f8f6f6f-5q4q5 -- /bin/sh dd if=/dev/zero of=/data/test bs=4k count=1000000 oflag=direct  终端2: /dev/LocalStorage_PoolHDD/pvc-c623054b-e7e9-41d7-a987-77acd8727e66 是节点上卷的路径。 您可以使用 kubectl get lvr 命令找到它。 iostat -d /dev/LocalStorage_PoolHDD/pvc-c623054b-e7e9-41d7-a987-77acd8727e66 -x -k 2  注意 由于 cgroupv1 限制，最大 IOPS 和吞吐量的设置可能对非直接 IO 不生效。 然而, 如果您使用 cgroupv2，那么最大 IOPS 和吞吐量的设置将对非直接 IO 生效。 ","version":"下一个","tagName":"h2"},{"title":"如何更改数据卷的最大 IOPS 和吞吐量​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#如何更改数据卷的最大-iops-和吞吐量","content":"最大 IOPS 和吞吐量在 StorageClass 的参数上指定，您不能直接更改它，因为它现在是不可变的。 与其他存储厂商不同的是，HwameiStor 是一个基于 Kubernetes 的存储解决方案，它定义了一组操作原语基于 Kubernetes CRD。这意味着您可以修改相关的 CRD 来更改卷的实际最大 IOPS 和吞吐量。 以下步骤显示如何更改数据卷的最大 IOPS 和吞吐量。 ","version":"下一个","tagName":"h2"},{"title":"查找给定 PVC 对应的 LocalVolume CR​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#查找给定-pvc-对应的-localvolume-cr","content":"$ kubectl get pvc pvc-sample NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE demo Bound pvc-c354a56a-5cf4-4ff6-9472-4e24c7371e10 10Gi RWO hwameistor-storage-lvm-hdd 5d23h pvc-sample Bound pvc-cac82087-6f6c-493a-afcd-09480de712ed 10Gi RWO hwameistor-storage-lvm-hdd-sample 5d23h $ kubectl get localvolume NAME POOL REPLICAS CAPACITY USED STATE RESOURCE PUBLISHED FSTYPE AGE pvc-c354a56a-5cf4-4ff6-9472-4e24c7371e10 LocalStorage_PoolHDD 1 10737418240 33783808 Ready -1 master xfs 5d23h pvc-cac82087-6f6c-493a-afcd-09480de712ed LocalStorage_PoolHDD 1 10737418240 33783808 Ready -1 master xfs 5d23h  根据打印输出，PVC 的 LocalVolume CR 为 pvc-cac82087-6f6c-493a-afcd-09480de712ed。 ","version":"下一个","tagName":"h3"},{"title":"修改 LocalVolume CR​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#修改-localvolume-cr","content":"kubectl edit localvolume pvc-cac82087-6f6c-493a-afcd-09480de712ed  在编辑器中，找到 spec.volumeQoS 部分并修改 iops 和 throughput 字段。顺便说一下，空值意味着没有限制。 最后，保存更改并退出编辑器。设置将在几秒钟后生效。 注意 将来，一旦 Kubernetes 支持它， 我们将允许用户直接修改卷的最大 IOPS 和吞吐量。 ","version":"下一个","tagName":"h3"},{"title":"如何检查数据卷的实际 IOPS 和吞吐量​","type":1,"pageTitle":"数据卷 IO 限速","url":"/cn/docs/volumes/volume_provisioned_io#如何检查数据卷的实际-iops-和吞吐量","content":"HwameiStor 使用 cgroupv1或 cgroupv2来限制数据卷的 IOPS 和吞吐量，因此您可以使用以下命令来检查数据卷的实际 IOPS 和吞吐量。 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 160G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 159G 0 part ├─centos-root 253:0 0 300G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm └─centos-home 253:2 0 101.1G 0 lvm /home sdb 8:16 0 100G 0 disk ├─LocalStorage_PoolHDD-pvc--cac82087--6f6c--493a--afcd--09480de712ed 253:3 0 10G 0 lvm /var/lib/kubelet/pods/3d6bc980-68ae-4a65-a1c8-8b410b7d240f/v └─LocalStorage_PoolHDD-pvc--c354a56a--5cf4--4ff6--9472--4e24c7371e10 253:4 0 10G 0 lvm /var/lib/kubelet/pods/521fd7b4-3bef-415b-8720-09225f93f231/v sdc 8:32 0 300G 0 disk └─sdc1 8:33 0 300G 0 part └─centos-root 253:0 0 300G 0 lvm / sr0 11:0 1 973M 0 rom # 如果 cgroup 版本是 v1。 $ cat /sys/fs/cgroup/blkio/blkio.throttle.read_iops_device 253:3 100 $ cat /sys/fs/cgroup/blkio/blkio.throttle.write_iops_device 253:3 100 $ cat /sys/fs/cgroup/blkio/blkio.throttle.read_bps_device 253:3 1048576 $ cat /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device 253:3 1048576 # 如果 cgroup 版本是 v2。 # cat /sys/fs/cgroup/kubepods.slice/io.max 253:0 rbps=1048576 wbps=1048576 riops=100 wiops=100  ","version":"下一个","tagName":"h2"}],"options":{"languages":["en","zh"],"id":"default"}}